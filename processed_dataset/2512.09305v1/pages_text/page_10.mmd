|  |  |  |  |
| --- | --- | --- | --- |
|  | \displaystyle\int\_{0}^{\infty}\tau^{2}f\_{\rho}(\tau)dH(\tau)\leq\int\_{0}^{% \infty}\tau^{2}f\_{0}(\tau)dH(\tau)=\frac{E(\tau)}{E(\tau^{-1})}. |  | (13) |

Hence from equation (2) and (13) we obtain

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | \displaystyle\varphi\_{2}(w,w\_{1};\eta,\rho) | \displaystyle\leq\frac{(1+w+p\_{1}w\_{1})}{\sqrt{(p\_{1}+p\_{2}-1)(p\_{1}+p\_{2}-2)}% }\left(\frac{E(\tau)}{E(\tau^{-1})}\right)^{1/2}. |  | (14) |

Hence from the equations (14) and (11) we get,

|  |  |  |
| --- | --- | --- |
|  | \displaystyle\varphi\_{2}(w,w\_{1};\eta,\rho)\leq\frac{(1+w+p\_{1}w\_{1})}{\sqrt{(% p\_{1}+p\_{2}-1)(p\_{1}+p\_{2}-2)}}\min\Bigg{\{}\left(\frac{E(\tau^{p\_{1}+p\_{2}+1}% )}{E(\tau^{p\_{1}+p\_{2}-1})}\right)^{1/2},\left(\frac{E(\tau)}{E(\tau^{-1})}% \right)^{1/2}\Bigg{\}}=\varphi\_{12}(w,w\_{1}). |  |

Now using the convexity of R{}\_{1}(\delta\_{\varphi\_{2}},\eta,\rho) for P(\varphi\_{12}(W,W\_{1})<\varphi\_{2}(W,W\_{1}))>0 we get the result. Proof of (ii) is similar to (i), so we omit it.

Now the Theorem 4 can be extended by using the information contained in the statistics Y, substituting Y for X within the class \mathcal{D}\_{2} in (6). We get the following theorem

###### Theorem 5.

1. (i)

   Under the L\_{1}(\cdot) loss function, the risk of the estimator

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | \delta\_{1S3}^{1}=\begin{cases}\min\left\{\varphi\_{3}(W,W\_{2}),\varphi\_{31}(W,W% \_{2})\right\}S\_{1},&\ W\_{2}>0\\ \varphi\_{3}(W,W\_{2})S\_{1},&\text{otherwise}\end{cases} |  | (15) |

   is nowhere larger than that of the estimator \delta\_{\varphi\_{3}} provide P(\varphi\_{3}(W,W\_{2})>\varphi\_{31}(W,W\_{2}))>0 where W\_{2}=\frac{Y}{S\_{1}} and

   |  |  |  |
   | --- | --- | --- |
   |  | \varphi\_{31}(W,W\_{2})=\frac{(1+W+p\_{2}W\_{2})}{\sqrt{(p\_{1}+p\_{2}-1)(p\_{1}+p\_{2% }-2)}}\min\Bigg{\{}\left(\frac{E(\tau^{p\_{1}+p\_{2}+1})}{E(\tau^{p\_{1}+p\_{2}-1}% )}\right)^{1/2},\left(\frac{E(\tau)}{E(\tau^{-1})}\right)^{1/2}\Bigg{\}}. |  |
2. (ii)

   For the loss function L\_{2}(\cdot), the risk of the estimator

   |  |  |  |  |
   | --- | --- | --- | --- |
   |  | \delta\_{1S3}^{2}=\begin{cases}\min\left\{\varphi\_{3}(W,W\_{2}),\varphi\_{32}(W,W% \_{2})\right\}S\_{1},&\ W\_{2}>0\\ \varphi\_{3}(W,W\_{2})S\_{1},&\text{otherwise}\end{cases} |  | (16) |

   is nowhere larger than that of the estimator \delta\_{\varphi\_{3}} provide P(\varphi\_{3}(W,W\_{2})>\varphi\_{32}(W,W\_{2}))>0, where

   |  |  |  |
   | --- | --- | --- |
   |  | \varphi\_{32}(W,W\_{2})=\frac{(1+W+p\_{2}W\_{2})}{p\_{1}+p\_{2}-1}\min\Bigg{\{}\frac% {E(\tau^{p\_{1}+p\_{2}})}{E(\tau^{p\_{1}+p\_{2}-1})},\frac{1}{E(\tau^{-1})}\Bigg{% \}}. |  |

###### Remark 8.

When \tau=1 with probability one, then the Theorem 4 and Theorem 5 reduces to the following result which was previously obtained by [29] in Example 2.3 and Example 2.4. under entropy and symmetric loss functions respectively.

## 3 Improved estimation for \sigma\_{2} when \sigma\_{1}\leq\sigma\_{2}

In this section, we will derive estimators of the parameter \sigma\_{2} which will improve upon the BAEE under the restriction \sigma\_{1}\leq\sigma\_{2}. We consider estimators of the form