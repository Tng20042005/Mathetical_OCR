The Kullback-Leibler divergence (see [5]) is useful to measure the difference between two probability distributions.
If Y\_{1} and Y\_{2} are two random variables with PDFs g\_{Y\_{1}}(y;\bm{\theta}) and g\_{Y\_{2}}(y;\bm{\theta}^{\prime}), respectively, where \bm{\theta}=(\theta\_{1},\theta\_{2},\theta\_{3},\theta\_{4},\theta\_{5},\theta\_{6}), \bm{\theta}^{\prime}=(\theta\_{1}^{\prime},\theta\_{2},\theta\_{3},\theta\_{4},%
\theta\_{5},\theta\_{6}^{\prime}), \theta\_{1}\neq\theta\_{1}^{\prime} and \theta\_{6}\neq\theta\_{6}^{\prime}, then their Kullback-Leibler divergence has the form

|  |  |  |
| --- | --- | --- |
|  | \displaystyle D\_{\rm KL}(g\_{Y\_{1}}\|g\_{Y\_{2}})=\int\_{0}^{\infty}g\_{Y\_{1}}(y;% \bm{\theta})\log\left({g\_{Y\_{1}}(y;\bm{\theta})\over g\_{Y\_{2}}(y;\bm{\theta}^{% \prime})}\right){\rm d}y. |  |

From (2) the above integral is

|  |  |  |
| --- | --- | --- |
|  | \displaystyle=\log(c(\bm{\theta}^{\prime})-\log(c(\bm{\theta})+(\theta\_{6}-% \theta\_{6}^{\prime})\int\_{0}^{\infty}\log(y)g\_{Y\_{1}}(y;\bm{\theta}){\rm d}y+(% \theta\_{1}^{\prime}-\theta\_{1})\int\_{0}^{\infty}yg\_{Y\_{1}}(y;\bm{\theta}){\rm d}y |  |
|  |  |  |
| --- | --- | --- |
|  | \displaystyle=\log(c(\bm{\theta}^{\prime})-\log(c(\bm{\theta})+(\theta\_{6}-% \theta\_{6}^{\prime})E\_{\bm{\theta}}[\log(Y)]+(\theta\_{1}^{\prime}-\theta\_{1})E% \_{\bm{\theta}}[Y] |  |
|  |  |  |
| --- | --- | --- |
|  | \displaystyle=\log(c(\bm{\theta}^{\prime})-\log(c(\bm{\theta})+(\theta\_{6}-% \theta\_{6}^{\prime})\xi\_{Y}+(\theta\_{1}^{\prime}-\theta\_{1})\,\frac{\mathbb{H}% (\theta\_{1},\theta\_{2},\theta\_{3},\theta\_{4},\theta\_{5},\theta\_{6}+1)}{\mathbb% {H}(\theta\_{1},\theta\_{2},\theta\_{3},\theta\_{4},\theta\_{5},\theta\_{6})}, |  |

where \xi\_{Y}:=E\_{\bm{\theta}}[\log(Y)] and in the last equality the formula (23) of E\_{\bm{\theta}}[Y] was used.
That is,

|  |  |  |  |
| --- | --- | --- | --- |
|  | \displaystyle D\_{\rm KL}(g\_{Y\_{1}}\|g\_{Y\_{2}})=\log(c(\bm{\theta}^{\prime})-% \log(c(\bm{\theta})+(\theta\_{6}-\theta\_{6}^{\prime})\xi\_{Y}+(\theta\_{1}^{% \prime}-\theta\_{1})\,\frac{\mathbb{H}(\theta\_{1},\theta\_{2},\theta\_{3},\theta\_% {4},\theta\_{5},\theta\_{6}+1)}{\mathbb{H}(\theta\_{1},\theta\_{2},\theta\_{3},% \theta\_{4},\theta\_{5},\theta\_{6})}. |  | (25) |

By using the well-known inequalities 1-x^{-1}<\log(x)\leq x-1, for x>0, from (23) we get the finiteness of \xi\_{Y}, more precisely,

|  |  |  |
| --- | --- | --- |
|  | \displaystyle 1-\frac{\mathbb{H}(\theta\_{1},\theta\_{2},\theta\_{3},\theta\_{4},% \theta\_{5},\theta\_{6}-1)}{\mathbb{H}(\theta\_{1},\theta\_{2},\theta\_{3},\theta\_{% 4},\theta\_{5},\theta\_{6})}<\xi\_{Y}\leq\frac{\mathbb{H}(\theta\_{1},\theta\_{2},% \theta\_{3},\theta\_{4},\theta\_{5},\theta\_{6}+1)}{\mathbb{H}(\theta\_{1},\theta\_{% 2},\theta\_{3},\theta\_{4},\theta\_{5},\theta\_{6})}-1. |  |

###### Remark 3.1.

By using the entropy formula (see Subsection 3.2) and Kullback-Leibler divergence formula in (25), we can obtain a closed-form expression for the cross entropy, denoted by H(g\_{Y\_{1}},g\_{Y\_{2}}), since D\_{\rm KL}(g\_{Y\_{1}}\|g\_{Y\_{2}})=H(g\_{Y\_{1}},g\_{Y\_{2}})-h(Y\_{1}).

## 4 Estimation

In this section, we present two classes of estimators for the model g(y;\bm{\theta}) (2).

### 4.1 Exponential family