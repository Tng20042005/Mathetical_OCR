|  |  |  |
| --- | --- | --- |
|  | \partial\_{\theta\_{6}}\ell(\bm{\theta})=-\frac{n}{c(\bm{\theta})}\partial\_{% \theta\_{6}}c(\bm{\theta})+\sum\_{j=1}^{n}\log Y\_{j}, |  |

where \partial\_{\theta\_{6}}c(\bm{\theta}) is given in (46).

###### Remark 4.2.

The gradient \nabla\ell(\bm{\theta}) can be computed for the general case \theta\_{5}\in\mathbb{R} using approximation (42).

### 4.4 Computational implementation

#### 4.4.1 Choosing a good initial guess

While the methods used for estimation may be straightforward in the sense that they rely on standard estimation techniques, the presence of integral expressions in the PDF means that the computational implementation/optimization of models involving special functions is not always simple. Optimizers in languages such as Python [30] and R [17] can easily become inefficient if a well-planned strategy for implementing objective functions is not adopted.
A good initial guess will be important for efficient convergence of the implemented methods.

###### Example 4.3.

Let \mathbf{Y}=(Y\_{1},\dots,Y\_{n}) be a sample from the distribution g(\cdot;\bm{\theta}). Suppose we estimate (\hat{\alpha},\hat{\beta}) by modeling \mathbf{Y} as a Gamma(\alpha,\beta) distribution and find that this model fits the data well. Then, from Table 1, we obtain the initial estimate \hat{\bm{\theta}}\_{0}=(\hat{\beta},0,1,1,\hat{\alpha}-1) for the parameter vector \bm{\theta} in the general model g(\cdot;\bm{\theta}).

In the next step, we can use \hat{\bm{\theta}}\_{0} as an initial guess to refine our estimation of \bm{\theta} without being constrained to the specific Gamma model. This procedure can also be applied using other distributions listed in Table 1.

Although simple, Example 4.3 provides a powerful tool for obtaining initial guesses, as the distributions in Table 1 are mostly available in standard Python [30] and R [17] libraries.
Another effective approach for obtaining an initial guess is to use the result of one estimation method (such as LSE, method of moments, methods based on characteristic function or Mellin transform) as the starting point for another method (such as MLE). This will be discussed in the next subsection.

Finally, a different option for choosing the initial guess involves defining a grid of values and testing various initial values within a reasonable range. The estimation method’s performance is then evaluated for each of these values. The aim is to identify the value from the grid that leads to the most accurate final estimate, such as the one with the best convergence or smallest error. This process can be carried out empirically by observing which initial guess produces the most favorable result.

#### 4.4.2 Algorithms for estimations

The parameter estimation was done by optimization procedures according to Equation (29) for the LSE and to Equation (31) for MLE.

The large number of parameters demanded by extreme value \mathbb{H}-function class of distributions lead to frequent convergence problems when the MLE approach was tried even using particular cases of Table 1 as initial guesses.

Thus, we decided to use LSE in Algorithm 4.4.2 as a first step in the parameter estimation due to its better convergence and try to refine the estimation by using a LSE-tuple as an initial guess to the MLE estimator of Algorithm 4.4.2.

{algorithm}

Estimation of parameters \bm{\theta} using the LSE
{algorithmic}[1]
\StatexInput: Data (Y\_{1},\cdots,Y\_{n}) sampled from the g(\cdot;\bm{\theta}).
\StatexOutput: Estimates \widehat{\theta\_{Y}}.

\State

Compute estimates \hat{\alpha}, \hat{\beta}, \hat{\gamma}, and \hat{\sigma} to fit the particular case chosen from Table 1.

\State

Set the initial guess for the numerical optimization to be \bm{\theta}\_{0}=(\theta\_{1},\theta\_{2},\theta\_{3},\theta\_{4},\theta\_{5},\theta%
\_{6}) also according to Table 1.

\State

Calculate \widehat{\theta\_{Y}} using (29).

\State

Return \widehat{\theta\_{Y}}.

{algorithm}

Estimation of parameters \bm{\theta} using the MLE
{algorithmic}[1]
\StatexInput: Data (Y\_{1},\cdots,Y\_{n}) sampled from the g(\cdot;\bm{\theta}).
\StatexOutput: Estimates \hat{\theta}^{MLE}.

\State

Set the initial guess for the numerical optimization to be \bm{\theta}\_{0}=\widehat{\theta\_{Y}} from Algorithm 4.4.2.

\State

Calculate \hat{\theta}^{MLE} using (31).

\State