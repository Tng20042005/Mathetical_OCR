<!DOCTYPE html><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Estimation of Stochastic Optimal Transport Maps</title>
<!--Generated on Thu Dec 11 22:13:29 2025 by LaTeXML (version 0.8.6) http://dlmf.nist.gov/LaTeXML/.-->

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Estimation of Stochastic Optimal Transport Maps</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sloan Nietert 
<br class="ltx_break">EPFL 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">sloan.nietert@epfl.ch</span> 
<br class="ltx_break">&amp;Ziv Goldfeld 
<br class="ltx_break">Cornell University 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">goldfeld@cornell.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning.
However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier’s theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass.
To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed
framework in settings where
existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.</p>
</div>
<span class="ltx_ERROR undefined">\addauthor</span>
<div id="p1" class="ltx_para">
<p class="ltx_p">snblue
<span class="ltx_ERROR undefined">\addauthor</span>zgred
 <span class="ltx_ERROR undefined">\Crefname</span>factFactFacts
<span class="ltx_ERROR undefined">\Crefname</span>assumptionAssumptionAssumptions

</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Optimal transport (OT) is a principled framework for comparing and transforming probability distributions according to the geometry of the underlying metric space <cite class="ltx_cite ltx_citemacro_citep">(Villani, <a href="#bib.bib45" title="" class="ltx_ref">2003</a>; Santambrogio, <a href="#bib.bib38" title="" class="ltx_ref">2015</a>)</cite>. Central to OT theory is the transport map, which performs said transformation. For <span id="S1.p1.m1" class="ltx_Math">\cX,\cY\subseteq\R^{d}</span>, we say that <span id="S1.p1.m2" class="ltx_Math">T:\cX\to\cY</span> is a <em class="ltx_emph ltx_font_italic">transport map</em> from source distribution <span id="S1.p1.m3" class="ltx_Math">\mu\in\cP(\cX)</span> to target <span id="S1.p1.m4" class="ltx_Math">\nu\in\cP(\cY)</span> if the pushforward measure <span id="S1.p1.m5" class="ltx_Math">T_{\sharp}\mu=\mu\circ T^{-1}</span> coincides with <span id="S1.p1.m6" class="ltx_Math">\nu</span>. An optimal transport map <span id="S1.p1.m7" class="ltx_Math">T^{\star}</span>, if it exists, is a solution to the <em class="ltx_emph ltx_font_italic">Monge OT problem</em> from <span id="S1.p1.m8" class="ltx_Math">\mu</span> to <span id="S1.p1.m9" class="ltx_Math">\nu</span>, which reads as follows for the <span id="S1.p1.m10" class="ltx_Math">p</span>-Wasserstein cost:</p>
<table id="S1.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S1.E1.m1" class="ltx_Math">\inf_{T:\,T_{\sharp}\mu=\nu}\E_{\mu}[\|X-T(X)\|^{p}]^{\frac{1}{p}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</table>
<p class="ltx_p">Monge maps are employed for many applications, including domain adaptation <cite class="ltx_cite ltx_citemacro_citep">(Courty et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>; Redko et al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>, single-cell genomics <cite class="ltx_cite ltx_citemacro_citep">(Schiebinger et al., <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Bunne et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, style transfer <cite class="ltx_cite ltx_citemacro_citep">(Kolkin et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>; Mroueh, <a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>, and generative modeling <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib48" title="" class="ltx_ref">2018</a>; Vesseron et al., <a href="#bib.bib44" title="" class="ltx_ref">2025</a>)</cite>. An important special case is when <span id="S1.p1.m11" class="ltx_Math">p=2</span> and <span id="S1.p1.m12" class="ltx_Math">\mu</span> is absolutely continuous with respect to (w.r.t.) the Lebesgue measure; then, Brenier’s theorem guarantees the existence of a unique Monge map, often called the <em class="ltx_emph ltx_font_italic">Brenier map</em>, given as the gradient of a convex potential <cite class="ltx_cite ltx_citemacro_citep">(Brenier, <a href="#bib.bib3" title="" class="ltx_ref">1991</a>)</cite>. More generally, existence of optimal maps is guaranteed if <span id="S1.p1.m13" class="ltx_Math">\mu</span> is absolutely continuous and uniqueness holds if further <span id="S1.p1.m14" class="ltx_Math">p&gt;1</span> (see, e.g., Section 2.4 of <cite class="ltx_cite ltx_citemacro_citep">Villani, <a href="#bib.bib45" title="" class="ltx_ref">2003</a></cite>).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">There is a rich literature on formal guarantees for estimation of Brenier maps <cite class="ltx_cite ltx_citemacro_citep">(Hütter and Rigollet, <a href="#bib.bib17" title="" class="ltx_ref">2021</a>; Pooladian and Niles-Weed, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>; Deb et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>; Manole et al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite> (see related work). However, all of these works impose stringent regularity assumptions on the density of <span id="S1.p2.m1" class="ltx_Math">\mu</span> (e.g., two-sided bounds) and/or the unique Brenier map <span id="S1.p2.m2" class="ltx_Math">T^{\star}</span> (e.g., Lipschitzness and Hölder smoothness). This is because the quality of the estimator is measured by its <span id="S1.p2.m3" class="ltx_Math">L^{p}(\mu)</span> distance from <span id="S1.p2.m4" class="ltx_Math">T^{\star}</span>, which inherently requires uniqueness (otherwise, the <span id="S1.p2.m5" class="ltx_Math">L^{p}(\mu)</span> metric is meaningless) and hinges on said regularity assumptions to obtain quantitative error bounds. However, such regularity assumptions are often impossible to verify in practice. Worse yet, many real-world applications violate the conditions of Brenier’s theorem, whence deterministic Monge maps may not exist, and optimal transportation strategies require stochasticity. For instance, this is the case in domain adaptation whenever the source distribution lies on a lower-dimensional manifold than the target <cite class="ltx_cite ltx_citemacro_citep">(Courty et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>; Redko et al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>, such as in text-to-image or sketch-to-photo translation. Similarly, single-cell developmental trajectories branch over time, so any measure-preserving map from an early snapshot to a later one must be stochastic <cite class="ltx_cite ltx_citemacro_citep">(Schiebinger et al., <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>. As such scenarios far exceed the account of current OT map estimation theory, this work sets out to close this gap by providing a broadly applicable estimation framework that offers strong recovery guarantees for a breadth of applications.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>New Framework for Stochastic OT Map Estimation and Contributions</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p class="ltx_p">The Kantorovich OT problem <cite class="ltx_cite ltx_citemacro_citep">(Kantorovich, <a href="#bib.bib18" title="" class="ltx_ref">1942</a>)</cite> relaxes that of Monge by allowing stochastic maps. Reparametrizing the standard formulation via couplings in terms of Markov kernels, it reads as</p>
</div>
<figure id="S1.F1" class="ltx_figure ltx_align_floatright"><img src="" id="S1.F1.g1" class="ltx_graphics" alt="Previous map estimation theory only accounts for the inner circle, despite many OT map applications lying outside it. The proposed estimation framework under ">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Previous map estimation theory only accounts for the inner circle, despite many OT map applications lying outside it. The proposed estimation framework under <span id="S1.F1.m3" class="ltx_Math">\cE_{p}</span> covers all possible <span id="S1.F1.m4" class="ltx_Math">\Wp</span> problems (subject to tail bounds for quantitative rates).
</figcaption>
</figure>
<div id="S1.SS1.p2" class="ltx_para">
<table id="S1.E2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S1.E2.m1" class="ltx_Math">\Wp(\mu,\nu)=\min_{\begin{subarray}{c}\kappa\in\cK(\cX,\cY)\\
\kappa_{\sharp}\mu=\nu\end{subarray}}\left(\iint\|x-y\|^{p}\dd\kappa(y|x)\dd%
\mu(x)\right)^{\frac{1}{p}},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr>
</table>
<p class="ltx_p">where <span id="S1.SS1.p2.m1" class="ltx_Math">\kappa(\cdot|\cdot)</span> varies over Markov kernels (regular conditional probability distributions) from <span id="S1.SS1.p2.m2" class="ltx_Math">\cX</span> to <span id="S1.SS1.p2.m3" class="ltx_Math">\cY</span> and <span id="S1.SS1.p2.m4" class="ltx_Math">\kappa_{\sharp}\mu</span> denotes the pushforward measure <span id="S1.SS1.p2.m5" class="ltx_Math">\int\kappa(\cdot|x)\dd\mu(x)</span>.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The standard Kantorovich OT problem optimizes the cost over couplings <span id="footnote1.m1" class="ltx_Math">\pi\in\Pi(\mu,\nu)</span>, but the disintegration theorem yields that each such coupling can be decomposed as <span id="footnote1.m2" class="ltx_Math">\dd\pi(x,y)=\dd\mu(x)\dd\pi(y|x)</span>, where <span id="footnote1.m3" class="ltx_Math">\pi(\cdot|\cdot)</span> is a Markov kernel induced by conditioning on the left argument. When a coupling is induced by a deterministic map <span id="footnote1.m4" class="ltx_Math">T</span>, i.e., <span id="footnote1.m5" class="ltx_Math">\pi=(\mathrm{Id},T)_{\sharp}\mu</span>, the corresponding kernel <span id="footnote1.m6" class="ltx_Math">\kappa</span> is given by <span id="footnote1.m7" class="ltx_Math">\kappa_{x}=\delta_{T(x)}</span>.</span></span></span> We propose a novel framework for stochastic OT map estimation by furnishing a suitable error metric. For source distribution <span id="S1.SS1.p2.m6" class="ltx_Math">\mu\in\cP(\cX)</span>, target distribution <span id="S1.SS1.p2.m7" class="ltx_Math">\nu\in\cP(\cY)</span>, and kernel <span id="S1.SS1.p2.m8" class="ltx_Math">\kappa</span> from <span id="S1.SS1.p2.m9" class="ltx_Math">\cX</span> to <span id="S1.SS1.p2.m10" class="ltx_Math">\cY</span>, we define the <em class="ltx_emph ltx_font_italic">transportation error</em> <span id="S1.SS1.p2.m11" class="ltx_Math">\cE_{p}(\kappa;\mu,\nu)</span> of <span id="S1.SS1.p2.m12" class="ltx_Math">\kappa</span> for the <span id="S1.SS1.p2.m13" class="ltx_Math">\Wp(\mu,\nu)</span> problem by</p>
<table id="A5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S1.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S1.Ex1.m1" class="ltx_Math">\displaystyle\underbrace{\biggl{[}\!\left(\iint\!\|x\!-\!y\|^{p}\dd\kappa_{x}(%
y)\dd\mu(x)\!\right)^{\frac{1}{p}}\!\!-\,\Wp(\mu,\nu)\biggr{]}_{+}}_{\text{%
optimality gap}}\!+\underbrace{\vphantom{\biggr{]}_{+}}\Wp(\kappa_{\sharp}\mu,%
\nu)}_{\text{feasibility gap}},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <span id="S1.SS1.p2.m14" class="ltx_Math">[c]_{+}\!\defeq\!\max\{c,0\}</span> and <span id="S1.SS1.p2.m15" class="ltx_Math">\kappa_{x}(\cdot)\defeq\kappa(\cdot|x)</span>. Under <span id="S1.SS1.p2.m16" class="ltx_Math">\cE_{p}</span>, the quality of <span id="S1.SS1.p2.m17" class="ltx_Math">\kappa</span> is thus measured by its transportation cost overhead on top of the optimum <span id="S1.SS1.p2.m18" class="ltx_Math">\Wp(\mu,\nu)</span> (dubbed <em class="ltx_emph ltx_font_italic">optimality gap</em>), plus its <span id="S1.SS1.p2.m19" class="ltx_Math">p</span>-Wasserstein gap from matching the target <span id="S1.SS1.p2.m20" class="ltx_Math">\nu</span> (the <em class="ltx_emph ltx_font_italic">feasibility gap</em>). While the <span id="S1.SS1.p2.m21" class="ltx_Math">\cE_{p}</span> error metric naturally accounts for deterministic OT maps, it does not require uniqueness or even existence thereof. This enables treating OT map estimation settings far beyond those accounted by existing theory, as illustrated in <a href="#S1.F1" title="Figure 1 ‣ 1.1 New Framework for Stochastic OT Map Estimation and Contributions ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to the right. Remarkably, beyond the broad coverage of the proposed framework, quantitative bounds on <span id="S1.SS1.p2.m22" class="ltx_Math">\cE_{p}</span> can be derived under minimal and easy-to-verify assumptions, rendering the guarantees applicable in practice.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p class="ltx_p">Our technical contributions build upon a foundation of stability lemmas for <span id="S1.SS1.p3.m1" class="ltx_Math">\cE_{p}</span> established in <a href="#S2" title="2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. These characterize how <span id="S1.SS1.p3.m2" class="ltx_Math">\cE_{p}</span> responds to TV and Wasserstein perturbations of the input measures and to compositions of the kernel. In <a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we apply these to finite-sample estimation and computation. Here, our strongest result holds when <span id="S1.SS1.p3.m3" class="ltx_Math">\nu</span> is sub-Gaussian and <span id="S1.SS1.p3.m4" class="ltx_Math">\mu</span> has bounded <span id="S1.SS1.p3.m5" class="ltx_Math">2p</span>th moments, but assuming no regularity of an optimal kernel. For i.i.d. samples <span id="S1.SS1.p3.m6" class="ltx_Math">X_{1},\dots,X_{n}\sim\mu</span> and <span id="S1.SS1.p3.m7" class="ltx_Math">Y_{1},\dots,Y_{n}\sim\nu</span>, we present a rounding-based estimator <span id="S1.SS1.p3.m8" class="ltx_Math">\hat{\kappa}_{n}</span> which achieves
<span id="S1.SS1.p3.m9" class="ltx_Math">\E[\cE_{p}(\hat{\kappa}_{n};\mu,\nu)]=\widetilde{O}_{p,d}\bigl{(}n^{-1/(d+2p)}%
\bigr{)}</span>, with running time <span id="S1.SS1.p3.m10" class="ltx_Math">O(n^{2+o_{d}(1)})</span> dominated by a single, low-accuracy call to an entropic OT solver. We also observe a minimax lower bound of <span id="S1.SS1.p3.m11" class="ltx_Math">\Omega(n^{-1/(d\lor 2p)})</span>, showing that our rate is near-optimal.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p class="ltx_p">In <a href="#S4" title="4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we examine the statistical landscape of estimation when there exists a Hölder continuous optimal kernel, a condition that is still significantly weaker than typical assumptions for Brenier map estimation when <span id="S1.SS1.p4.m1" class="ltx_Math">p=2</span>. In particular, for the case of a Lipschitz optimal kernel <span id="S1.SS1.p4.m2" class="ltx_Math">\kappa_{\star}</span>, i.e., when <span id="S1.SS1.p4.m3" class="ltx_Math">\Wp((\kappa_{\star})_{x},(\kappa_{\star})_{x^{\prime}})\lesssim\|x-x^{\prime}\|</span> for all <span id="S1.SS1.p4.m4" class="ltx_Math">x,x^{\prime}\in\cX</span>, we show that kernel estimation under <span id="S1.SS1.p4.m5" class="ltx_Math">\cE_{p}</span> has the same statistical complexity as estimating <span id="S1.SS1.p4.m6" class="ltx_Math">\mu</span> and <span id="S1.SS1.p4.m7" class="ltx_Math">\nu</span> under <span id="S1.SS1.p4.m8" class="ltx_Math">\Wp</span>, with rate <span id="S1.SS1.p4.m9" class="ltx_Math">\widetilde{O}(n^{-1/(d\lor 2p)})</span>. For this reduction, we employ an estimator based on Wasserstein distributionally robust optimization.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p class="ltx_p">In <a href="#S5" title="5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we show that effective kernel estimation is possible in the presence of adversarial data contamination. Historically, robust statistics has been well-studied under Huber’s <span id="S1.SS1.p5.m1" class="ltx_Math">\eps</span>-contamination model for global outliers, which is subsumed by TV <span id="S1.SS1.p5.m2" class="ltx_Math">\eps</span>-corruptions of the input data <cite class="ltx_cite ltx_citemacro_citep">(Huber, <a href="#bib.bib16" title="" class="ltx_ref">1964</a>)</cite>. More recently, statisticians have examined robust estimation under localized Wasserstein corruptions of the input samples <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>; Chao and Dobriban, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Liu and Loh, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>. We consider a strong corruption model where the clean samples can be corrupted both in TV and under the Wasserstein metric. This combination of local and global corruptions only has only explored recently <cite class="ltx_cite ltx_citemacro_citep">(Nietert et al., <a href="#bib.bib31" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib32" title="" class="ltx_ref">2024</a>; Pittas and Pensia, <a href="#bib.bib33" title="" class="ltx_ref">2025</a>)</cite>, and their interaction has required careful analysis.
Here, stability of <span id="S1.SS1.p5.m3" class="ltx_Math">\cE_{p}</span> enables us to cleanly decouple the two corruption types. Against an adversary with TV budget <span id="S1.SS1.p5.m4" class="ltx_Math">\eps</span> and <span id="S1.SS1.p5.m5" class="ltx_Math">\Wp</span> budget <span id="S1.SS1.p5.m6" class="ltx_Math">\rho</span>, we show that a convolutional estimator achieves error <span id="S1.SS1.p5.m7" class="ltx_Math">\sqrt{d}\eps^{1/p}+\sqrt{d}\rho^{1/(p+1)}+O_{p,d}(n^{-1/(d+2p)})</span>. An accompanying minimax lower bound of <span id="S1.SS1.p5.m8" class="ltx_Math">\sqrt{d}\eps^{1/p}+d^{1/4}\rho^{1/2}+n^{-1/(d\lor 2p)}</span> implies a separation between robust map estimation under <span id="S1.SS1.p5.m9" class="ltx_Math">\cE_{p}</span> and robust distribution estimation under <span id="S1.SS1.p5.m10" class="ltx_Math">\Wp</span>, where one can achieve linear dependence on <span id="S1.SS1.p5.m11" class="ltx_Math">\rho</span>.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p class="ltx_p">In <a href="#S6" title="6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> , we validate our theory with numerical simulations for two settings with irregular OT maps that are poorly suited for existing theory. These showcase the performance of our rounding estimator and the benefits of <span id="S1.SS1.p6.m1" class="ltx_Math">\cE_{p}</span> over <span id="S1.SS1.p6.m2" class="ltx_Math">L^{p}</span>. Overall, our results constitute a general-purpose theory for (possibly stochastic) OT map estimation, subject to minimal primitive assumptions. As such, it is capable of providing formal performance guarantees in the <span id="S1.SS1.p6.m3" class="ltx_Math">\cE_{p}</span> sense in various practically relevant settings.</p>
</div>
<section id="S1.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Related work.</h4>

<div id="S1.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Most related to this paper is a line of statistics work on the minimax sample complexity of Brenier map estimation when <span id="S1.SS1.SSS0.Px1.p1.m1" class="ltx_Math">p=2</span>, initiated by <cite class="ltx_cite ltx_citemacro_cite">Hütter and Rigollet (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. Under density assumptions on <span id="S1.SS1.SSS0.Px1.p1.m2" class="ltx_Math">\mu</span> and smoothness conditions on the unique Brenier map <span id="S1.SS1.SSS0.Px1.p1.m3" class="ltx_Math">T^{\star}</span> (in particular, Lipschitzness), they obtain near-optimal risk bounds of the form <span id="S1.SS1.SSS0.Px1.p1.m4" class="ltx_Math">\smash{\|\hat{T}-T^{\star}\|_{L^{2}(\mu)}}=\widetilde{O}(n^{-1/d})</span>, using empirical risk minimization for a semi-dual objective.
The myriad of follow-ups include <cite class="ltx_cite ltx_citemacro_cite">Pooladian and Niles-Weed (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>); Deb et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Manole et al. (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>, all of which impose density and smoothness assumptions. <cite class="ltx_cite ltx_citemacro_cite">Pooladian et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> considered the semi-discrete setting where the Brenier map is piecewise constant, employing an estimator based on entropic OT (EOT).
Recently, <cite class="ltx_cite ltx_citemacro_cite">Balakrishnan and Manole (<a href="#bib.bib1" title="" class="ltx_ref">2025</a>)</cite> provided refined guarantees that sidestep the typical density assumptions, but they still rely on the Brenier map being the gradient of a sufficiently regular convex potential. Lastly, a variety of neural map estimators have been developed by the machine learning community <cite class="ltx_cite ltx_citemacro_citep">(Seguy et al., <a href="#bib.bib40" title="" class="ltx_ref">2018</a>; Meng et al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>; Wang and Goldfeld, <a href="#bib.bib46" title="" class="ltx_ref">2024</a>)</cite>, with applications to domain adaptation, style transfer, trajectory estimation, and the like.</p>
</div>
<div id="S1.SS1.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p">Two recent approaches for neural map estimation warrant further discussion. First is the Monge gap regularizer of <cite class="ltx_cite ltx_citemacro_cite">Uscidda and Cuturi (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. For the <span id="S1.SS1.SSS0.Px1.p2.m1" class="ltx_Math">p</span>-Wasserstein cost, this work proposes training a deterministic map estimator to minimize the objective <span id="S1.SS1.SSS0.Px1.p2.m2" class="ltx_Math">\cJ_{p}(T;\mu,\nu)=\cM_{p}(T;\mu)+\mathsf{D}(T_{\sharp}\mu,\nu)</span>, where <span id="S1.SS1.SSS0.Px1.p2.m3" class="ltx_Math">\mathsf{D}</span> is a statistical divergence and the <em class="ltx_emph ltx_font_italic">Monge gap</em> <span id="S1.SS1.SSS0.Px1.p2.m4" class="ltx_Math">\cM_{p}</span> is defined by</p>
<table id="S1.E3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S1.E3.m1" class="ltx_Math">\cM_{p}(T;\mu)\defeq\int\|x-T(x)\|^{p}\,\dd\mu(x)-\Wp(\mu,T_{\sharp}\mu)^{p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr>
</table>
<p class="ltx_p">They show that <span id="S1.SS1.SSS0.Px1.p2.m5" class="ltx_Math">\cM_{p}\geq 0</span> with equality if and only if <span id="S1.SS1.SSS0.Px1.p2.m6" class="ltx_Math">T</span> is <span id="S1.SS1.SSS0.Px1.p2.m7" class="ltx_Math">c</span>-cyclically monotone over <span id="S1.SS1.SSS0.Px1.p2.m8" class="ltx_Math">\supp(\mu)</span>. Consequently, <span id="S1.SS1.SSS0.Px1.p2.m9" class="ltx_Math">\cJ_{p}</span> nullifies exactly when <span id="S1.SS1.SSS0.Px1.p2.m10" class="ltx_Math">T</span> is optimal for the <span id="S1.SS1.SSS0.Px1.p2.m11" class="ltx_Math">\Wp(\mu,\nu)</span> problem. The statistical analysis of that work accounts for consistency, under the assumption that a deterministic and continuous optimal map exists. In practice, they suggest taking <span id="S1.SS1.SSS0.Px1.p2.m12" class="ltx_Math">\mathsf{D}</span> as an EOT cost, estimating <span id="S1.SS1.SSS0.Px1.p2.m13" class="ltx_Math">\Wp</span> with EOT, and substituting <span id="S1.SS1.SSS0.Px1.p2.m14" class="ltx_Math">\mu</span> and <span id="S1.SS1.SSS0.Px1.p2.m15" class="ltx_Math">\nu</span> with their empirical measures. Parameterizing <span id="S1.SS1.SSS0.Px1.p2.m16" class="ltx_Math">T</span> via a multilayer perceptron, they achieve competitive empirical performance on a range of map estimation tasks. As we will show in <a href="#S2" title="2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span id="S1.SS1.SSS0.Px1.p2.m17" class="ltx_Math">\cE_{p}</span> and <span id="S1.SS1.SSS0.Px1.p2.m18" class="ltx_Math">\cJ_{p}</span> are very connected; in particular, they coincide up to constant factors when <span id="S1.SS1.SSS0.Px1.p2.m19" class="ltx_Math">p=1</span> and <span id="S1.SS1.SSS0.Px1.p2.m20" class="ltx_Math">\Delta=\Wone</span>. We view <span id="S1.SS1.SSS0.Px1.p2.m21" class="ltx_Math">\cE_{p}</span> as better suited for quantitative statistical analysis, enabling rates which seem difficult to prove under <span id="S1.SS1.SSS0.Px1.p2.m22" class="ltx_Math">\cJ_{p}</span> for general <span id="S1.SS1.SSS0.Px1.p2.m23" class="ltx_Math">p</span> (and we are unaware of any existing rates proven under <span id="S1.SS1.SSS0.Px1.p2.m24" class="ltx_Math">\cJ_{p}</span>). On the other hand, as discussed in <a href="#S6" title="6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we find that <span id="S1.SS1.SSS0.Px1.p2.m25" class="ltx_Math">\cJ_{p}</span> is better suited for neural implementation, since its gradients seem to carry a stronger signal when far from optimality.
</p>
</div>
<div id="S1.SS1.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p">Lastly, there is an existing line of work on the design of neural estimators for stochastic OT maps <cite class="ltx_cite ltx_citemacro_citep">(Korotin et al., <a href="#bib.bib20" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib21" title="" class="ltx_ref">b</a>)</cite>. They show that any optimal kernel is the solution of a certain maximin problem, which they approximately solve via a neural net parameterization and stochastic gradient ascent-descent. However, this maximin problem sometimes admits spurious solutions associated with suboptimal maps. In general, these are more empirical works which do not address statistical rates.</p>
</div>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Preliminaries</h3>

<section id="S1.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Notation.</h4>

<div id="S1.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Let <span id="S1.SS2.SSS0.Px1.p1.m1" class="ltx_Math">\|\cdot\|</span> denote the Euclidean norm on <span id="S1.SS2.SSS0.Px1.p1.m2" class="ltx_Math">\R^{d}</span>, and <span id="S1.SS2.SSS0.Px1.p1.m3" class="ltx_Math">\mathbb{B}^{d}</span> be the <span id="S1.SS2.SSS0.Px1.p1.m4" class="ltx_Math">d</span>-dimensional unit ball. For measurable <span id="S1.SS2.SSS0.Px1.p1.m5" class="ltx_Math">S\subseteq\R^{d}</span>, write <span id="S1.SS2.SSS0.Px1.p1.m6" class="ltx_Math">\cP(S)</span> for the space of probability measures over <span id="S1.SS2.SSS0.Px1.p1.m7" class="ltx_Math">S</span> and <span id="S1.SS2.SSS0.Px1.p1.m8" class="ltx_Math">\diam(S)</span> for its diameter. Let <span id="S1.SS2.SSS0.Px1.p1.m9" class="ltx_Math">\cP_{q}(S)</span> denote those with finite <span id="S1.SS2.SSS0.Px1.p1.m10" class="ltx_Math">q</span>th moments, and write <span id="S1.SS2.SSS0.Px1.p1.m11" class="ltx_Math">\cN(x,\Sigma)</span> for the multi-variate Gaussian distribution with mean <span id="S1.SS2.SSS0.Px1.p1.m12" class="ltx_Math">x\in\R^{d}</span> and covariance <span id="S1.SS2.SSS0.Px1.p1.m13" class="ltx_Math">\Sigma\in\R^{d\times d}</span>. We say that <span id="S1.SS2.SSS0.Px1.p1.m14" class="ltx_Math">\mu\in\cP(\R^{d})</span> is <span id="S1.SS2.SSS0.Px1.p1.m15" class="ltx_Math">\sigma^{2}</span>-sub-Gaussian if <span id="S1.SS2.SSS0.Px1.p1.m16" class="ltx_Math">\E_{\mu}[\exp(\|X\|^{2}/\sigma^{2})]\leq 2</span>. Write <span id="S1.SS2.SSS0.Px1.p1.m17" class="ltx_Math">\cM(S)</span> for the space of finite signed measures on <span id="S1.SS2.SSS0.Px1.p1.m18" class="ltx_Math">S</span>, equipped with the TV norm <span id="S1.SS2.SSS0.Px1.p1.m19" class="ltx_Math">\|\nu\|_{\tv}\defeq\frac{1}{2}|\nu|(S)</span>, and <span id="S1.SS2.SSS0.Px1.p1.m20" class="ltx_Math">\cM^{+}(S)</span> for those which are non-negative. Let <span id="S1.SS2.SSS0.Px1.p1.m21" class="ltx_Math">\hat{\mu}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{X_{i}}</span> be the empirical measure of <span id="S1.SS2.SSS0.Px1.p1.m22" class="ltx_Math">n</span> i.i.d. samples <span id="S1.SS2.SSS0.Px1.p1.m23" class="ltx_Math">X_{1},\dots,X_{n}</span> from <span id="S1.SS2.SSS0.Px1.p1.m24" class="ltx_Math">\mu</span>. We write <span id="S1.SS2.SSS0.Px1.p1.m25" class="ltx_Math">a\lor b\defeq\max\{a,b\}</span>, <span id="S1.SS2.SSS0.Px1.p1.m26" class="ltx_Math">a\land b\defeq\min\{a,b\}</span>, and <span id="S1.SS2.SSS0.Px1.p1.m27" class="ltx_Math">\lesssim_{x},\gtrsim_{x},\asymp_{x}</span> for (in)equalities up to a constant depending only on <span id="S1.SS2.SSS0.Px1.p1.m28" class="ltx_Math">x</span> (omitting <span id="S1.SS2.SSS0.Px1.p1.m29" class="ltx_Math">x</span> for absolute constants).</p>
</div>
</section>
<section id="S1.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Kernels and their composition.</h4>

<div id="S1.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">Writing <span id="S1.SS2.SSS0.Px2.p1.m1" class="ltx_Math">\cB(\cY)</span> for the Borel subsets of <span id="S1.SS2.SSS0.Px2.p1.m2" class="ltx_Math">\cY</span>, we recall that a Markov kernel <span id="S1.SS2.SSS0.Px2.p1.m3" class="ltx_Math">\kappa\in\cK(\cX,\cY)</span> is a map <span id="S1.SS2.SSS0.Px2.p1.m4" class="ltx_Math">(A,x):\cB(\cY)\times\cX\mapsto\kappa_{x}(A)\in[0,1]</span> which is measurable in <span id="S1.SS2.SSS0.Px2.p1.m5" class="ltx_Math">x</span> for fixed <span id="S1.SS2.SSS0.Px2.p1.m6" class="ltx_Math">A</span> and is a probability measure on <span id="S1.SS2.SSS0.Px2.p1.m7" class="ltx_Math">\cY</span> for fixed <span id="S1.SS2.SSS0.Px2.p1.m8" class="ltx_Math">x</span>. Consequently, given any <span id="S1.SS2.SSS0.Px2.p1.m9" class="ltx_Math">\mu\in\cP(\cX)</span>, the pushforward measure <span id="S1.SS2.SSS0.Px2.p1.m10" class="ltx_Math">\kappa_{\sharp}\mu(\cdot)\defeq\int\kappa_{x}(\cdot)\dd\mu(x)</span> is well-defined probability measure on <span id="S1.SS2.SSS0.Px2.p1.m11" class="ltx_Math">\cY</span>. Moreover, fixing any intermediate space <span id="S1.SS2.SSS0.Px2.p1.m12" class="ltx_Math">\cZ\subseteq\R^{d}</span>, kernels <span id="S1.SS2.SSS0.Px2.p1.m13" class="ltx_Math">\kappa\in\cK(\cZ,\cY)</span> and <span id="S1.SS2.SSS0.Px2.p1.m14" class="ltx_Math">\lambda\in\cK(\cX,\cZ)</span> can be composed to obtain the composite kernel <span id="S1.SS2.SSS0.Px2.p1.m15" class="ltx_Math">\kappa\circ\lambda\in\cK(\cX,\cY)</span> defined by <span id="S1.SS2.SSS0.Px2.p1.m16" class="ltx_Math">(\kappa\circ\lambda)(A|x)\defeq\int\kappa_{z}(A)\dd\lambda_{x}(z)</span>.</p>
</div>
</section>
<section id="S1.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Statistical distances and empirical convergence.</h4>

<div id="S1.SS2.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p">We often use the following standard results.</p>
</div>
<div id="Thmfact1" class="ltx_theorem ltx_theorem_fact">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Fact 1</span></span><span class="ltx_text ltx_font_bold"> </span>(<span id="Thmfact1.m1" class="ltx_Math">\Wp</span>-TV comparison)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmfact1.p1" class="ltx_para">
<p class="ltx_p">For <span id="Thmfact1.p1.m1" class="ltx_Math">\mu,\nu\in\cP(\cX)</span>, we have <span id="Thmfact1.p1.m2" class="ltx_Math">\Wp(\mu,\nu)\leq\diam(\cX)\|\mu-\nu\|_{\tv}^{1/p}</span>.</p>
</div>
</div>
<div id="Thmlemma1" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 1</span></span><span class="ltx_text ltx_font_bold"> </span>(<span id="Thmlemma1.m1" class="ltx_Math">\Wp</span> empirical convergence, <cite class="ltx_cite ltx_citemacro_citep">Lei, <a href="#bib.bib22" title="" class="ltx_ref">2020</a></cite>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">If <span id="Thmlemma1.p1.m1" class="ltx_Math">q&gt;p</span> and <span id="Thmlemma1.p1.m2" class="ltx_Math">\mu\in\cP_{q}(\R^{d})</span>, then <span id="Thmlemma1.p1.m3" class="ltx_Math">\E[\Wp(\mu,\hat{\mu}_{n})]</span><span id="Thmlemma1.p1.m4" class="ltx_Math">\lesssim_{p,q}\E_{\mu}[\|X\|^{q}]^{\frac{1}{q}}n^{-\left[\frac{1}{(2p)\lor d}%
\land\left(\frac{1}{p}-\frac{1}{q}\right)\right]}\log^{2}(n)</span>.
If <span id="Thmlemma1.p1.m5" class="ltx_Math">d&gt;q&gt;2p</span>, then <span id="Thmlemma1.p1.m6" class="ltx_Math">\E[\Wp(\mu,\hat{\mu}_{n})]\lesssim_{p,q}\E_{\mu}[\|X\|^{q}]^{\frac{1}{q}}n^{-%
\frac{1}{d}}</span>.</span></p>
</div>
</div>
</section>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Basic Properties of the Error Functional</h2>

<figure id="S2.F2" class="ltx_figure"><img src="processed_dataset/2512.09499v1/temp_source/figures/Ep_vs_Lp.png" id="S2.F2.g1" class="ltx_graphics ltx_centering" width="541" height="155" alt="Diagrams of 2 maps and a kernel for ">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Diagrams of 2 maps and a kernel for <span id="S2.F2.m21" class="ltx_Math">\Wp(\mu,\nu)</span>, where in each <span id="S2.F2.m22" class="ltx_Math">\mu</span> is uniform over the blue line connecting <span id="S2.F2.m23" class="ltx_Math">(0,0)</span> and <span id="S2.F2.m24" class="ltx_Math">(0,1)</span> and, in orange, <span id="S2.F2.m25" class="ltx_Math">\nu=T^{\star}_{\sharp}\mu</span> for <span id="S2.F2.m26" class="ltx_Math">T^{\star}(x)=((-1)^{\lfloor x_{2}/\delta\rfloor},x_{2})</span>. We depict <span id="S2.F2.m27" class="ltx_Math">T^{\star}</span> on the left, <span id="S2.F2.m28" class="ltx_Math">\smash{T(x)=(-(-1)^{\lfloor x_{2}/\delta\rfloor},x_{2})}</span> in the center, and kernel <span id="S2.F2.m29" class="ltx_Math">\kappa_{x}=\Unif(\{(-1,x_{2}),(1,x_{2})\})</span> on the right. While <span id="S2.F2.m30" class="ltx_Math">T</span> and <span id="S2.F2.m31" class="ltx_Math">\kappa</span> are far from <span id="S2.F2.m32" class="ltx_Math">T^{\star}</span> in an <span id="S2.F2.m33" class="ltx_Math">L^{p}</span> sense, they achieve <span id="S2.F2.m34" class="ltx_Math">\cE_{p}\leq\delta</span> (indeed, both <span id="S2.F2.m35" class="ltx_Math">T</span> and <span id="S2.F2.m36" class="ltx_Math">\kappa</span> achieve zero optimality gap and at most <span id="S2.F2.m37" class="ltx_Math">\delta</span> feasibility gap). Taking <span id="S2.F2.m38" class="ltx_Math">\delta\to 0</span>, <span id="S2.F2.m39" class="ltx_Math">T^{\star}</span> becomes impossible to recover from finite samples, whereas <span id="S2.F2.m40" class="ltx_Math">\kappa</span> can be estimated effectively.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Before turning to estimation, we establish some fundamental properties of our error functional</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S2.E4.m1" class="ltx_Math">\cE_{p}(\kappa;\mu,\nu)\coloneqq\biggl{[}\!\left(\iint\!\|x\!-\!y\|^{p}\dd%
\kappa_{x}(y)\dd\mu(x)\!\right)^{\frac{1}{p}}\!\!-\,\Wp(\mu,\nu)\biggr{]}_{+}%
\!+\vphantom{\biggr{]}_{+}}\Wp(\kappa_{\sharp}\mu,\nu).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr>
</table>
<p class="ltx_p">This metric is natural because it vanishes for any optimal kernel, namely, <span id="S2.p1.m1" class="ltx_Math">\cE_{p}(\kappa;\mu,\nu)=0</span> if and only if <span id="S2.p1.m2" class="ltx_Math">\kappa</span> minimizes (<a href="#S1.E2" title="(2) ‣ 1.1 New Framework for Stochastic OT Map Estimation and Contributions ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Further, it generalizes the existing <span id="S2.p1.m3" class="ltx_Math">L^{p}</span> benchmark, which applies only when an optimal deterministic map <span id="S2.p1.m4" class="ltx_Math">T^{\star}</span> exists.</p>
</div>
<div id="Thmproposition1" class="ltx_theorem ltx_theorem_proposition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Proposition 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Relation to <span id="Thmproposition1.m1" class="ltx_Math">L^{p}</span> loss)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmproposition1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For any map <span id="Thmproposition1.p1.m1" class="ltx_Math">T:\cX\to\cY</span> and <span id="Thmproposition1.p1.m2" class="ltx_Math">T^{\star}:\cX\to\cY</span> minimizing (<a href="#S1.E1" title="(1) ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), <span id="Thmproposition1.p1.m3" class="ltx_Math">\cE_{p}(T;\mu,\nu)\leq 2\|T-T^{\star}\|_{L^{p}(\mu)}</span>.</span></p>
</div>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">See <a href="#A1.SS1" title="A.1 Proof of 1 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for the proof. In <a href="#S2.F2" title="Figure 2 ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show how <span id="S2.p2.m1" class="ltx_Math">L^{p}</span> can be arbitrarily large compared to <span id="S2.p2.m2" class="ltx_Math">\cE_{p}</span>, failing to recognize the performance of map estimates that deviate pointwise from <span id="S2.p2.m3" class="ltx_Math">T^{\star}</span>.</p>
</div>
<div id="Thmremark1" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Optimality vs. feasibility)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremark1.p1" class="ltx_para">
<p class="ltx_p">An alternative version of <span id="Thmremark1.p1.m1" class="ltx_Math">\cE_{p}</span> weights the feasibility gap by a regularization strength <span id="Thmremark1.p1.m2" class="ltx_Math">\lambda</span>. When <span id="Thmremark1.p1.m3" class="ltx_Math">\lambda=0</span>, the identity map is optimal, and, as <span id="Thmremark1.p1.m4" class="ltx_Math">\lambda\to\infty</span>, the constant kernel <span id="Thmremark1.p1.m5" class="ltx_Math">\kappa(\cdot|x)\defeq\nu</span> becomes optimal. Our setting is a natural balance between these two extremes.</p>
</div>
</div>
<div id="Thmremark2" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 2</span></span><span class="ltx_text ltx_font_bold"> </span>(Reverse <span id="Thmremark2.m1" class="ltx_Math">L^{2}</span> comparison)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremark2.p1" class="ltx_para">
<p class="ltx_p">If <span id="Thmremark2.p1.m1" class="ltx_Math">p=2</span> and <span id="Thmremark2.p1.m2" class="ltx_Math">T^{\star}</span> is the gradient of an <span id="Thmremark2.p1.m3" class="ltx_Math">L</span>-smooth convex potential, we show in <a href="#A1.SS2" title="A.2 Reverse L2 comparison (2) ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a> that <span id="Thmremark2.p1.m4" class="ltx_Math">\|T-T_{\star}\|_{L^{2}(\mu)}^{2}\lesssim(L\lor 1)\,\cE_{2}(T;\mu,\nu)\bigl{(}%
\Wtwo(\mu,\nu)+\cE_{2}(T;\mu,\nu)\bigr{)}</span>.</p>
</div>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">We also compare <span id="S2.p3.m1" class="ltx_Math">\cE_{p}</span> to the Monge gap objective discussed in <a href="#S1" title="1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The proof in <a href="#A1.SS3" title="A.3 Proof of 2 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a> essentially follows by the <span id="S2.p3.m2" class="ltx_Math">\Wp</span> triangle inequality.</p>
</div>
<div id="Thmlemma2" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 2</span></span><span class="ltx_text ltx_font_bold"> </span>(Comparison to Monge gap)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For the alternative objective <span id="Thmlemma2.p1.m1" class="ltx_Math">\cE^{\prime}_{p}</span> defined by</span></p>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S2.Ex2.m1" class="ltx_Math">\cE^{\prime}_{p}(\kappa;\mu,\nu)^{p}\defeq\underbrace{\iint\|x-y\|^{p}\dd%
\kappa_{x}(y)\dd\mu(x)-\Wp(\mu,\kappa_{\sharp}\mu)^{p}}_{\text{Monge gap}}+\Wp%
(T_{\sharp}\mu,\nu)^{p},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">we have <span id="Thmlemma2.p1.m2" class="ltx_Math">\cE_{p}\leq 4\cE^{\prime}_{p}</span>. For <span id="Thmlemma2.p1.m3" class="ltx_Math">p=1</span>, we have <span id="Thmlemma2.p1.m4" class="ltx_Math">\cE^{\prime}_{1}/2\leq\cE_{1}\leq 2\cE^{\prime}_{1}</span>.</span></p>
</div>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">We now examine stability of <span id="S2.p4.m1" class="ltx_Math">\cE_{p}</span> with respect to perturbations of the source and target distributions. These stability results form the backbone for the estimation risk analysis of the estimators proposed in the subsequent sections. Their proofs in <a href="#A1" title="Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> generally follow via the <span id="S2.p4.m2" class="ltx_Math">L^{p}</span> triangle inequality.</p>
</div>
<div id="Thmlemma3" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 3</span></span><span class="ltx_text ltx_font_bold"> </span>(Stability in <span id="Thmlemma3.m1" class="ltx_Math">\nu</span>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fix <span id="Thmlemma3.p1.m1" class="ltx_Math">\mu\in\cP(\cX)</span> and <span id="Thmlemma3.p1.m2" class="ltx_Math">\nu,\nu^{\prime}\in\cP(\cY)</span>. For each <span id="Thmlemma3.p1.m3" class="ltx_Math">\kappa\in\cK(\cX,\cY)</span>, we have</span></p>
<table id="A5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S2.Ex3.m1" class="ltx_Math">\displaystyle\bigl{|}\cE_{p}(\kappa;\mu,\nu)-\cE_{p}(\kappa;\mu,\nu^{\prime})%
\bigr{|}\leq 2\,\Wp(\nu,\nu^{\prime})\leq 2\diam(\cY)\|\nu-\nu^{\prime}\|_{\tv%
}^{1/p}.\vspace{-1mm}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">The <span id="S2.p5.m1" class="ltx_Math">\cE_{p}</span> metric tolerates <span id="S2.p5.m2" class="ltx_Math">\Wp</span> perturbations of <span id="S2.p5.m3" class="ltx_Math">\mu</span> when <span id="S2.p5.m4" class="ltx_Math">\kappa</span> is appropriately Hölder continuous.</p>
</div>
<div id="Thmlemma4" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 4</span></span><span class="ltx_text ltx_font_bold"> </span>(<span id="Thmlemma4.m1" class="ltx_Math">\Wp</span> stability in <span id="Thmlemma4.m2" class="ltx_Math">\mu</span>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fix <span id="Thmlemma4.p1.m1" class="ltx_Math">\mu,\mu^{\prime}\!\in\!\cP(\cX)</span>, <span id="Thmlemma4.p1.m2" class="ltx_Math">\nu\!\in\!\cP(\cY)</span>, and <span id="Thmlemma4.p1.m3" class="ltx_Math">\kappa\!\in\!\cK(\cX,\cY)</span> with <span id="Thmlemma4.p1.m4" class="ltx_Math">\Wp(\kappa_{x},\kappa_{x^{\prime}})\!\leq\!L\|x\!-\!x^{\prime}\|^{\alpha}</span> for all <span id="Thmlemma4.p1.m5" class="ltx_Math">x,x^{\prime}\in\cX</span>, where <span id="Thmlemma4.p1.m6" class="ltx_Math">0&lt;\alpha\leq 1</span>. Setting <span id="Thmlemma4.p1.m7" class="ltx_Math">\rho=\Wp(\mu^{\prime},\mu)</span>, we have</span></p>
<table id="S2.Ex4" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S2.Ex4.m1" class="ltx_Math">|\cE_{p}(\kappa;\mu^{\prime},\nu)-\cE_{p}(\kappa;\mu,\nu)|\leq 2\rho+2L\rho^{%
\alpha}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In particular, the above holds with <span id="Thmlemma4.p1.m8" class="ltx_Math">\alpha=1</span> whenever <span id="Thmlemma4.p1.m9" class="ltx_Math">\kappa</span> is induced by a deterministic, <span id="Thmlemma4.p1.m10" class="ltx_Math">L</span>-Lipschitz map.</span></p>
</div>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">We can treat TV perturbations of the source measure when <span id="S2.p6.m1" class="ltx_Math">\cY</span> is bounded.</p>
</div>
<div id="Thmlemma5" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 5</span></span><span class="ltx_text ltx_font_bold"> </span>(TV stability in <span id="Thmlemma5.m1" class="ltx_Math">\mu</span>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma5.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fix <span id="Thmlemma5.p1.m1" class="ltx_Math">\mu,\mu^{\prime}\in\cP(\cX)</span>, <span id="Thmlemma5.p1.m2" class="ltx_Math">\nu\in\cP(\cY)</span>, and kernel <span id="Thmlemma5.p1.m3" class="ltx_Math">\kappa\in\cK(\cX,\cY)</span>. Setting <span id="Thmlemma5.p1.m4" class="ltx_Math">\eps=\|\mu-\mu^{\prime}\|_{\tv}</span>, we have</span></p>
<table id="S2.Ex5" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S2.Ex5.m1" class="ltx_Math">\cE_{p}(\kappa;\mu^{\prime},\nu)\lesssim\cE_{p}(\kappa;\mu,\nu)+\cE_{p}(\kappa%
;\mu,\nu)^{\frac{1}{p}}\Wp(\mu,\nu)^{\frac{p-1}{p}}+\diam(\cY)\eps^{1/p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In particular, we have <span id="Thmlemma5.p1.m5" class="ltx_Math">\cE_{1}(\kappa;\mu^{\prime},\nu)\lesssim\cE_{1}(\kappa;\mu,\nu)+\diam(\cY)\eps</span>.</span></p>
</div>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">That is, if <span id="S2.p7.m1" class="ltx_Math">\mu^{\prime}</span> and <span id="S2.p7.m2" class="ltx_Math">\mu</span> share substantial mass, and <span id="S2.p7.m3" class="ltx_Math">\kappa</span> performs well on <span id="S2.p7.m4" class="ltx_Math">\mu</span>, then its performance on <span id="S2.p7.m5" class="ltx_Math">\mu^{\prime}</span> under <span id="S2.p7.m6" class="ltx_Math">\cE_{p}</span> cannot be substantially worse. In particular, the proof in <a href="#A1.SS6" title="A.6 Proof of 5 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.6</span></a> decomposes <span id="S2.p7.m7" class="ltx_Math">\mu=\alpha+\beta</span>, where <span id="S2.p7.m8" class="ltx_Math">\alpha</span> is its shared mass with <span id="S2.p7.m9" class="ltx_Math">\mu^{\prime}</span>, and uses that, if <span id="S2.p7.m10" class="ltx_Math">\kappa</span> is optimal for the <span id="S2.p7.m11" class="ltx_Math">\Wp(\mu,\nu)</span> problem, then it must also be optimal for the <span id="S2.p7.m12" class="ltx_Math">\Wp(\alpha,\kappa_{\sharp}\alpha)</span> sub-problem.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p class="ltx_p">Finally, we consider the behavior of <span id="S2.p8.m1" class="ltx_Math">\cE_{p}</span> when evaluated on composite kernels, which we employ in some of the subsequent kernel estimators.</p>
</div>
<div id="Thmlemma6" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 6</span></span><span class="ltx_text ltx_font_bold"> </span>(Kernel composition)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma6.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fixing an intermediate space <span id="Thmlemma6.p1.m1" class="ltx_Math">\cZ\subseteq\R^{d}</span>, let <span id="Thmlemma6.p1.m2" class="ltx_Math">\mu\in\cP(\cX)</span>, <span id="Thmlemma6.p1.m3" class="ltx_Math">\nu\in\cP(\cY)</span>, <span id="Thmlemma6.p1.m4" class="ltx_Math">\lambda\in\cK(\cX,\cZ)</span>, and <span id="Thmlemma6.p1.m5" class="ltx_Math">\kappa\in\cK(\cZ,\cY)</span>. We then bound</span></p>
<table id="S2.Ex6" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S2.Ex6.m1" class="ltx_Math">\bigl{|}\cE_{p}(\kappa\circ\lambda;\mu,\nu)-\cE_{p}(\kappa;\lambda_{\sharp}\mu%
,\nu)\bigr{|}\leq 2\smash{\left(\iint\|z-x\|^{p}\dd\lambda_{x}(z)\dd\mu(x)%
\right)^{\frac{1}{p}}}.\vspace{1mm}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In particular, for <span id="Thmlemma6.p1.m6" class="ltx_Math">\lambda</span> induced by a deterministic map <span id="Thmlemma6.p1.m7" class="ltx_Math">f:\cX\to\cZ</span>, we have</span></p>
<table id="S2.Ex7" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S2.Ex7.m1" class="ltx_Math">\bigl{|}\cE_{p}(\kappa\circ f;\mu,\nu)-\cE_{p}(\kappa;f_{\sharp}\mu,\nu)\bigr{%
|}\leq 2\|f-\Id\|_{L^{p}(\mu)}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</div>
<div id="S2.p9" class="ltx_para">
<p class="ltx_p">This can be applied iteratively to analyze the composition of many kernels, peeling off one at a time.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Finite-Sample Estimation and Computation</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Under <span id="S3.p1.m1" class="ltx_Math">\cE_{p}</span>, we can perform kernel estimation without regularity or existence of a Brenier map. We analyze one estimator based on EOT using <span id="S3.p1.m2" class="ltx_Math">\Wp</span> stability (<a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and another based on rounding using TV stability (<a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). The former is closer to existing estimators, but the latter achieves sharper rates under milder assumptions. Fixing <span id="S3.p1.m3" class="ltx_Math">n\geq 3</span>, we have i.i.d. samples <span id="S3.p1.m4" class="ltx_Math">X_{1},\dots,X_{n}\sim\mu\in\cP(\cX)</span> and <span id="S3.p1.m5" class="ltx_Math">Y_{1},\dots,Y_{n}\sim\nu\in\cP(\cY)</span>, whose empirical measures we denote by <span id="S3.p1.m6" class="ltx_Math">\hat{\mu}_{n}</span> and <span id="S3.p1.m7" class="ltx_Math">\hat{\nu}_{n}</span> respectively.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Entropic kernel estimator.</span> As a warm-up, we recall the EOT problem, defined for <span id="S3.p2.m1" class="ltx_Math">\tau&gt;0</span> by</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S3.E5.m1" class="ltx_Math">S_{p,\tau}(\mu,\nu)\defeq\inf_{\pi\in\Pi(\mu,\nu)}\iint\|x-y\|^{p}\dd\pi(x,y)+%
\tau\mathsf{D}_{\mathrm{KL}}(\pi\|\mu\otimes\nu),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr>
</table>
<p class="ltx_p">where the Kullback-Leibler (KL) divergence is <span id="S3.p2.m2" class="ltx_Math">\mathsf{D}_{\mathrm{KL}}(\mu\|\nu)\coloneqq\int\log\bigl{(}\frac{d\mu}{d\nu}%
\bigr{)}\dd\mu</span> if <span id="S3.p2.m3" class="ltx_Math">\mu\ll\nu</span> and <span id="S3.p2.m4" class="ltx_Math">+\infty</span> otherwise. This objective is strictly convex due to the KL penalty, so a unique optimal coupling always exists (supposing that the value is finite). Most solvers for EOT use its equivalent dual formulation:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S3.E6.m1" class="ltx_Math">S_{p,\tau}(\mu,\nu)=\sup_{\begin{subarray}{c}f\in L^{1}(\mu)\\
g\in L^{1}(\nu)\end{subarray}}\int f\dd\mu+\int g\,\dd\nu-\tau\iint e^{(f(x)+g%
(x)-\|x-y\|^{p})/\tau}\dd\mu(x)\dd\nu(y)+\tau.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr>
</table>
<p class="ltx_p">The primal and dual structure of EOT is well-studied (see, e.g., <cite class="ltx_cite ltx_citemacro_citep">Cuturi, <a href="#bib.bib7" title="" class="ltx_ref">2013</a>; Genevay et al., <a href="#bib.bib11" title="" class="ltx_ref">2019</a></cite>). In particular, if <span id="S3.p2.m5" class="ltx_Math">\pi_{\tau}</span> minimizes (<a href="#S3.E5" title="(5) ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), then there exists maximizers <span id="S3.p2.m6" class="ltx_Math">f_{\tau},g_{\tau}</span> for (<a href="#S3.E6" title="(6) ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), termed <em class="ltx_emph ltx_font_italic">entropic potentials</em>, such that the conditional <em class="ltx_emph ltx_font_italic">entropic kernel</em> <span id="S3.p2.m7" class="ltx_Math">\pi_{\tau}(\cdot|x)</span> can be written as</p>
<table id="S3.E7" class="ltx_equationgroup ltx_eqn_table">

<tr id="S3.E7X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.E7X.m2" class="ltx_Math">\displaystyle\dd\pi_{\tau}(y|x)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.E7X.m3" class="ltx_Math">\displaystyle=\exp\bigl{(}(f_{\tau}(x)+g_{\tau}(y)-\|x-y\|^{p})/\tau\bigr{)}\,%
\dd\nu(y)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
<tr id="S3.E7Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.E7Xa.m2" class="ltx_Math">\displaystyle=\frac{\exp\bigl{(}(g_{\tau}(y)-\|x-y\|^{p})/\tau\bigr{)}\,\dd\nu%
(y)}{\int\exp\bigl{(}(g_{\tau}(y^{\prime})-\|x-y^{\prime}\|^{p})/\tau\bigr{)}%
\,\dd\nu(y^{\prime})}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">By this result, we may assume that the entropic kernel is defined over all <span id="S3.p2.m8" class="ltx_Math">x\in\R^{d}</span>.
We show that, if <span id="S3.p2.m9" class="ltx_Math">\diam(\cX\cup\cY)</span> is bounded, the empirical entropic kernel achieves a vanishing <span id="S3.p2.m10" class="ltx_Math">\cE_{p}</span> error.</p>
</div>
<div id="Thmtheorem1" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Entropic kernel estimator)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Assume <span id="Thmtheorem1.p1.m1" class="ltx_Math">\cX,\cY\subseteq[0,1]^{d}</span> and set <span id="Thmtheorem1.p1.m2" class="ltx_Math">\tau=d^{p/4}n^{-1/(2d\lor 4)}\log n</span>.
Let <span id="Thmtheorem1.p1.m3" class="ltx_Math">\hat{\pi}_{\tau,n}</span> be the optimal coupling for <span id="Thmtheorem1.p1.m4" class="ltx_Math">S_{p,\tau}(\hat{\mu}_{n},\hat{\nu}_{n})</span>. Then, the conditional kernel <span id="Thmtheorem1.p1.m5" class="ltx_Math">\hat{\kappa}_{n}</span> defined by <span id="Thmtheorem1.p1.m6" class="ltx_Math">(\hat{\kappa}_{n})_{x}=\pi_{\tau,n}(\cdot|x)</span> satisfies <span id="Thmtheorem1.p1.m7" class="ltx_Math">\E[\cE_{p}(\hat{\kappa}_{n};\mu,\nu)]\lesssim_{p,d}n^{-1/(2pd\lor 4p)}\log^{2}%
(n)</span>.</span></p>
</div>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">The proof in <a href="#A2.SS1" title="B.1 Proof of 1 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a> has three steps. First, we control <span id="S3.p3.m1" class="ltx_Math">\cE_{p}(\hat{\kappa}_{n};\hat{\mu}_{n},\hat{\nu}_{n})=\widetilde{O}_{d}(\tau^{%
1/p})</span> using a known bound of <cite class="ltx_cite ltx_citemacro_cite">Genevay et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>. Then, using the support constraint and the softmax form in (<a href="#S3.E7" title="(7) ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>), we bound the TV Lipschitz constant of <span id="S3.p3.m2" class="ltx_Math">\hat{\kappa}_{n}</span> by <span id="S3.p3.m3" class="ltx_Math">O_{d}(\tau^{-1})</span>, which implies the kernel is <span id="S3.p3.m4" class="ltx_Math">\Wp</span> Hölder continuous with exponent <span id="S3.p3.m5" class="ltx_Math">1/p</span> and constant <span id="S3.p3.m6" class="ltx_Math">\smash{O_{d}(\tau^{-1/p})}</span>. Finally, we apply <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_ERROR undefined">\crefpairconjunction</span><a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to bound
</p>
<table id="S3.Ex8" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S3.Ex8.m1" class="ltx_Math">\cE_{p}(\hat{\kappa}_{n};\mu,\nu)\leq\cE_{p}(\hat{\kappa}_{n};\hat{\mu}_{n},%
\hat{\nu}_{n})+O_{d}((\rho/\tau)^{1/p})+O(\rho)=\widetilde{O}_{d}(\tau^{1/p}+(%
\rho/\tau)^{1/p}+\rho),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <span id="S3.p3.m7" class="ltx_Math">\rho=\Wp(\hat{\mu}_{n},\mu)\lor\Wp(\hat{\nu}_{n},\nu)</span>. Applying <a href="#Thmlemma1" title="Lemma 1 (\Wp empirical convergence, Lei, 2020). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to bound <span id="S3.p3.m8" class="ltx_Math">\rho</span> and tuning <span id="S3.p3.m9" class="ltx_Math">\tau</span> gives the theorem. Note that <span id="S3.p3.m10" class="ltx_Math">\tau</span> controls a bias-variance trade-off (<span id="S3.p3.m11" class="ltx_Math">\tau\!\to\!0</span> overfits, while <span id="S3.p3.m12" class="ltx_Math">\tau\!\to\!\infty</span> blurs out all structure).</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">To understand the quality of this bound, we compare to Brenier map estimation with <span id="S3.p4.m1" class="ltx_Math">p=2</span>. Here, the conditional kernel is usually converted into a deterministic map <span id="S3.p4.m2" class="ltx_Math">\smash{\hat{T}_{n}}</span> via barycentric projection, which sends <span id="S3.p4.m3" class="ltx_Math">x</span> to the mean of <span id="S3.p4.m4" class="ltx_Math">Y\sim\hat{\pi}_{\tau,n}(\cdot|x)</span>. Existing work has derived a variety of <span id="S3.p4.m5" class="ltx_Math">L^{2}</span> estimation guarantees for this estimator with respect to the Brenier map <span id="S3.p4.m6" class="ltx_Math">T^{\star}</span>. In particular, <cite class="ltx_cite ltx_citemacro_cite">Pooladian and Niles-Weed (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> show that <span id="S3.p4.m7" class="ltx_Math">\E\|\hat{T}_{n}-T^{\star}\|_{L^{2}(\mu)}\lesssim_{d}n^{-(\alpha+1)/[4(d+\alpha%
+1)]}\log n</span> if <span id="S3.p4.m8" class="ltx_Math">T^{\star}\in\cC^{\alpha+1}</span> for <span id="S3.p4.m9" class="ltx_Math">1&lt;\alpha\leq 3</span> and <span id="S3.p4.m10" class="ltx_Math">\nabla T^{\star}</span> has eigenvalues bounded from above and below. For the sake of comparison, we can take a formal limit as <span id="S3.p4.m11" class="ltx_Math">\alpha\to 0</span> (although not covered by their theory) to obtain a rate of <span id="S3.p4.m12" class="ltx_Math">n^{-1/(4d+4)}</span>, which is always worse than our <span id="S3.p4.m13" class="ltx_Math">n^{-1/(4d\lor 8)}</span> rate (which does not require <span id="S3.p4.m14" class="ltx_Math">p=2</span> nor the existence of <span id="S3.p4.m15" class="ltx_Math">T^{\star}</span>).</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Improved rounding estimator.</span> While guarantees for the entropic kernel estimator from <a href="#Thmtheorem1" title="Theorem 1 (Entropic kernel estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> are compelling, we can achieve sharper rates using the following rounding estimator, via an analysis based on TV stability of <span id="S3.p5.m1" class="ltx_Math">\cE_{p}</span>. The estimator is specified by an accuracy <span id="S3.p5.m2" class="ltx_Math">\delta\geq 0</span>, a trimming radius <span id="S3.p5.m3" class="ltx_Math">R&gt;0</span>, a partition <span id="S3.p5.m4" class="ltx_Math">\cP</span> of <span id="S3.p5.m5" class="ltx_Math">\R^{d}</span>, and a collection of centers <span id="S3.p5.m6" class="ltx_Math">C_{\cP}=\{c_{P}\}_{P\in\cP}</span> such that each <span id="S3.p5.m7" class="ltx_Math">c_{P}\in P</span>. This induces a rounding function <span id="S3.p5.m8" class="ltx_Math">r_{\cP}:\R^{d}\to C_{\cP}</span> which, for each <span id="S3.p5.m9" class="ltx_Math">P\in\cP</span>, maps <span id="S3.p5.m10" class="ltx_Math">x\in P</span> to <span id="S3.p5.m11" class="ltx_Math">c_{P}</span>. Given empirical measures <span id="S3.p5.m12" class="ltx_Math">\hat{\mu}_{n}</span> and <span id="S3.p5.m13" class="ltx_Math">\hat{\nu}_{n}</span>, we proceed as follows:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p">Round <span id="S3.I1.i1.p1.m1" class="ltx_Math">\hat{\mu}_{n}</span> onto <span id="S3.I1.i1.p1.m2" class="ltx_Math">\cP</span>, taking <span id="S3.I1.i1.p1.m3" class="ltx_Math">\mu^{\prime}_{n}=(r_{\cP})_{\sharp}\hat{\mu}_{n}</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p">Compute a preliminary kernel <span id="S3.I1.i2.p1.m1" class="ltx_Math">\bar{\kappa}_{n}\in\cK(C_{\cP},\supp(\hat{\nu}_{n}))</span> which pushes <span id="S3.I1.i2.p1.m2" class="ltx_Math">\mu^{\prime}_{n}</span> onto <span id="S3.I1.i2.p1.m3" class="ltx_Math">\hat{\nu}_{n}</span> and is near-optimal for the <span id="S3.I1.i2.p1.m4" class="ltx_Math">\Wp</span> problem, satisfying <span id="S3.I1.i2.p1.m5" class="ltx_Math">\iint\|x-y\|^{p}\dd\bar{\kappa}_{n}(y|x)\dd\mu^{\prime}_{n}(x)\leq\Wp(\mu^{%
\prime}_{n},\hat{\nu}_{n})^{p}+\delta</span>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p">Return kernel <span id="S3.I1.i3.p1.m1" class="ltx_Math">\hat{\kappa}_{n}=\bar{\kappa}_{n}\circ r_{\cP}</span>, which, given <span id="S3.I1.i3.p1.m2" class="ltx_Math">x\in\R^{d}</span>, rounds it to <span id="S3.I1.i3.p1.m3" class="ltx_Math">r_{\cP}(x)</span> before applying <span id="S3.I1.i3.p1.m4" class="ltx_Math">\bar{\kappa}_{n}</span>.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">For a simple choice of <span id="S3.p6.m1" class="ltx_Math">\cP</span>, this procedure achieves low <span id="S3.p6.m2" class="ltx_Math">\cE_{p}</span> error when <span id="S3.p6.m3" class="ltx_Math">\mu</span> and <span id="S3.p6.m4" class="ltx_Math">\nu</span> are sub-Gaussian. With a more complex partition, we can support <span id="S3.p6.m5" class="ltx_Math">\mu</span> with only bounded <span id="S3.p6.m6" class="ltx_Math">2p</span>th moments.</p>
</div>
<div id="Thmtheorem2" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 2</span></span><span class="ltx_text ltx_font_bold"> </span>(Rounding estimator)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <span id="Thmtheorem2.p1.m1" class="ltx_Math">\mu,\nu</span> be <span id="Thmtheorem2.p1.m2" class="ltx_Math">1</span>-sub-Gaussian, and take <span id="Thmtheorem2.p1.m3" class="ltx_Math">\cP</span> as the regular partition of <span id="Thmtheorem2.p1.m4" class="ltx_Math">\R^{d}</span> into cubes of side length <span id="Thmtheorem2.p1.m5" class="ltx_Math">r</span>. Then, for <span id="Thmtheorem2.p1.m6" class="ltx_Math">R</span>, <span id="Thmtheorem2.p1.m7" class="ltx_Math">r</span>, and <span id="Thmtheorem2.p1.m8" class="ltx_Math">\delta</span> tuned independently of <span id="Thmtheorem2.p1.m9" class="ltx_Math">\mu</span> and <span id="Thmtheorem2.p1.m10" class="ltx_Math">\nu</span>, we have <span id="Thmtheorem2.p1.m11" class="ltx_Math">\E[\cE_{p}(\hat{\kappa}_{n};\mu,\nu)]=\widetilde{O}_{p,d}\bigl{(}n^{-1/(d+2p)}%
\bigr{)}</span>. For an alternative, non-uniform partition, this guarantee still holds if the sub-Gaussianity assumption on <span id="Thmtheorem2.p1.m12" class="ltx_Math">\mu</span> is relaxed to <span id="Thmtheorem2.p1.m13" class="ltx_Math">\E_{\mu}[\|X\|^{2p}]\leq 1</span>. In both cases, computation is dominated by Step 2 which, if implemented via an EOT solver, runs in time <span id="Thmtheorem2.p1.m14" class="ltx_Math">O((C_{\infty}+d)n^{2+o_{d}(1)})</span>, where <span id="Thmtheorem2.p1.m15" class="ltx_Math">C_{\infty}=\max_{i,j}\|X_{i}-Y_{j}\|</span>.</span></p>
</div>
</div>
<div id="S3.p7" class="ltx_para">
<p class="ltx_p">This improved <span id="S3.p7.m1" class="ltx_Math">n^{-1/(d+2p)}</span> rate is near-optimal; indeed, when <span id="S3.p7.m2" class="ltx_Math">\mu</span> is a point mass, the problem reduces to estimation of <span id="S3.p7.m3" class="ltx_Math">\nu</span> under <span id="S3.p7.m4" class="ltx_Math">\Wp</span>, for which there are existing minimax lower bounds of order <span id="S3.p7.m5" class="ltx_Math">n^{-1/(d\lor 2p)}</span> <cite class="ltx_cite ltx_citemacro_citep">(Singh and Póczos, <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite> (see Appendix <a href="#A2.SS2" title="B.2 Minimax Lower Bound under Sampling ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> for full details). We also note that the tail bounds on <span id="S3.p7.m6" class="ltx_Math">\mu</span> and <span id="S3.p7.m7" class="ltx_Math">\nu</span> can be weakened further under our analysis, but not without worsening the rate.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p class="ltx_p">We sketch the proof when <span id="S3.p8.m1" class="ltx_Math">\mu</span> is sub-Gaussian, <span id="S3.p8.m2" class="ltx_Math">\delta=0</span>, and <span id="S3.p8.m3" class="ltx_Math">\diam(\cY)\leq 1</span>; see Appendix <a href="#A2.SS3" title="B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a> for the full derivation. We first show, for the cubic partition <span id="S3.p8.m4" class="ltx_Math">\cP</span> with side length <span id="S3.p8.m5" class="ltx_Math">r</span>, that <span id="S3.p8.m6" class="ltx_Math">\mu^{\prime}_{n}=(r_{\cP})_{\sharp}\hat{\mu}_{n}</span> and <span id="S3.p8.m7" class="ltx_Math">\mu^{\prime}=(r_{\cP})_{\sharp}\mu</span> converge in TV at rate <span id="S3.p8.m8" class="ltx_Math">\sqrt{r^{-d}/n}</span>. Here, <span id="S3.p8.m9" class="ltx_Math">r^{-d}</span> arises as a bound on the number of relevant partition blocks. We then bound</p>
<table id="A5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.Ex9.m1" class="ltx_Math">\displaystyle\cE_{p}(\hat{\kappa}_{n};\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.Ex9.m2" class="ltx_Math">\displaystyle=\cE_{p}(\bar{\kappa}_{n}\circ r_{\cP};\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.Ex10.m1" class="ltx_Math">\displaystyle\leq\cE_{p}(\bar{\kappa}_{n};\mu^{\prime},\nu)+\sqrt{d}r</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>)</span></td>
</tr></tbody>
<tbody id="S3.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S3.Ex11.m1" class="ltx_Math">\displaystyle\lesssim\Wp(\nu,\hat{\nu}_{n})+(nr^{d})^{-\frac{1}{2p}}+\sqrt{d}r</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_ERROR undefined">\crefpairconjunction</span><a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Applying <a href="#Thmlemma1" title="Lemma 1 (\Wp empirical convergence, Lei, 2020). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and tuning <span id="S3.p8.m10" class="ltx_Math">r</span> gives the theorem. The general sub-Gaussian case follows by a similar argument. If <span id="S3.p8.m11" class="ltx_Math">\mu</span> only has bounded <span id="S3.p8.m12" class="ltx_Math">2p</span>th moments, we can still achieve TV convergence at a comparable rate by employing a partition whose bins increase in size away from the origin.</p>
</div>
<div id="Thmremark3" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 3</span></span><span class="ltx_text ltx_font_bold"> </span>(One-dimensional refinements)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremark3.p1" class="ltx_para">
<p class="ltx_p">Given the gap between our <span id="Thmremark3.p1.m1" class="ltx_Math">n^{-1/(d+2p)}</span> upper bound and the <span id="Thmremark3.p1.m2" class="ltx_Math">n^{-1/(d\lor 2p)}</span> lower bound of <cite class="ltx_cite ltx_citemacro_cite">Singh and Póczos (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>, it is natural to ask if the lower bound can be improved. At least when <span id="Thmremark3.p1.m3" class="ltx_Math">d=1</span>, this is impossible. In <a href="#A2.SS4" title="B.4 One-Dimensional Refinements (3) ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4</span></a>, we improve the rate from <a href="#Thmtheorem2" title="Theorem 2 (Rounding estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to <span id="Thmremark3.p1.m4" class="ltx_Math">n^{-1/2}</span> when <span id="Thmremark3.p1.m5" class="ltx_Math">d=1</span>, using stability of <span id="Thmremark3.p1.m6" class="ltx_Math">\cE_{p}</span> under the Kolmogorov-Smirnov metric.</p>
</div>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Improved Statistical Guarantees with Hölder Continuous Optimal Kernels</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">For <span id="S4.p1.m1" class="ltx_Math">p=2</span>, many existing works assume the existence of a Brenier map <span id="S4.p1.m2" class="ltx_Math">T^{\star}</span> whose gradient has eigenvalues bounded from above and below (in particular, such a <span id="S4.p1.m3" class="ltx_Math">T^{\star}</span> is Lipschitz). For example, <cite class="ltx_cite ltx_citemacro_cite">Balakrishnan and Manole (<a href="#bib.bib1" title="" class="ltx_ref">2025</a>)</cite> show that a nearest-neighbor estimator achieves <span id="S4.p1.m4" class="ltx_Math">\E\|\hat{T}_{n}-T^{\star}\|_{L^{2}(\mu)}=\smash{\widetilde{O}(n^{-1/(d\lor 4)})}</span>, matching the <span id="S4.p1.m5" class="ltx_Math">\Wtwo</span> empirical convergence rate. Of course, by <a href="#Thmproposition1" title="Proposition 1 (Relation to Lp loss). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this guarantee also holds under <span id="S4.p1.m6" class="ltx_Math">\cE_{2}</span>. In this section, we treat general <span id="S4.p1.m7" class="ltx_Math">p\geq 1</span> under the related but distinctly weaker assumption that <span id="S4.p1.m8" class="ltx_Math">\Wp(\mu,\nu)</span> admits an optimal kernel which is Hölder continuous under <span id="S4.p1.m9" class="ltx_Math">\Wp</span>.</p>
</div>
<div id="Thmassumption1" class="ltx_theorem ltx_theorem_assumption">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Assumption 1</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmassumption1.p1" class="ltx_para">
<p class="ltx_p">There exists <span id="Thmassumption1.p1.m1" class="ltx_Math">L\geq 1</span>, <span id="Thmassumption1.p1.m2" class="ltx_Math">\alpha\in(0,1]</span>, and an optimal kernel <span id="Thmassumption1.p1.m3" class="ltx_Math">\kappa^{\star}\in\cK(\cX,\cY)</span> for <span id="Thmassumption1.p1.m4" class="ltx_Math">\Wp(\mu,\nu)</span> such that <span id="Thmassumption1.p1.m5" class="ltx_Math">\Wp(\kappa^{\star}_{x},\kappa^{\star}_{x^{\prime}})\leq L\|x-x^{\prime}\|^{\alpha}</span> for all <span id="Thmassumption1.p1.m6" class="ltx_Math">x,x^{\prime}\in\cX</span>.</p>
</div>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">Here, we propose an estimator based on Wasserstein distributionally robust optimization (WDRO):</p>
<table id="S4.Ex12" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S4.Ex12.m1" class="ltx_Math">\hat{\kappa}^{\rho}_{\mathrm{DRO}}[\hat{\mu},\hat{\nu}]\defeq\argmin_{\kappa%
\in\cK(\R^{d},\cY)}\sup_{\begin{subarray}{c}\mu^{\prime}\in\cP(\R^{d}):\,\Wp(%
\mu^{\prime},\hat{\mu})\leq\rho\end{subarray}}\cE_{p}(\kappa;\mu^{\prime},\hat%
{\nu}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <span id="S4.p2.m1" class="ltx_Math">\hat{\mu},\hat{\nu}</span> are any proxies for <span id="S4.p2.m2" class="ltx_Math">\mu,\nu</span> (potentially their empirical measures).
This estimator is computationally inefficient but allows us to better understand the statistical limits of estimation.</p>
</div>
<div id="Thmtheorem3" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 3</span></span><span class="ltx_text ltx_font_bold"> </span>(WDRO estimator)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Under Assumption <a href="#Thmassumption1" title="Assumption 1. ‣ 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, suppose <span id="Thmtheorem3.p1.m1" class="ltx_Math">\Wp(\hat{\mu},\mu)\leq\rho_{\mu}</span> and <span id="Thmtheorem3.p1.m2" class="ltx_Math">\Wp(\hat{\nu},\nu)\leq\rho_{\nu}</span>. Then the estimate <span id="Thmtheorem3.p1.m3" class="ltx_Math">\hat{\kappa}=\hat{\kappa}_{\mathrm{DRO}}^{2\rho_{\mu}}[\hat{\mu},\hat{\nu}]</span> achieves <span id="Thmtheorem3.p1.m4" class="ltx_Math">\cE_{p}(\hat{\kappa};\mu,\nu)\lesssim L\rho_{\mu}^{\alpha}+\rho_{\mu}+\rho_{\nu}</span>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">Using the WDRO problem structure and our stability lemmas, we bound</p>
<table id="A5.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S4.Ex13.m1" class="ltx_Math">\displaystyle\cE_{p}(\hat{\kappa};\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S4.Ex13.m2" class="ltx_Math">\displaystyle\leq\cE_{p}(\hat{\kappa};\mu,\hat{\nu})+2\rho_{\nu}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)</span></td>
</tr></tbody>
<tbody id="S4.Ex15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S4.Ex15.m4" class="ltx_Math">\displaystyle\leq\sup_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu%
^{\prime}:\,\Wp(\mu^{\prime},\hat{\mu})\leq\rho_{\mu}\end{subarray}}\cE_{p}(%
\hat{\kappa};\mu^{\prime},\hat{\nu})+2\rho_{\nu}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="S4.Ex15.m1" class="ltx_Math">\Wp(\mu,\hat{\mu})\leq\rho_{\mu}</span>)</span></td>
</tr></tbody>
<tbody id="S4.Ex17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S4.Ex17.m4" class="ltx_Math">\displaystyle\leq\sup_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu%
^{\prime}:\,\Wp(\mu^{\prime},\hat{\mu})\leq\rho_{\mu}\end{subarray}}\cE_{p}(%
\kappa_{\star};\mu^{\prime},\hat{\nu})+2\rho_{\nu}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(optimality of <span id="S4.Ex17.m1" class="ltx_Math">\hat{\kappa}</span>)</span></td>
</tr></tbody>
<tbody id="S4.Ex19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S4.Ex19.m4" class="ltx_Math">\displaystyle\leq\sup_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu%
^{\prime}\in\cP(\R^{d}):\,\Wp(\mu^{\prime},\mu)\leq 2\rho_{\mu}\end{subarray}}%
\cE_{p}(\kappa_{\star};\mu^{\prime},\hat{\nu})+2\rho_{\nu}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="S4.Ex19.m1" class="ltx_Math">\Wp</span> triangle inequality)</span></td>
</tr></tbody>
<tbody id="S4.Ex20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S4.Ex20.m1" class="ltx_Math">\displaystyle\leq 4L\rho_{\mu}^{\alpha}+4\rho_{\mu}+2\rho_{\nu},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.
∎</p>
</div>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">This gives an information-theoretic reduction from kernel estimation under <span id="S4.p4.m1" class="ltx_Math">\cE_{p}</span> to estimation of <span id="S4.p4.m2" class="ltx_Math">\mu</span> and <span id="S4.p4.m3" class="ltx_Math">\nu</span> under <span id="S4.p4.m4" class="ltx_Math">\Wp</span>, i.e., if we can estimate <span id="S4.p4.m5" class="ltx_Math">\mu</span> up to error <span id="S4.p4.m6" class="ltx_Math">\rho_{\mu}</span> and <span id="S4.p4.m7" class="ltx_Math">\nu</span> up to error <span id="S4.p4.m8" class="ltx_Math">\rho_{\nu}</span>, then we can find a kernel with error <span id="S4.p4.m9" class="ltx_Math">O(L\rho_{\mu}^{\alpha}+\rho_{\mu}+\rho_{\nu})</span>. Focusing on the Lipschitz case with <span id="S4.p4.m10" class="ltx_Math">\alpha=1</span>, we first apply <a href="#Thmtheorem3" title="Theorem 3 (WDRO estimator). ‣ 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> using the plug-in estimators <span id="S4.p4.m11" class="ltx_Math">\mu=\hat{\mu}_{n}</span>, <span id="S4.p4.m12" class="ltx_Math">\nu=\hat{\nu}_{n}</span>.</p>
</div>
<div id="Thmcorollary1" class="ltx_theorem ltx_theorem_corollary">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 1</span></span><span class="ltx_text ltx_font_bold"> </span>(Plug-in estimators)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmcorollary1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Under Assumption <a href="#Thmassumption1" title="Assumption 1. ‣ 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with <span id="Thmcorollary1.p1.m1" class="ltx_Math">\alpha=1</span> and <span id="Thmcorollary1.p1.m2" class="ltx_Math">L=O(1)</span>, suppose <span id="Thmcorollary1.p1.m3" class="ltx_Math">\mu,\nu\in\cP_{2p}(\R^{d})</span>. Then taking <span id="Thmcorollary1.p1.m4" class="ltx_Math">\hat{\mu}=\hat{\mu}_{n}</span> and <span id="Thmcorollary1.p1.m5" class="ltx_Math">\hat{\nu}=\hat{\nu}_{n}</span>, <span id="Thmcorollary1.p1.m6" class="ltx_Math">\rho</span> can be tuned to achieve <span id="Thmcorollary1.p1.m7" class="ltx_Math">\cE_{p}(\hat{\kappa}_{\mathrm{DRO}}^{\rho};\mu,\nu)=\widetilde{O}_{p,d}(n^{-1/%
(d\lor 2p)})</span> with probability 0.9, which is minimax optimal up to logarithmic factors.</span></p>
</div>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">Plugging in <span id="S4.p5.m1" class="ltx_Math">p=2</span>, we recover the <span id="S4.p5.m2" class="ltx_Math">L^{2}</span> rate of <cite class="ltx_cite ltx_citemacro_cite">Balakrishnan and Manole (<a href="#bib.bib1" title="" class="ltx_ref">2025</a>)</cite>; however, this result holds under our significantly weaker assumption and for general <span id="S4.p5.m3" class="ltx_Math">p\geq 1</span>.
If further <span id="S4.p5.m4" class="ltx_Math">\mu</span> and <span id="S4.p5.m5" class="ltx_Math">\nu</span> are compactly supported with smooth densities, we can employ wavelet-based distribution estimators (see, e.g., <cite class="ltx_cite ltx_citemacro_cite">Weed and Berthet (<a href="#bib.bib47" title="" class="ltx_ref">2019</a>); Manole et al. (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>) to attain faster rates.</p>
</div>
<div id="Thmcorollary2" class="ltx_theorem ltx_theorem_corollary">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Corollary 2</span></span><span class="ltx_text ltx_font_bold"> </span>(Wavelet estimators)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmcorollary2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Under Assumption <a href="#Thmassumption1" title="Assumption 1. ‣ 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with <span id="Thmcorollary2.p1.m1" class="ltx_Math">\alpha=1</span> and <span id="Thmcorollary2.p1.m2" class="ltx_Math">L=O(1)</span>, suppose that <span id="Thmcorollary2.p1.m3" class="ltx_Math">\mu,\nu\in\cP([0,1]^{d})</span> admit Lebesgue densities <span id="Thmcorollary2.p1.m4" class="ltx_Math">f,g\in\cC^{s}([0,1]^{d})</span>. Then taking <span id="Thmcorollary2.p1.m5" class="ltx_Math">\hat{\mu}</span> and <span id="Thmcorollary2.p1.m6" class="ltx_Math">\hat{\nu}</span> as appropriate wavelet-based estimators, one can tune <span id="Thmcorollary2.p1.m7" class="ltx_Math">\delta</span> to achieve <span id="Thmcorollary2.p1.m8" class="ltx_Math">\cE_{p}(\hat{\kappa};\mu,\nu)=\widetilde{O}_{p,d}(n^{-[(1+s/p)/(d+s)\land 1/(2%
p)]})</span> with probability 0.9, which is minimax optimal up to logarithmic factors.</span></p>
</div>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Balakrishnan and Manole (<a href="#bib.bib1" title="" class="ltx_ref">2025</a>)</cite> also reduce map estimation to distribution estimation, so they prove a variety of similar guarantees. However, unlike our derivation, their analysis relies crucially on the structure of the Brenier map as the gradient of a sufficiently regular convex potential.</p>
</div>
<div id="Thmremark4" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 4</span></span><span class="ltx_text ltx_font_bold"> </span>(Lipschitz regularization)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremark4.p1" class="ltx_para">
<p class="ltx_p">Wasserstein DRO is known to be closely related to Lipschitz regularization (see, e.g., <cite class="ltx_cite ltx_citemacro_citep">Gao et al., <a href="#bib.bib10" title="" class="ltx_ref">2024</a></cite>). So perhaps expectedly, one can show for <span id="Thmremark4.p1.m1" class="ltx_Math">p=\alpha=1</span> that the guarantees of <span id="Thmremark4.p1.m2" class="ltx_Math">\hat{\kappa}_{\DRO}</span> are matched by the estimator which minimizes the regularized empirical risk <span id="Thmremark4.p1.m3" class="ltx_Math">\kappa\mapsto\cE_{1}(\kappa;\hat{\mu},\hat{\nu})+\lambda\Lip(x\mapsto\kappa_{x%
};\Wone)</span>. For deterministic map estimation, <cite class="ltx_cite ltx_citemacro_cite">González-Sanz et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> considered related neural estimators that enforced Lipschitz constraints on the estimated map. In general, minimizing the unregularized empirical risk <span id="Thmremark4.p1.m4" class="ltx_Math">\cE_{1}</span>, or, by <a href="#Thmlemma2" title="Lemma 2 (Comparison to Monge gap). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the corresponding Monge gap objective, achieves good rates whenever the obtained minimizer has a small Lipschitz constant (whether this arises due to explicit constraints or implicit optimization bias). This gives a partial explanation for the empirical success of the Monge gap regularizer for neural map estimation.</p>
</div>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Robust Estimation with Adversarial Corruptions</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">The previous sections allowed us to handle sampling error under <span id="S5.p1.m1" class="ltx_Math">\cE_{p}</span>. We now address local and global adversarial perturbations of the data points with minimal technical overhead, thanks to the strong stability properties of <span id="S5.p1.m2" class="ltx_Math">\cE_{p}</span> in both TV and <span id="S5.p1.m3" class="ltx_Math">\Wp</span>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Formal corruption model and assumptions.</span>
As discussed in the introduction, TV and <span id="S5.p2.m1" class="ltx_Math">\Wp</span> perturbations have historically been studied separately in robust statistics to model outliers and adversarial examples, respectively, with the former dating back to <cite class="ltx_cite ltx_citemacro_cite">Huber (<a href="#bib.bib16" title="" class="ltx_ref">1964</a>)</cite>. Our work adopts a recent combined model permitting both local and global perturbations of the input data <cite class="ltx_cite ltx_citemacro_citep">(Nietert et al., <a href="#bib.bib31" title="" class="ltx_ref">2023b</a>)</cite>.
Here, clean i.i.d. data from the unknown distributions <span id="S5.p2.m2" class="ltx_Math">\mu\in\cP(\cX)</span> and <span id="S5.p2.m3" class="ltx_Math">\nu\in\cP(\cY)</span> are first nudged by small local perturbations (namely, in Wasserstein distance with budget <span id="S5.p2.m4" class="ltx_Math">\rho\geq 0</span>) and then partially overwritten by global outliers (in TV, with allowed fraction <span id="S5.p2.m5" class="ltx_Math">\eps\in[0,1]</span>).
More precisely, letting <span id="S5.p2.m6" class="ltx_Math">X_{1},\dots,X_{n}\stackrel{{\scriptstyle\text{i.i.d.}}}{{\sim}}\mu</span> and <span id="S5.p2.m7" class="ltx_Math">Y_{1},\dots,Y_{n}\stackrel{{\scriptstyle\text{i.i.d.}}}{{\sim}}\nu</span> denote the clean samples, we observe <span id="S5.p2.m8" class="ltx_Math">\smash{\tilde{X}_{1},\dots,\tilde{X}_{n}\in\cX}</span> and <span id="S5.p2.m9" class="ltx_Math">\smash{\tilde{Y}_{1},\dots,\tilde{Y}_{n}\in\cY}</span> such that <span id="S5.p2.m10" class="ltx_Math">\frac{1}{n}\sum_{i\in S}\|\tilde{X}_{i}-X_{i}\|^{p}\lor\frac{1}{n}\sum_{i\in T%
}\|\tilde{X}_{i}-X_{i}\|^{p}\leq\rho^{p}</span> for some <span id="S5.p2.m11" class="ltx_Math">S,T\subseteq[n]</span> with <span id="S5.p2.m12" class="ltx_Math">|S|,|T|\geq(1-\eps)n</span>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">Write <span id="S5.p3.m1" class="ltx_Math">\hat{\mu}_{n},\tilde{\mu}_{n}</span> and <span id="S5.p3.m2" class="ltx_Math">\hat{\nu}_{n},\tilde{\nu}_{n}</span> for the clean and corrupted empirical measures for <span id="S5.p3.m3" class="ltx_Math">\mu</span> and <span id="S5.p3.m4" class="ltx_Math">\nu</span>, respectively. We further suppose that <span id="S5.p3.m5" class="ltx_Math">\mu</span> is 1-sub-Gaussian and <span id="S5.p3.m6" class="ltx_Math">\cY\subseteq[0,1]^{d}</span>. These assumptions can be relaxed at the cost of estimation complexity, as in <a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We impose them to focus on the new aspects of adversarial robustness without distractions.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">An initial idea is to combine the <a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> approaches, since the entropic kernel used <span id="S5.p4.m1" class="ltx_Math">\Wp</span> stability and the rounding estimator used TV stability. This is viable, however our entropic kernel analysis requires that <span id="S5.p4.m2" class="ltx_Math">\cX\cup\cY</span> is bounded. To avoid this, we employ a similar approach to the rounding estimator, but replace deterministic rounding with Gaussian convolution. Defining the kernel <span id="S5.p4.m3" class="ltx_Math">N^{\sigma}_{x}=\cN(x,\sigma^{2}I_{d})</span> and letting <span id="S5.p4.m4" class="ltx_Math">\kappa^{\star}_{p}[\alpha\to\beta]</span> denote (any) optimal kernel for the <span id="S5.p4.m5" class="ltx_Math">\Wp(\alpha,\beta)</span> problem, we consider</p>
<table id="A5.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S5.Ex21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S5.Ex21.m1" class="ltx_Math">\displaystyle\hat{\kappa}_{\mathrm{conv}}^{\sigma}[\tilde{\mu}_{n},\tilde{\nu}%
_{n}]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S5.Ex21.m2" class="ltx_Math">\displaystyle\defeq\kappa^{\star}_{p}[N^{\sigma}_{\sharp}\tilde{\mu}_{n}\to%
\tilde{\nu}_{n}]\circ N^{\sigma}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">That is, we find an optimal kernel for the convolved <span id="S5.p4.m6" class="ltx_Math">\Wp(N^{\sigma}_{\sharp}\tilde{\mu}_{n},\tilde{\nu}_{n})</span> problem and compose it with the convolution kernel. The initial convolution of <span id="S5.p4.m7" class="ltx_Math">\tilde{\mu}_{n}</span> ensures that the inner kernel is defined over all of <span id="S5.p4.m8" class="ltx_Math">\R^{d}</span>, potentially outside the support of <span id="S5.p4.m9" class="ltx_Math">\tilde{\mu}_{n}</span>. The subsequent composition ensures that the outer kernel is sufficiently continuous, as needed to apply <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We prove the following in <a href="#A4" title="Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div id="Thmtheorem4" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4</span></span><span class="ltx_text ltx_font_bold"> </span>(Robust estimation guarantee)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Under the setting above, we have</span></p>
<table id="S5.Ex22" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="S5.Ex22.m1" class="ltx_Math">\E[\cE_{p}(\hat{\kappa}_{\mathrm{conv}}^{\sigma};\mu,\nu)]\lesssim\sqrt{d}\eps%
^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+\rho+O_{p,d}(n^{-\frac{1}{d+2p}}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">for tuned <span id="Thmtheorem4.p1.m1" class="ltx_Math">\sigma=\sigma(\rho,d,p)</span>. Also, the naïve estimator <span id="Thmtheorem4.p1.m2" class="ltx_Math">(\hat{\kappa}_{\mathrm{null}})_{x}\equiv\delta_{0}</span> satisfies <span id="Thmtheorem4.p1.m3" class="ltx_Math">\cE_{p}(\hat{\kappa}_{\mathrm{null}};\mu,\nu)\leq\sqrt{d}</span>. By selecting between the two estimators according to which bound is smaller, we achieve an error bound of <span id="Thmtheorem4.p1.m4" class="ltx_Math">\bigl{(}\sqrt{d}\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+\rho+O_{p,d}(n%
^{-\frac{1}{d+2p}})\bigr{)}\land\sqrt{d}</span>. Moreover, up to constants, no estimator can achieve worst-case expected error less than <span id="Thmtheorem4.p1.m5" class="ltx_Math">\bigl{(}\sqrt{d}\eps^{\frac{1}{p}}+d^{1/4}\rho^{1/2}+n^{-\frac{1}{d\lor 2p}}%
\bigr{)}\land\sqrt{d}</span>.</span></p>
</div>
</div>
<div id="S5.p5" class="ltx_para">
<p class="ltx_p">The upper bound follows by a remarkably straightforward application of our stability lemmas. For the <span id="S5.p5.m1" class="ltx_Math">d^{1/4}\rho^{1/2}</span> term in the LB, we construct a pair of instances (with all distributions supported on two points) which are indistinguishable from <span id="S5.p5.m2" class="ltx_Math">\rho</span>-corrupted samples and such that no kernel achieves error <span id="S5.p5.m3" class="ltx_Math">o(d^{1/4}\rho^{1/2})</span> on both. Interestingly, this <span id="S5.p5.m4" class="ltx_Math">\sqrt{\rho}</span> dependence rules out a lossless reduction from estimation under <span id="S5.p5.m5" class="ltx_Math">\cE_{p}</span> to distribution estimation under <span id="S5.p5.m6" class="ltx_Math">\Wp</span>. That is, our rounding estimator from <a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> achieves <span id="S5.p5.m7" class="ltx_Math">\cE_{p}=\widetilde{O}(n^{-1/(d+2p)})</span> but the guarantee that <span id="S5.p5.m8" class="ltx_Math">\Wp(\hat{\mu}_{n},\mu)\lor\Wp(\hat{\nu}_{n},\nu)=\widetilde{O}(n^{-1/(d\lor 2p%
)})</span> alone cannot imply a rate faster than <span id="S5.p5.m9" class="ltx_Math">\widetilde{O}(n^{-1/(2d\lor 4p)})</span>. Finally, although the convolved OT problem for our estimator may not be efficiently solvable, we show in <a href="#A4" title="Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> that an additional rounding step enables efficient computation, mirroring the proof of <a href="#Thmtheorem2" title="Theorem 2 (Rounding estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">To empirically validate our theory, we run experiments in two synthetic settings with OT maps whose irregularities limit the utility of the <span id="S6.p1.m1" class="ltx_Math">L^{p}</span> objective and prevent application of existing theory.
For Setting A, we fix <span id="S6.p1.m2" class="ltx_Math">\mu</span> and <span id="S6.p1.m3" class="ltx_Math">\nu</span> as uniform discrete measures over <span id="S6.p1.m4" class="ltx_Math">N=2000</span> points, obtained as i.i.d. samples from <span id="S6.p1.m5" class="ltx_Math">\Unif(\{0\}\times[0,1]^{d-1})</span> and <span id="S6.p1.m6" class="ltx_Math">\frac{1}{2}\Unif(\{-1\}\times[0,1]^{d-1})+\frac{1}{2}\Unif(\{1\}\times[0,1]^{d%
-1})</span>, respectively.
In the <span id="S6.p1.m7" class="ltx_Math">N\to\infty</span> limit, the optimal kernel satisfies <span id="S6.p1.m8" class="ltx_Math">\kappa^{\star}_{(0,x_{2:d})}=\Unif(\{(-1,x_{2:d}),(1,x_{2:d})\})</span>. For our discrete <span id="S6.p1.m9" class="ltx_Math">\mu</span> and <span id="S6.p1.m10" class="ltx_Math">\nu</span>, there is an optimal deterministic map <span id="S6.p1.m11" class="ltx_Math">T^{\star}</span> induced by a permutation, but it is highly oscillatory. For Setting B, we set <span id="S6.p1.m12" class="ltx_Math">\mu</span> and <span id="S6.p1.m13" class="ltx_Math">\nu</span> as discrete distributions over <span id="S6.p1.m14" class="ltx_Math">N</span> samples from <span id="S6.p1.m15" class="ltx_Math">\Unif([-1,1]^{d})</span> and <span id="S6.p1.m16" class="ltx_Math">f_{\sharp}\Unif([-1,1]^{d})</span>, respectively, where <span id="S6.p1.m17" class="ltx_Math">f(x)=x+(\operatorname{sign}(x_{1}),\dots,\operatorname{sign}(x_{d}))</span> pushes each orthant of the cube away from the origin. Here, the OT map is discontinuous but Lipschitz within each orthant.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">Now, for each setting and sample size <span id="S6.p2.m1" class="ltx_Math">n\in\{10,20,\dots,100\}</span>, we take <span id="S6.p2.m2" class="ltx_Math">n</span> i.i.d. samples from <span id="S6.p2.m3" class="ltx_Math">\mu</span> and <span id="S6.p2.m4" class="ltx_Math">\nu</span> and compute the <span id="S6.p2.m5" class="ltx_Math">p=1</span> nearest-neighbor map estimate <span id="S6.p2.m6" class="ltx_Math">\smash{\hat{T}_{n}^{\mathrm{NN}}}</span> <cite class="ltx_cite ltx_citemacro_citep">(Manole et al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite> and the rounding kernel estimate <span id="S6.p2.m7" class="ltx_Math">\hat{\kappa}_{n}^{\mathrm{round}}</span> (<a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). (Specifically, the NN estimator first computes an optimal <span id="S6.p2.m8" class="ltx_Math">\Wone</span> map <span id="S6.p2.m9" class="ltx_Math">\bar{T}_{n}</span> from <span id="S6.p2.m10" class="ltx_Math">\hat{\mu}_{n}</span> to <span id="S6.p2.m11" class="ltx_Math">\hat{\nu}_{n}</span>. Then, <span id="S6.p2.m12" class="ltx_Math">\hat{T}_{n}^{\mathrm{NN}}</span> maps each <span id="S6.p2.m13" class="ltx_Math">x\in\R^{d}</span> to the image of its nearest source point under <span id="S6.p2.m14" class="ltx_Math">\bar{T}_{n}</span>.) We then compute the <span id="S6.p2.m15" class="ltx_Math">L^{1}</span> error <span id="S6.p2.m16" class="ltx_Math">\|\hat{T}_{n}^{\mathrm{NN}}-T^{\star}\|_{L^{1}(\mu)}</span> and the <span id="S6.p2.m17" class="ltx_Math">\cE_{1}</span> errors <span id="S6.p2.m18" class="ltx_Math">\smash{\cE_{1}(\hat{T}_{n}^{\mathrm{NN}};\mu,\nu),\cE_{1}(\hat{\kappa}_{n}^{%
\mathrm{round}};\mu,\nu)}</span>. Since <span id="S6.p2.m19" class="ltx_Math">\mu</span> and <span id="S6.p2.m20" class="ltx_Math">\nu</span> are discrete, these can be computed using finite sums and the default Python Optimal Transport solver <cite class="ltx_cite ltx_citemacro_citep">(Flamary et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>. Repeating this process for <span id="S6.p2.m21" class="ltx_Math">K=100</span> iterations, we compute mean errors for each sample size and dimension <span id="S6.p2.m22" class="ltx_Math">d\in\{3,5,10\}</span>, along with bootstrapped 10% and 90% quantiles (via 1000 bootstrap resamples). In <a href="#S6.F3" title="Figure 3 ‣ 6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (left), we compare the <span id="S6.p2.m23" class="ltx_Math">\cE_{1}</span> vs <span id="S6.p2.m24" class="ltx_Math">L^{1}</span> performance of the NN estimator under Setting A (where the latter is well-defined since each <span id="S6.p2.m25" class="ltx_Math">\hat{T}_{n}^{\mathrm{NN}}</span> is a deterministic map). As expected, <span id="S6.p2.m26" class="ltx_Math">L^{1}</span> performance is quite poor, with error always greater than 1. Although we currently lack formal guarantees for the NN estimator (and our setting lies outside of existing theory), it achieves strong <span id="S6.p2.m27" class="ltx_Math">\cE_{1}</span> performance, with faster rates in lower dimensions.
In <a href="#S6.F3" title="Figure 3 ‣ 6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (center), we compare the NN and rounding estimators under <span id="S6.p2.m28" class="ltx_Math">\cE_{1}</span>, the latter enjoying formal guarantees by <a href="#Thmtheorem2" title="Theorem 2 (Rounding estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Empirically, the NN estimator performs better, but this gap diminishes in high dimensions. We suspect that low-dimensional performance of the rounding estimator is more sensitive to its side-length hyperparameter, which we have simply set to <span id="S6.p2.m29" class="ltx_Math">n^{-1/(d+2)}</span> as per the proof of <a href="#Thmtheorem2" title="Theorem 2 (Rounding estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Finally, in <a href="#S6.F3" title="Figure 3 ‣ 6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (right) we turn to Setting B, again comparing <span id="S6.p2.m30" class="ltx_Math">\smash{\hat{T}_{n}^{\mathrm{NN}}}</span> and <span id="S6.p2.m31" class="ltx_Math">\hat{\kappa}_{n}^{\mathrm{round}}</span> under <span id="S6.p2.m32" class="ltx_Math">\cE_{1}</span> and observing similar trends. We note that all experiments were performed on an M1 MacBook Air with 16GB RAM and 8 CPU cores. See Appendix <a href="#A5" title="Appendix E Additional Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> for full experiment details and two additional experiments (one with larger parameter settings, but no bootstrapping, and one in two dimensions, so that our estimator can be visualized).</p>
</div>
<div id="Thmremark5" class="ltx_theorem ltx_theorem_remark">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 5</span></span><span class="ltx_text ltx_font_bold"> </span>(Neural map estimation)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmremark5.p1" class="ltx_para">
<p class="ltx_p">Given the connections between <span id="Thmremark5.p1.m1" class="ltx_Math">\cE_{p}</span> and the Monge gap objective discussed in <a href="#S1" title="1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_ERROR undefined">\crefpairconjunction</span><a href="#S2" title="2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, one could consider training a neural map estimator to minimize an empirical <span id="Thmremark5.p1.m2" class="ltx_Math">\cE_{p}</span> objective, perhaps after approximating the <span id="Thmremark5.p1.m3" class="ltx_Math">\Wp</span> terms via EOT. However, while both <span id="Thmremark5.p1.m4" class="ltx_Math">\cE_{p}</span> and the Monge gap objective nullify on optimal maps, they behave quite differently when far from optimality. Indeed, gradients of the feasibility gap term in <span id="Thmremark5.p1.m5" class="ltx_Math">\cE_{p}</span> push towards the identity map (since it achieves the minimum transport cost of zero), while gradients of the Monge gap push towards the much larger set of <span id="Thmremark5.p1.m6" class="ltx_Math">c</span>-cyclically monotone maps. In preliminary tests, we found that the Monge gap objective led to significantly more stable training dynamics, which we attribute to this difference. Thus, we maintain our recommendation of <span id="Thmremark5.p1.m7" class="ltx_Math">\cE_{p}</span> as an evaluation metric, enabling provable error guarantees under weaker assumptions, rather than a training objective for neural map estimation. Still, we hope that our analysis under <span id="Thmremark5.p1.m8" class="ltx_Math">\cE_{p}</span> might inspire new regularization methods in the future.</p>
</div>
</div>
<figure id="S6.F3" class="ltx_figure">
<p class="ltx_p"><span class="ltx_text" style="width:433.6pt;">
<img src="" id="S6.F3.g1" class="ltx_graphics" alt=" and ">
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S6.F3.m3" class="ltx_Math">\cE_{1}</span> and <span id="S6.F3.m4" class="ltx_Math">L^{1}</span> performance of nearest-neighbor and rounding estimators in two settings.</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">This work proposed a novel error metric <span id="S7.p1.m1" class="ltx_Math">\cE_{p}</span> which broadens the scope of OT map estimation research to support stochastic maps, sidestepping existence, uniqueness, and regularity issues faced by existing approaches and treating <span id="S7.p1.m2" class="ltx_Math">p\neq 2</span>. We developed an efficient rounding estimator with near-optimal rates under <span id="S7.p1.m3" class="ltx_Math">\cE_{p}</span> and characterized the minimax rate for Lipschitz continuous kernels. Our analysis extends naturally to adversarial corruptions, and our theory is supported by numerical simulations.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">There are two clear open questions. First, what is the minimax finite-sample risk for estimation under <span id="S7.p2.m1" class="ltx_Math">\cE_{p}</span>, say for <span id="S7.p2.m2" class="ltx_Math">\mu,\nu\in\cP([0,1]^{d})</span>? We have established that the correct rate lies between <span id="S7.p2.m3" class="ltx_Math">n^{-1/(d\lor 2p)}</span> and <span id="S7.p2.m4" class="ltx_Math">n^{-1/(d+2p)}</span>. The slower rate mirrors that attained by bounding <span id="S7.p2.m5" class="ltx_Math">\E[\Wp(\hat{\mu}_{n},\mu)]</span> without analyzing sampling error at multiple geometric scales. Can a multi-scale approach extend to kernel estimation and improve the current upper bound in <a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>?
Second, under the setting of <a href="#S4" title="4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> with <span id="S7.p2.m6" class="ltx_Math">\alpha=1</span>, where there exists an optimal Lipschitz kernel, can a computationally efficient estimator achieve the optimal <span id="S7.p2.m7" class="ltx_Math">n^{-1/(d\lor 2p)}</span> rate? Our experiments demonstrate strong empirical performance of the NN estimator in varied settings, so it seems to be a promising candidate to attain such a guarantee.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">Finally, our objective can naturally be extended to many OT variants, including EOT, weak OT <cite class="ltx_cite ltx_citemacro_citep">(Gozlan et al., <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite>, conditional OT <cite class="ltx_cite ltx_citemacro_citep">(Hosseini et al., <a href="#bib.bib15" title="" class="ltx_ref">2025</a>)</cite>, and adapted OT <cite class="ltx_cite ltx_citemacro_citep">(Bartl et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>)</cite>. Adapting our toolkit of stability lemmas to such settings is an interesting direction for future work.</p>
</div>
<span class="ltx_ERROR undefined">\ack</span>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">Z. Goldfeld is partially supported by NSF grants CCF-2046018, DMS-2210368, and CCF-2308446, and the IBM Academic Award.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balakrishnan and Manole [2025]</span>
<span class="ltx_bibblock">
S. Balakrishnan and T. Manole.

</span>
<span class="ltx_bibblock">Stability bounds for smooth optimal transport maps and their statistical implications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2502.12326</em>, 2025.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartl et al. [2024]</span>
<span class="ltx_bibblock">
D. Bartl, M. Beiglböck, and G. Pammer.

</span>
<span class="ltx_bibblock">The Wasserstein space of stochastic processes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of the European Mathematical Society</em>, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brenier [1991]</span>
<span class="ltx_bibblock">
Y. Brenier.

</span>
<span class="ltx_bibblock">Polar factorization and monotone rearrangement of vector-valued functions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Communications on Pure and Applied Mathematics</em>, 44(4):375–417, 1991.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bunne et al. [2023]</span>
<span class="ltx_bibblock">
C. Bunne, S. G. Stark, G. Gut, J. S. Del Castillo, M. Levesque, K.-V. Lehmann, L. Pelkmans, A. Krause, and G. Rätsch.

</span>
<span class="ltx_bibblock">Learning single-cell perturbation responses using neural optimal transport.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature Methods</em>, 20(11):1759–1768, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao and Dobriban [2023]</span>
<span class="ltx_bibblock">
P. Chao and E. Dobriban.

</span>
<span class="ltx_bibblock">Statistical estimation under distribution shift: Wasserstein perturbations and minimax theory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.01853</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Courty et al. [2016]</span>
<span class="ltx_bibblock">
N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy.

</span>
<span class="ltx_bibblock">Optimal transport for domain adaptation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 39(9):1853–1865, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuturi [2013]</span>
<span class="ltx_bibblock">
M. Cuturi.

</span>
<span class="ltx_bibblock">Sinkhorn distances: Lightspeed computation of optimal transport.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deb et al. [2021]</span>
<span class="ltx_bibblock">
N. Deb, P. Ghosal, and B. Sen.

</span>
<span class="ltx_bibblock">Rates of estimation of optimal transport maps using plug-in estimators via barycentric projections.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>, 34, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flamary et al. [2021]</span>
<span class="ltx_bibblock">
R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos, K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko, A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer.

</span>
<span class="ltx_bibblock">Pot: Python optimal transport.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 22(78):1–8, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2024]</span>
<span class="ltx_bibblock">
R. Gao, X. Chen, and A. J. Kleywegt.

</span>
<span class="ltx_bibblock">Wasserstein distributionally robust optimization and variation regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Operations Research</em>, 72(3), 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Genevay et al. [2019]</span>
<span class="ltx_bibblock">
A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyré.

</span>
<span class="ltx_bibblock">Sample complexity of Sinkhorn divergences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldfeld et al. [2020]</span>
<span class="ltx_bibblock">
Z. Goldfeld, K. Greenewald, J. Niles-Weed, and Y. Polyanskiy.

</span>
<span class="ltx_bibblock">Convergence of smoothed empirical measures with applications to entropy estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE Transactions on Information Theory</em>, 66(7):4368–4391, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">González-Sanz et al. [2022]</span>
<span class="ltx_bibblock">
A. González-Sanz, L. De Lara, L. Béthune, and J.-M. Loubes.

</span>
<span class="ltx_bibblock">GAN estimation of Lipschitz optimal transport maps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07965</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gozlan et al. [2017]</span>
<span class="ltx_bibblock">
N. Gozlan, C. Roberto, P.-M. Samson, and P. Tetali.

</span>
<span class="ltx_bibblock">Kantorovich duality for general transport costs and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Functional Analysis</em>, 273(11):3327–3405, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et al. [2025]</span>
<span class="ltx_bibblock">
B. Hosseini, A. W. Hsu, and A. Taghvaei.

</span>
<span class="ltx_bibblock">Conditional optimal transport on function spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">SIAM/ASA Journal on Uncertainty Quantification</em>, 13(1):304–338, 2025.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huber [1964]</span>
<span class="ltx_bibblock">
P. J. Huber.

</span>
<span class="ltx_bibblock">Robust Estimation of a Location Parameter.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Annals of Mathematical Statistics</em>, 35(1):73–101, 1964.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hütter and Rigollet [2021]</span>
<span class="ltx_bibblock">
J.-C. Hütter and P. Rigollet.

</span>
<span class="ltx_bibblock">Minimax estimation of smooth optimal transport maps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Annals of Statistics</em>, 49(2):1166–1194, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantorovich [1942]</span>
<span class="ltx_bibblock">
L. V. Kantorovich.

</span>
<span class="ltx_bibblock">On the translocation of masses.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Doklady Akademii Nauk USSR</em>, volume 37, pages 199–201, 1942.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolkin et al. [2019]</span>
<span class="ltx_bibblock">
N. Kolkin, J. Salavon, and G. Shakhnarovich.

</span>
<span class="ltx_bibblock">Style transfer by relaxed optimal transport and self-similarity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korotin et al. [2023a]</span>
<span class="ltx_bibblock">
A. Korotin, D. Selikhanovych, and E. Burnaev.

</span>
<span class="ltx_bibblock">Kernel neural optimal transport.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023a.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korotin et al. [2023b]</span>
<span class="ltx_bibblock">
A. Korotin, D. Selikhanovych, and E. Burnaev.

</span>
<span class="ltx_bibblock">Neural optimal transport.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023b.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei [2020]</span>
<span class="ltx_bibblock">
J. Lei.

</span>
<span class="ltx_bibblock">Convergence and concentration of empirical measures under Wasserstein distance in unbounded functional spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Bernoulli</em>, 26(1):767–798, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Nochetto [2021]</span>
<span class="ltx_bibblock">
W. Li and R. H. Nochetto.

</span>
<span class="ltx_bibblock">Quantitative stability and error estimates for optimal transport plans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IMA Journal of Numerical Analysis</em>, 41(3):1941–1965, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Loh [2023]</span>
<span class="ltx_bibblock">
Z. Liu and P.-L. Loh.

</span>
<span class="ltx_bibblock">Robust W-GAN-based estimation under Wasserstein contamination.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Information and Inference: A Journal of the IMA</em>, 12(1):312–362, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2023]</span>
<span class="ltx_bibblock">
Y. Luo, Y. Xie, and X. Huo.

</span>
<span class="ltx_bibblock">Improved rate of first order algorithms for entropic optimal transport.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics</em>, pages 2723–2750. PMLR, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manole et al. [2024]</span>
<span class="ltx_bibblock">
T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasserman.

</span>
<span class="ltx_bibblock">Plugin estimation of smooth optimal transport maps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Annals of Statistics</em>, 52(3):966–998, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Massart [1990]</span>
<span class="ltx_bibblock">
P. Massart.

</span>
<span class="ltx_bibblock">The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Annals of Probability</em>, 18(4):1269–1283, 1990.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. [2019]</span>
<span class="ltx_bibblock">
C. Meng, Y. Ke, J. Zhang, M. Zhang, W. Zhong, and P. Ma.

</span>
<span class="ltx_bibblock">Large-scale optimal transport map estimation using projection pursuit.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mroueh [2020]</span>
<span class="ltx_bibblock">
Y. Mroueh.

</span>
<span class="ltx_bibblock">Wasserstein style transfer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nietert et al. [2023a]</span>
<span class="ltx_bibblock">
S. Nietert, R. Cummings, and Z. Goldfeld.

</span>
<span class="ltx_bibblock">Robust estimation under the Wasserstein distance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.01237</em>, 2023a.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nietert et al. [2023b]</span>
<span class="ltx_bibblock">
S. Nietert, Z. Goldfeld, and S. Shafiee.

</span>
<span class="ltx_bibblock">Outlier-robust Wasserstein DRO.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023b.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nietert et al. [2024]</span>
<span class="ltx_bibblock">
S. Nietert, Z. Goldfeld, and S. Shafiee.

</span>
<span class="ltx_bibblock">Robust distribution estimation with local and global adversarial corruptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Learning Theory (COLT)</em>, 2024.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pittas and Pensia [2025]</span>
<span class="ltx_bibblock">
T. Pittas and A. Pensia.

</span>
<span class="ltx_bibblock">Optimal robust estimation under local and global corruptions: Stronger adversary and smaller error.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Learning Theory (COLT)</em>, 2025.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pooladian and Niles-Weed [2021]</span>
<span class="ltx_bibblock">
A.-A. Pooladian and J. Niles-Weed.

</span>
<span class="ltx_bibblock">Entropic estimation of optimal transport maps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.12004</em>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pooladian et al. [2023]</span>
<span class="ltx_bibblock">
A.-A. Pooladian, V. Divol, and J. Niles-Weed.

</span>
<span class="ltx_bibblock">Minimax estimation of discontinuous optimal transport maps: The semi-discrete case.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redko et al. [2019]</span>
<span class="ltx_bibblock">
A. Redko, N. Courty, R. Flamary, and D. Tuia.

</span>
<span class="ltx_bibblock">Optimal transport for multi-source domain adaptation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 127(8):1923–1953, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rigollet [2015]</span>
<span class="ltx_bibblock">
P. Rigollet.

</span>
<span class="ltx_bibblock">18.s997: High dimensional statistics lecture notes.

</span>
<span class="ltx_bibblock"><a href="https://ocw.mit.edu/courses/18-s997-high-dimensional-statistics-spring-2015/619e4ae252f1b26cbe0f7a29d5932978_MIT18_S997S15_CourseNotes.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ocw.mit.edu/courses/18-s997-high-dimensional-statistics-spring-2015/619e4ae252f1b26cbe0f7a29d5932978_MIT18_S997S15_CourseNotes.pdf</a>, 2015.

</span>
<span class="ltx_bibblock">Lecture notes for MIT course 18.S997, Spring 2015.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santambrogio [2015]</span>
<span class="ltx_bibblock">
F. Santambrogio.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Optimal Transport for Applied Mathematicians</em>.

</span>
<span class="ltx_bibblock">Birkhäuser, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schiebinger et al. [2019]</span>
<span class="ltx_bibblock">
G. Schiebinger, J. Shu, B. T. Tabaka, J. Ashouri, D. J. Cleary, V. Subramanian, and et al.

</span>
<span class="ltx_bibblock">Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Cell</em>, 176(4):928–943.e22, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seguy et al. [2018]</span>
<span class="ltx_bibblock">
V. Seguy, B. B. Damodaran, R. Flamary, N. Courty, A. Rolet, and M. Blondel.

</span>
<span class="ltx_bibblock">Large scale optimal transport and mapping estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh and Póczos [2018]</span>
<span class="ltx_bibblock">
S. Singh and B. Póczos.

</span>
<span class="ltx_bibblock">Minimax distribution estimation in Wasserstein distance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.08855</em>, 2018.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneian [2019]</span>
<span class="ltx_bibblock">
D. Toneian.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Measurable selection in optimal transport and Skorokhod embeddings</em>.

</span>
<span class="ltx_bibblock">PhD thesis, Wien, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uscidda and Cuturi [2023]</span>
<span class="ltx_bibblock">
T. Uscidda and M. Cuturi.

</span>
<span class="ltx_bibblock">The Monge gap: A regularizer to learn all transport maps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vesseron et al. [2025]</span>
<span class="ltx_bibblock">
N. Vesseron, L. Béthune, and M. Cuturi.

</span>
<span class="ltx_bibblock">Sample and map from a single convex potential: Generation using conjugate moment measures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.10576</em>, 2025.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villani [2003]</span>
<span class="ltx_bibblock">
C. Villani.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Topics in Optimal Transportation</em>.

</span>
<span class="ltx_bibblock">Graduate Studies in Mathematics. American Mathematical Society, 2003.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Goldfeld [2024]</span>
<span class="ltx_bibblock">
T. Wang and Z. Goldfeld.

</span>
<span class="ltx_bibblock">Neural estimation of entropic optimal transport.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Symposium on Information Theory (ISIT)</em>, 2024.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weed and Berthet [2019]</span>
<span class="ltx_bibblock">
J. Weed and Q. Berthet.

</span>
<span class="ltx_bibblock">Estimation of smooth densities in Wasserstein distance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Learning Theory (COLT)</em>, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2018]</span>
<span class="ltx_bibblock">
L. Zhang, L. Wang, et al.

</span>
<span class="ltx_bibblock">Monge-Ampère flow for generative modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.10188</em>, 2018.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2022]</span>
<span class="ltx_bibblock">
B. Zhu, J. Jiao, and J. Steinhardt.

</span>
<span class="ltx_bibblock">Generalized resilience and robust statistics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Annals of Statistics</em>, 50(4):2256 – 2283, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Proofs for <a href="#S2" title="2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Proof of <a href="#Thmproposition1" title="Proposition 1 (Relation to Lp loss). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p class="ltx_p">Clearly, <span id="A1.SS1.p1.m1" class="ltx_Math">\cE_{p}(\kappa;\mu,\nu)=0</span> if <span id="A1.SS1.p1.m2" class="ltx_Math">\kappa</span> minimizes (<a href="#S1.E2" title="(2) ‣ 1.1 New Framework for Stochastic OT Map Estimation and Contributions ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). On the other hand, if <span id="A1.SS1.p1.m3" class="ltx_Math">\cE_{p}(\kappa,\mu,\nu)=0</span>, then <span id="A1.SS1.p1.m4" class="ltx_Math">\kappa_{\sharp}\mu=\nu</span>. Thus, <span id="A1.SS1.p1.m5" class="ltx_Math">\kappa</span> is feasible for (<a href="#S1.E2" title="(2) ‣ 1.1 New Framework for Stochastic OT Map Estimation and Contributions ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) with optimal objective value, i.e., it is a minimizer.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p class="ltx_p">Further, if <span id="A1.SS1.p2.m1" class="ltx_Math">T^{\star}</span> is an optimal map, then <span id="A1.SS1.p2.m2" class="ltx_Math">\Wp(\mu,\nu)=\|T^{\star}-\Id\|_{L^{p}(\mu)}</span> and <span id="A1.SS1.p2.m3" class="ltx_Math">T^{\star}_{\sharp}\mu=\nu</span>. We thus bound
</p>
<table id="A5.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex23.m1" class="ltx_Math">\displaystyle\cE_{p}(T;\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex23.m2" class="ltx_Math">\displaystyle=\left[\|T-\Id\|_{L^{p}(\mu)}-\Wp(\mu,\nu)\right]_{+}+\Wp(T_{%
\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex24"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex24.m1" class="ltx_Math">\displaystyle=\left[\|T-\Id\|_{L^{p}(\mu)}-\|T^{\star}-\Id\|_{L^{p}(\mu)}%
\right]_{+}+\Wp(T_{\sharp}\mu,T^{\star}_{\sharp}\mu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex25"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex25.m1" class="ltx_Math">\displaystyle\leq 2\|T-T^{\star}\|_{L^{p}(\mu)},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.∎</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Reverse <span id="A1.SS2.m1" class="ltx_Math">L^{2}</span> comparison (<a href="#Thmremark2" title="Remark 2 (Reverse L2 comparison). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p class="ltx_p">Suppose that there exists a unique Brenier map of the form <span id="A1.SS2.p1.m1" class="ltx_Math">T^{\star}=\nabla\varphi</span>, where <span id="A1.SS2.p1.m2" class="ltx_Math">\varphi:\R^{d}\to\R</span> is convex and twice differentiable such that <span id="A1.SS2.p1.m3" class="ltx_Math">H\varphi\preceq LI_{d}</span>. Fixing any map <span id="A1.SS2.p1.m4" class="ltx_Math">T:\cX\to\cY</span>, we abbreviate <span id="A1.SS2.p1.m5" class="ltx_Math">\eps=\cE_{2}(T;\mu,\nu)</span>. By the definition of <span id="A1.SS2.p1.m6" class="ltx_Math">\cE_{2}</span>, we have <span id="A1.SS2.p1.m7" class="ltx_Math">\Wtwo(T_{\sharp}\mu,\nu)\leq\eps</span>. Let <span id="A1.SS2.p1.m8" class="ltx_Math">\lambda\in\cK(\cY,\cY)</span> be a kernel which achieves this bound, and take <span id="A1.SS2.p1.m9" class="ltx_Math">\kappa=\lambda\circ T</span>. By construction, we have <span id="A1.SS2.p1.m10" class="ltx_Math">\kappa_{\sharp}\mu=\nu</span> and</p>
<table id="A5.EGx7" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex26.m1" class="ltx_Math">\displaystyle\left(\iint\|y-x\|^{2}\dd\kappa(y|x)\dd\mu(x)\right)^{\frac{1}{2}%
}-\Wtwo(\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex26.m2" class="ltx_Math">\displaystyle\leq\left(\iint\|T(x)-x\|^{2}\dd\mu(x)\right)^{\frac{1}{2}}-\Wtwo%
(\mu,\nu)+\eps</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex27.m1" class="ltx_Math">\displaystyle\leq 2\eps.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Consequently, we have</p>
<table id="A5.EGx8" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex28.m1" class="ltx_Math">\displaystyle\iint\|y-x\|^{2}\dd\kappa(y|x)\dd\mu(x)-\Wtwo(\mu,\nu)^{2}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex28.m2" class="ltx_Math">\displaystyle\leq 2\eps\left(\left(\iint\|y-x\|^{2}\dd\kappa(y|x)\dd\mu(x)%
\right)^{\frac{1}{2}}+\Wtwo(\mu,\nu)\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex29.m1" class="ltx_Math">\displaystyle\leq 2\eps\cdot\left(2\Wtwo(\mu,\nu)+2\eps\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, by Proposition 3.1 of <cite class="ltx_cite ltx_citemacro_cite">Li and Nochetto [<a href="#bib.bib23" title="" class="ltx_ref">2021</a>]</cite>, we have</p>
<table id="A5.EGx9" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex30.m1" class="ltx_Math">\displaystyle\iint\|y-T^{\star}(x)\|^{2}\dd\kappa(y|x)\dd\mu(x)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex30.m2" class="ltx_Math">\displaystyle\leq L\left(\iint\|y-x\|^{2}\dd\kappa(y|x)\dd\mu(x)-\Wtwo(\mu,\nu%
)^{2}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex31.m1" class="ltx_Math">\displaystyle\leq 4L\eps\cdot\left(\Wtwo(\mu,\nu)+\eps\right).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Finally, we bound</p>
<table id="A5.EGx10" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex32"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex32.m1" class="ltx_Math">\displaystyle\|T-T_{\star}\|_{L^{2}(\mu)}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex32.m2" class="ltx_Math">\displaystyle\leq\left(\iint\|y-T^{\star}(x)\|^{2}\dd\kappa(y|x)\dd\mu(x)%
\right)^{\frac{1}{2}}+\eps</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex33"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex33.m1" class="ltx_Math">\displaystyle\leq\sqrt{4L\eps\cdot\left(\Wtwo(\mu,\nu)+\eps\right)}+\eps</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex34"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex34.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{(L\lor 1)\cdot\eps\cdot\left(\Wtwo(\mu,\nu)+\eps%
\right)},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Proof of <a href="#Thmlemma2" title="Lemma 2 (Comparison to Monge gap). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p class="ltx_p">First, we bound <span id="A1.SS3.p1.m1" class="ltx_Math">\cE_{p}</span> in terms of <span id="A1.SS3.p1.m2" class="ltx_Math">\cE^{\prime}_{p}</span>, computing</p>
<table id="A5.EGx11" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex35"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex35.m1" class="ltx_Math">\displaystyle\cE_{p}(\kappa;\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex35.m2" class="ltx_Math">\displaystyle=\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu(x)\right)^{%
\frac{1}{p}}-\Wp(\mu,\nu)\right]_{+}+\Wp(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex36"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex36.m1" class="ltx_Math">\displaystyle\leq\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu(x)\right)^%
{\frac{1}{p}}-\Wp(\mu,T_{\sharp}\mu)\right]_{+}+2\Wp(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex37"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex37.m1" class="ltx_Math">\displaystyle\leq\left[\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu(x)-\Wp(\mu,T_{%
\sharp}\mu)^{p}\right]_{+}^{\frac{1}{p}}+2\Wp(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex38"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex38.m1" class="ltx_Math">\displaystyle=\left[\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu(x)-\Wp(\mu,T_{%
\sharp}\mu)^{p}\right]^{\frac{1}{p}}+2\Wp(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex39"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex39.m1" class="ltx_Math">\displaystyle\leq 2^{2-\frac{1}{p}}\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd%
\mu(x)-\Wp(\mu,T_{\sharp}\mu)^{p}+\Wp(T_{\sharp}\mu,\nu)^{p}\right)^{\frac{1}{%
p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex40"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex40.m1" class="ltx_Math">\displaystyle\leq 2^{2-\frac{1}{p}}\cE^{\prime}_{p}(\kappa;\mu,\nu),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the second inequality uses that <span id="A1.SS3.p1.m3" class="ltx_Math">[a^{1/p}-b^{1/p}]_{+}\leq[a-b]_{+}^{1/p}</span> for all <span id="A1.SS3.p1.m4" class="ltx_Math">a,b\geq 0</span>, and the penultimate inequality uses that <span id="A1.SS3.p1.m5" class="ltx_Math">\ell_{1}\leq 2^{1-1/p}\ell_{p}</span> in <span id="A1.SS3.p1.m6" class="ltx_Math">\R^{2}</span>. This implies the claimed bound of <span id="A1.SS3.p1.m7" class="ltx_Math">\cE_{p}\leq 4\cE^{\prime}_{p}</span>. When <span id="A1.SS3.p1.m8" class="ltx_Math">p=1</span>, the above gives <span id="A1.SS3.p1.m9" class="ltx_Math">\cE_{1}\leq 2\cE^{\prime}_{1}</span>, and we similarly bound</p>
<table id="A5.EGx12" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex41"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex41.m1" class="ltx_Math">\displaystyle\cE_{1}^{\prime}(\kappa;\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex41.m2" class="ltx_Math">\displaystyle=\iint\|y-x\|\dd\kappa_{x}(y)\dd\mu(x)-\Wone(\mu,T_{\sharp}\mu)+%
\Wone(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex42"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex42.m1" class="ltx_Math">\displaystyle=\left[\iint\|y-x\|\dd\kappa_{x}(y)\dd\mu(x)-\Wone(\mu,T_{\sharp}%
\mu)\right]_{+}+\Wone(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex43"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex43.m1" class="ltx_Math">\displaystyle\leq\left[\iint\|y-x\|\dd\kappa_{x}(y)\dd\mu(x)-\Wone(\mu,\nu)%
\right]_{+}+2\Wone(T_{\sharp}\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex44"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex44.m1" class="ltx_Math">\displaystyle\leq 2\cE_{1}(\kappa;\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.∎</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Proof of <a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>
</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p class="ltx_p">We simply bound</p>
<table id="A5.EGx13" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex45"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex45.m1" class="ltx_Math">\displaystyle\bigl{|}\cE_{p}(\kappa;\mu,\nu)-\cE_{p}(\kappa;\mu,\nu^{\prime})%
\bigr{|}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex45.m2" class="ltx_Math">\displaystyle\leq|\Wp(\mu,\nu)-\Wp(\mu,\nu^{\prime})|+|\Wp(\kappa_{\sharp}\mu,%
\nu)-\Wp(\kappa_{\sharp}\mu,\nu^{\prime})|</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex46"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex46.m1" class="ltx_Math">\displaystyle\leq 2\Wp(\nu,\nu^{\prime})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex47"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex47.m1" class="ltx_Math">\displaystyle\leq 2\diam(\cY)\|\nu-\nu^{\prime}\|_{\tv},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the final inequality uses Fact <a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.∎</p>
</div>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Proof of <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
</h3>

<div id="A1.SS5.p1" class="ltx_para">
<p class="ltx_p">While the key ideas of this proof are straightforward, measurability issues require some care (we encourage the reader to skip such details on an initial read). In what follows, we equip all spaces of distributions with the weak topology and always employ Borel measurability. By the definition of a Markov kernel, <span id="A1.SS5.p1.m1" class="ltx_Math">x\in\cX\mapsto\kappa_{x}(A)</span> is a measurable function for each measurable <span id="A1.SS5.p1.m2" class="ltx_Math">A\subseteq\cY</span>. Thus, <span id="A1.SS5.p1.m3" class="ltx_Math">(x,x^{\prime})\in\cX^{2}\mapsto(\kappa_{x}(A),\kappa_{x^{\prime}}(B))</span> is measurable for fixed, measurable <span id="A1.SS5.p1.m4" class="ltx_Math">A,B\subseteq\cY</span>, implying that <span id="A1.SS5.p1.m5" class="ltx_Math">(x,x^{\prime})\in\cX^{2}\mapsto(\kappa_{x},\kappa_{x^{\prime}})\in\cP(\cY)^{2}</span> is measurable. Therefore, by Theorem 3.0.8 of <cite class="ltx_cite ltx_citemacro_cite">Toneian [<a href="#bib.bib42" title="" class="ltx_ref">2019</a>]</cite>, there exists a measurable map <span id="A1.SS5.p1.m6" class="ltx_Math">(x,x^{\prime})\in\cX^{2}\mapsto\gamma_{x,x^{\prime}}\in\Pi(\kappa_{x},\kappa_{%
x^{\prime}})</span> such that <span id="A1.SS5.p1.m7" class="ltx_Math">\gamma_{x,x^{\prime}}</span> is an OT plan for <span id="A1.SS5.p1.m8" class="ltx_Math">\Wp(\kappa_{x},\kappa_{x^{\prime}})</span> for all <span id="A1.SS5.p1.m9" class="ltx_Math">x,x^{\prime}\in\cX</span>.</p>
</div>
<div id="A1.SS5.p2" class="ltx_para">
<p class="ltx_p">Now, let <span id="A1.SS5.p2.m1" class="ltx_Math">\pi_{0}\in\Pi(\mu,\mu^{\prime})</span> be an OT plan for <span id="A1.SS5.p2.m2" class="ltx_Math">\Wp(\mu,\mu^{\prime})</span>, and define the joint law <span id="A1.SS5.p2.m3" class="ltx_Math">\pi</span> by <span id="A1.SS5.p2.m4" class="ltx_Math">\pi(A\times B\times C\times D)\defeq\iint_{A\times B}\gamma_{x,x^{\prime}}(C%
\times D)\dd\pi_{0}(x,x^{\prime})</span>, which is well-defined due to the measurability argument above. Taking <span id="A1.SS5.p2.m5" class="ltx_Math">(X,X^{\prime},Y,Y^{\prime})\sim\pi</span>, our construction ensures the following:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span id="A1.I1.i1.p1.m1" class="ltx_Math">X\sim\mu</span> and <span id="A1.I1.i1.p1.m2" class="ltx_Math">X^{\prime}\sim\mu^{\prime}</span> such that <span id="A1.I1.i1.p1.m3" class="ltx_Math">\E[\|X-X^{\prime}\|^{p}]=\rho^{p}</span>,</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span id="A1.I1.i2.p1.m1" class="ltx_Math">Y\sim\kappa_{X}</span> and <span id="A1.I1.i2.p1.m2" class="ltx_Math">Y^{\prime}\sim\kappa_{X^{\prime}}</span> such that <span id="A1.I1.i2.p1.m3" class="ltx_Math">\E[\|Y-Y^{\prime}\|^{p}|X,X^{\prime}]=\Wp(\kappa_{X},\kappa_{X^{\prime}})^{p}</span>.</p>
</div>
</li>
</ul>
<p class="ltx_p">Consequently, we bound
</p>
<table id="A5.EGx14" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex48"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex48.m1" class="ltx_Math">\displaystyle\E[\|Y-Y^{\prime}\|^{p}]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex48.m2" class="ltx_Math">\displaystyle=\E\left[\E\left[\|Y-Y^{\prime}\|^{p}\big{|}X,X^{\prime}\right]\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex49"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex49.m1" class="ltx_Math">\displaystyle=\E\left[\,\Wp(\kappa_{X},\kappa_{X^{\prime}})^{p}\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex50"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex50.m4" class="ltx_Math">\displaystyle\leq\E\left[L^{p}\|X-X^{\prime}\|^{\alpha p}\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(Hölder continuity of <span id="A1.Ex50.m1" class="ltx_Math">\kappa</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex51"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex51.m4" class="ltx_Math">\displaystyle\leq L^{p}\E\left[L\|X-X^{\prime}\|^{p}\right]^{\alpha}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(Jensen’s inequality, <span id="A1.Ex51.m1" class="ltx_Math">0&lt;\alpha\leq 1</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex52"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex52.m1" class="ltx_Math">\displaystyle=L^{p}\rho^{\alpha p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Moreover, using Minkoswki’s inequality, we compute</p>
<table id="A5.EGx15" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex53"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex53.m1" class="ltx_Math">\displaystyle\bigl{|}\E[\|Y-X\|^{p}]^{\frac{1}{p}}-\E[\|Y^{\prime}-X^{\prime}%
\|^{p}]^{\frac{1}{p}}\bigr{|}\leq\E[\|X-X^{\prime}\|^{p}]^{\frac{1}{p}}+\E[\|Y%
-Y^{\prime}\|^{p}]^{\frac{1}{p}}\leq\rho+L\rho^{\alpha}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Finally, we bound <span id="A1.SS5.p2.m6" class="ltx_Math">|\Wp(\mu,\nu)-\Wp(\mu^{\prime},\nu)|\leq\Wp(\mu,\mu^{\prime})\leq\rho</span> and</p>
<table id="A5.EGx16" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex54"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex54.m1" class="ltx_Math">\displaystyle|\Wp(\kappa_{\sharp}\mu,\nu)-\Wp(\kappa_{\sharp}\mu^{\prime},\nu)%
|\leq\Wp(\kappa_{\sharp}\mu,\kappa_{\sharp}\mu^{\prime})\leq\E[\|Y-Y^{\prime}%
\|^{p}]^{\frac{1}{p}}\leq L\rho^{\alpha}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The definition of <span id="A1.SS5.p2.m7" class="ltx_Math">\cE_{p}</span> and these bounds give the lemma.∎</p>
</div>
</section>
<section id="A1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Proof of <a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>
</h3>

<div id="A1.SS6.p1" class="ltx_para">
<p class="ltx_p">To show this result, we prove a slightly more general lemma.</p>
</div>
<div id="Thmlemma7" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 7</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma7.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fix <span id="Thmlemma7.p1.m1" class="ltx_Math">\mu,\mu^{\prime}\in\cP(\cX)</span>, <span id="Thmlemma7.p1.m2" class="ltx_Math">\nu\in\cP(\cY)</span>, and kernel <span id="Thmlemma7.p1.m3" class="ltx_Math">\kappa\in\cK(\cX,\cY)</span> with <span id="Thmlemma7.p1.m4" class="ltx_Math">\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu(x)\leq\Wp(\mu,\kappa_{\sharp}\mu)^{p}+%
\tau^{p}</span> and <span id="Thmlemma7.p1.m5" class="ltx_Math">\Wp(\kappa_{\sharp}\mu,\nu)\leq\tau</span> for some <span id="Thmlemma7.p1.m6" class="ltx_Math">\tau\geq 0</span>. Then, setting <span id="Thmlemma7.p1.m7" class="ltx_Math">\eps=\|\mu-\mu^{\prime}\|_{\tv}</span>, we have <span id="Thmlemma7.p1.m8" class="ltx_Math">\cE_{p}(\kappa;\mu^{\prime},\nu)\leq 3\diam(\cY)\eps^{1/p}+3\tau</span>.
</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A1.SS6.p2" class="ltx_para">
<p class="ltx_p">In what follows, we encourage the reader to focus on the <span id="A1.SS6.p2.m1" class="ltx_Math">p=1</span> case, where computations are more direct. Write <span id="A1.SS6.p2.m2" class="ltx_Math">\eps=\|\mu-\mu^{\prime}\|_{\tv}</span>. By the TV bound, there exist <span id="A1.SS6.p2.m3" class="ltx_Math">\alpha,\beta,\gamma\in\cM_{+}(\cX)</span> with <span id="A1.SS6.p2.m4" class="ltx_Math">\gamma(\cX)=1-\eps</span> and <span id="A1.SS6.p2.m5" class="ltx_Math">\alpha(\cX)=\beta(\cX)=\eps</span> such that <span id="A1.SS6.p2.m6" class="ltx_Math">\mu=\gamma+\alpha</span> and <span id="A1.SS6.p2.m7" class="ltx_Math">\mu^{\prime}=\gamma+\beta</span>.</p>
</div>
<div id="A1.SS6.p3" class="ltx_para">
<p class="ltx_p">First, we note that <span id="A1.SS6.p3.m1" class="ltx_Math">\kappa</span> must perform well on <span id="A1.SS6.p3.m2" class="ltx_Math">\gamma</span>. Specifically, we have</p>
<table id="A5.EGx17" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex55"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex55.m1" class="ltx_Math">\displaystyle\iint\|x</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex55.m2" class="ltx_Math">\displaystyle-y\|^{p}\dd\kappa_{x}(y)\dd\gamma(x)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex56"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex56.m4" class="ltx_Math">\displaystyle=\iint\|x-y\|^{p}\dd\kappa_{x}(y)\dd\mu(x)-\iint\|x-y\|^{p}\dd%
\kappa_{x}(y)\dd\alpha(x)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex56.m1" class="ltx_Math">\gamma=\mu-\alpha</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex57"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex57.m7" class="ltx_Math">\displaystyle\leq\Wp(\mu,\kappa_{\sharp}\mu)^{p}+\tau^{p}-\Wp(\alpha,\kappa_{%
\sharp}\alpha)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(error bound for <span id="A1.Ex57.m1" class="ltx_Math">\kappa</span>, def. of <span id="A1.Ex57.m2" class="ltx_Math">\Wp</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex58"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex58.m4" class="ltx_Math">\displaystyle\leq\Wp(\gamma,\kappa_{\sharp}\gamma)^{p}+\Wp(\alpha,\kappa_{%
\sharp}\alpha)^{p}+\tau^{p}-\Wp(\alpha,\kappa_{\sharp}\alpha)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex58.m1" class="ltx_Math">\mu=\gamma+\alpha</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex59"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex59.m1" class="ltx_Math">\displaystyle=\Wp(\gamma,\kappa_{\sharp}\gamma)^{p}+\tau^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex60"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex60.m4" class="ltx_Math">\displaystyle\leq\left(\Wp(\gamma,\kappa_{\sharp}\gamma)+\tau\right)^{p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex60.m1" class="ltx_Math">\ell_{p}\leq\ell_{1}</span>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, letting <span id="A1.SS6.p3.m3" class="ltx_Math">\kappa^{\prime}</span> be an optimal kernel for the <span id="A1.SS6.p3.m4" class="ltx_Math">\Wp(\mu^{\prime},\nu)</span> problem and writing <span id="A1.SS6.p3.m5" class="ltx_Math">D=\diam(\cY)</span>, we have</p>
<table id="A5.EGx18" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex61"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex61.m1" class="ltx_Math">\displaystyle\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu^{\prime}(x)\right)^{%
\frac{1}{p}}-\Wp(\mu^{\prime},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex62"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex62.m4" class="ltx_Math">\displaystyle=\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex62.m5" class="ltx_Math">\displaystyle\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu^{\prime}(x)\right)^{%
\frac{1}{p}}-\left(\iint\|y-x\|^{p}\dd\kappa^{\prime}_{x}(y)\dd\mu^{\prime}(x)%
\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(optimality of <span id="A1.Ex62.m1" class="ltx_Math">\kappa^{\prime}</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex63"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex63.m4" class="ltx_Math">\displaystyle=\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex63.m5" class="ltx_Math">\displaystyle\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\gamma(x)+\iint\|y-x\|^{%
p}\dd\kappa_{x}(y)\dd\beta(x)\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex63.m1" class="ltx_Math">\mu^{\prime}=\gamma+\beta</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex64"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex64.m1" class="ltx_Math">\displaystyle-\left(\iint\|y-x\|^{p}\dd\kappa^{\prime}_{x}(y)\dd\gamma(x)+%
\iint\|y-x\|^{p}\dd\kappa^{\prime}_{x}(y)\dd\beta(x)\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex65"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex65.m1" class="ltx_Math">\displaystyle\leq\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex65.m2" class="ltx_Math">\displaystyle\Bigg{(}\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\gamma(x)%
\right)^{\!\frac{1}{p}}-\left(\iint\|y-x\|^{p}\dd\kappa^{\prime}_{x}(y)\dd%
\gamma(x)\right)^{\!\frac{1}{p}}\right]_{+}^{\!p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex66"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex66.m1" class="ltx_Math">\displaystyle+\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\beta(x)\right)^{%
\!\frac{1}{p}}-\left(\iint\|y-x\|^{p}\dd\kappa^{\prime}_{x}(y)\dd\beta(x)%
\right)^{\!\frac{1}{p}}\right]_{+}^{\!p}\,\,\Bigg{)}^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex67"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex67.m1" class="ltx_Math">\displaystyle\leq\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex67.m2" class="ltx_Math">\displaystyle\Bigg{(}\left[\Wp(\gamma,\kappa_{\sharp}\gamma)+\tau-\Wp(\gamma,%
\kappa^{\prime}_{\sharp}\gamma)\right]_{+}^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex68"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex68.m1" class="ltx_Math">\displaystyle+\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\beta(x)\right)^{%
\!\frac{1}{p}}-\left(\iint\|y-x\|^{p}\dd\kappa^{\prime}_{x}(y)\dd\beta(x)%
\right)^{\!\frac{1}{p}}\right]_{+}^{\!p}\,\,\Bigg{)}^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex69"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex69.m1" class="ltx_Math">\displaystyle\leq\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex69.m2" class="ltx_Math">\displaystyle\Bigg{(}\left(\Wp(\kappa_{\sharp}\gamma,\kappa^{\prime}_{\sharp}%
\gamma)+\tau\right)^{p}+\iiint\|y-y^{\prime}\|^{p}\dd\kappa_{x}(y)\dd\kappa^{%
\prime}_{x}(y^{\prime})\dd\beta(x)\Bigg{)}^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex70"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex70.m1" class="ltx_Math">\displaystyle\leq\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex70.m2" class="ltx_Math">\displaystyle\left(\left(\Wp(\kappa_{\sharp}\gamma,\kappa^{\prime}_{\sharp}%
\gamma)+\tau\right)^{p}+\eps D^{p}\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)</span></td>
</tr></tbody>
<tbody id="A1.Ex71"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex71.m4" class="ltx_Math">\displaystyle\leq\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex71.m5" class="ltx_Math">\displaystyle\Wp(\kappa_{\sharp}\gamma,\kappa^{\prime}_{\sharp}\gamma)+\tau+D%
\eps^{\frac{1}{p}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex71.m1" class="ltx_Math">\ell_{p}\leq\ell_{1}</span>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">The first inequality uses that <span id="A1.SS6.p3.m6" class="ltx_Math">(A^{p}+B^{p})^{1/p}-(a^{p}+b^{p})^{1/p}\leq([A-a]_{+}^{p}+[B-b]_{+}^{p})^{1/p}</span>, which can be obtained by rearranging the <span id="A1.SS6.p3.m7" class="ltx_Math">\ell_{p}</span> triangle inequality and using that <span id="A1.SS6.p3.m8" class="ltx_Math">A=[A-a]_{+}+A\land a</span>. The second inequality uses the previous bound and the fact that <span id="A1.SS6.p3.m9" class="ltx_Math">\kappa^{\prime}</span> is feasible for the <span id="A1.SS6.p3.m10" class="ltx_Math">\Wp(\gamma,\kappa^{\prime}_{\sharp}\gamma)</span> problem. The third uses the <span id="A1.SS6.p3.m11" class="ltx_Math">\Wp</span> triangle inequality and Minkoswki’s inequality.</p>
</div>
<div id="A1.SS6.p4" class="ltx_para">
<p class="ltx_p">We next bound <span id="A1.SS6.p4.m1" class="ltx_Math">\Wp(\kappa_{\sharp}\gamma,\kappa^{\prime}_{\sharp}\gamma)</span>. Let <span id="A1.SS6.p4.m2" class="ltx_Math">\pi\in\Pi(\kappa_{\sharp}\mu,\nu)</span> be an optimal plan for <span id="A1.SS6.p4.m3" class="ltx_Math">\Wp(\kappa_{\sharp}\mu,\nu)</span> and define <span id="A1.SS6.p4.m4" class="ltx_Math">\lambda\in(1-\eps)\cP(\cY)</span> by <span id="A1.SS6.p4.m5" class="ltx_Math">\lambda(\cdot)=\int\pi(\cdot|x)\dd\gamma(x)</span>. By construction, <span id="A1.SS6.p4.m6" class="ltx_Math">\Wp(\kappa_{\sharp}\gamma,\lambda)\leq\Wp(\kappa_{\sharp}\mu,\nu)\leq\tau</span>. Moreover, both <span id="A1.SS6.p4.m7" class="ltx_Math">\lambda</span> and <span id="A1.SS6.p4.m8" class="ltx_Math">\kappa^{\prime}_{\sharp}\gamma</span> are submeasures of <span id="A1.SS6.p4.m9" class="ltx_Math">\nu</span> with mass <span id="A1.SS6.p4.m10" class="ltx_Math">1-\eps</span>, and so they must share common mass at least <span id="A1.SS6.p4.m11" class="ltx_Math">1-2\eps</span>. This implies that their TV distance is at most <span id="A1.SS6.p4.m12" class="ltx_Math">\eps</span>, and so Fact <a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives that</p>
<table id="A1.E8" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A1.E8.m1" class="ltx_Math">\Wp(\kappa_{\sharp}\gamma,\kappa^{\prime}_{\sharp}\gamma)\leq\tau+\Wp(\lambda,%
\kappa^{\prime}_{\sharp}\gamma)\leq\tau+D\eps^{\frac{1}{p}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr>
</table>
<p class="ltx_p">Thus, the previous bound on the optimality gap can be tightened to</p>
<table id="A1.Ex72" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A1.Ex72.m1" class="ltx_Math">\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu^{\prime}(x)\right)^{\frac{1}{p}}-%
\Wp(\mu^{\prime},\nu)\leq\tau+2D\eps^{\frac{1}{p}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Similarly, we bound the feasibility gap by</p>
<table id="A5.EGx19" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex73"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex73.m7" class="ltx_Math">\displaystyle\Wp(\kappa_{\sharp}\mu^{\prime},\nu)^{p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex73.m8" class="ltx_Math">\displaystyle=\Wp(\kappa_{\sharp}\gamma+\kappa_{\sharp}\beta,\kappa^{\prime}_{%
\sharp}\gamma+\kappa^{\prime}_{\sharp}\beta)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex73.m1" class="ltx_Math">\mu^{\prime}=\gamma+\beta</span>, <span id="A1.Ex73.m2" class="ltx_Math">\kappa^{\prime}_{\sharp}\gamma=\nu</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex74"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex74.m4" class="ltx_Math">\displaystyle\leq\Wp(\kappa_{\sharp}\gamma,\kappa^{\prime}_{\sharp}\gamma)^{p}%
+\Wp(\kappa_{\sharp}\beta,\kappa^{\prime}_{\sharp}\beta)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(joint convexity of <span id="A1.Ex74.m1" class="ltx_Math">\Wp^{p}</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex75"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex75.m1" class="ltx_Math">\displaystyle\leq\tau^{p}+(\tau+D\eps^{\frac{1}{p}})^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(Fact <a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Eq. <a href="#A1.E8" title="(8) ‣ A.6 Proof of 5 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>)</span></td>
</tr></tbody>
<tbody id="A1.Ex76"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex76.m4" class="ltx_Math">\displaystyle\leq(2\tau+D\eps^{\frac{1}{p}})^{p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex76.m1" class="ltx_Math">\ell_{p}\leq\ell_{1}</span>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Combining, we have that <span id="A1.SS6.p4.m13" class="ltx_Math">\cE_{p}(\kappa;\mu^{\prime},\nu)\leq 3\tau+3D\eps^{1/p}</span>, as desired.∎</p>
</div>
</div>
<div id="A1.SS6.p5" class="ltx_para">
<p class="ltx_p">We now seek to find a suitable error bound <span id="A1.SS6.p5.m1" class="ltx_Math">\tau</span> in terms of <span id="A1.SS6.p5.m2" class="ltx_Math">\cE_{p}(\kappa;\mu,\nu)</span>. We compute</p>
<table id="A5.EGx20" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex77"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex77.m1" class="ltx_Math">\displaystyle\invisequals\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu(x)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex78"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex78.m4" class="ltx_Math">\displaystyle\leq\left(\Wp(\mu,\nu)+\cE_{p}(\kappa;\mu,\nu)\right)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(def. of <span id="A1.Ex78.m1" class="ltx_Math">\cE_{p}</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex79"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex79.m4" class="ltx_Math">\displaystyle\leq\left(\Wp(\mu,\kappa_{\sharp}\mu)+2\cE_{p}(\kappa;\mu,\nu)%
\right)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex79.m1" class="ltx_Math">\Wp(\kappa_{\sharp}\mu,\nu)\leq\cE_{p}(\kappa;\mu,\nu)</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex80"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex80.m1" class="ltx_Math">\displaystyle\leq\Wp(\mu,\kappa_{\sharp}\mu)^{p}+2p\cE_{p}(\kappa;\mu,\nu)%
\left(\Wp(\mu,\kappa_{\sharp}\mu)\lor 2\cE_{p}(\kappa;\mu,\nu)\right)^{p-1}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex81"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex81.m4" class="ltx_Math">\displaystyle\leq\Wp(\mu,\kappa_{\sharp}\mu)^{p}+2p\cE_{p}(\kappa;\mu,\nu)%
\left(\Wp(\mu,\nu)+3\cE_{p}(\kappa;\mu,\nu)\right)^{p-1}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A1.Ex81.m1" class="ltx_Math">\Wp(\kappa_{\sharp}\mu,\nu)\leq\cE_{p}(\kappa;\mu,\nu)</span>)</span></td>
</tr></tbody>
<tbody id="A1.Ex82"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex82.m1" class="ltx_Math">\displaystyle\leq\Wp(\mu,\kappa_{\sharp}\mu)^{p}+2^{p-1}p\cE_{p}(\kappa;\mu,%
\nu)\Wp(\mu,\nu)^{p-1}+3^{p}2^{p-1}p\cE_{p}(\kappa;\mu,\nu)^{p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, we can take</p>
<table id="A5.EGx21" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex83"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex83.m1" class="ltx_Math">\displaystyle\tau</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex83.m2" class="ltx_Math">\displaystyle=\cE_{p}(\kappa;\mu,\nu)\lor\left[2^{p-1}p\cE_{p}(\kappa;\mu,\nu)%
\Wp(\mu,\nu)^{p-1}+3^{p}2^{p-1}p\cE_{p}(\kappa;\mu,\nu)^{p}\right]^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex84"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex84.m1" class="ltx_Math">\displaystyle\leq 3\cE_{p}(\kappa;\mu,\nu)^{\frac{1}{p}}\Wp(\mu,\nu)^{\frac{p-%
1}{p}}+7\cE_{p}(\kappa;\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Plugging into <a href="#Thmlemma7" title="Lemma 7. ‣ A.6 Proof of 5 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> gives that</p>
<table id="A1.Ex85" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A1.Ex85.m1" class="ltx_Math">\cE_{p}(\kappa;\mu^{\prime},\nu)\leq 3D\eps^{1/p}+9\cE_{p}(\kappa;\mu,\nu)^{%
\frac{1}{p}}\Wp(\mu,\nu)^{\frac{p-1}{p}}+21\cE_{p}(\kappa;\mu,\nu),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">as desired. The <span id="A1.SS6.p5.m3" class="ltx_Math">p=1</span> result is immediate.∎</p>
</div>
</section>
<section id="A1.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Proof of <a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>
</h3>

<div id="A1.SS7.p1" class="ltx_para">
<p class="ltx_p">First, we note that <span id="A1.SS7.p1.m1" class="ltx_Math">\kappa_{\sharp}(\lambda_{\sharp}\mu)=(\kappa\circ\lambda)_{\sharp}\mu</span> by the definition of kernel composition. This implies that the two feasibility gaps coincide. Moreover, by Minkowski’s inequality, we have</p>
<table id="A5.EGx22" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex86"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex86.m1" class="ltx_Math">\displaystyle\left|\left(\iint\|y-z\|^{p}\dd\kappa_{z}(y)\dd(\lambda_{\sharp}%
\mu)(z)\right)^{\frac{1}{p}}-\left(\iint\|y-x\|^{p}\dd(\kappa\circ\lambda)_{x}%
(y)\dd\mu(x)\right)^{\frac{1}{p}}\right|</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A1.Ex87"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex87.m1" class="ltx_Math">\displaystyle\leq\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A1.Ex87.m2" class="ltx_Math">\displaystyle\left(\iint\|z-x\|^{p}\dd\lambda_{x}(z)\dd\mu(x)\right)^{\frac{1}%
{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and
</p>
<table id="A5.EGx23" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.Ex88"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A1.Ex88.m1" class="ltx_Math">\displaystyle\left|\Wp(\lambda_{\sharp}\mu,\nu)-\Wp(\mu,\nu)\right|\leq\Wp(\mu%
,\lambda_{\sharp}\mu)\leq\left(\iint\|z-x\|^{p}\dd\lambda_{x}(z)\dd\mu(x)%
\right)^{\frac{1}{p}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Combining these two error bounds gives the lemma.∎</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Proofs for <a href="#S3" title="3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>
</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Proof of <a href="#Thmtheorem1" title="Theorem 1 (Entropic kernel estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p class="ltx_p">By the support constraint, our cost <span id="A2.SS1.p1.m1" class="ltx_Math">\|x-y\|^{p}</span> is <span id="A2.SS1.p1.m2" class="ltx_Math">pd^{(p-1)/2}</span>-Lipschitz over <span id="A2.SS1.p1.m3" class="ltx_Math">\cX\times\cY</span>. Thus, by Theorem 1 of <cite class="ltx_cite ltx_citemacro_cite">Genevay et al. [<a href="#bib.bib11" title="" class="ltx_ref">2019</a>]</cite>, we have</p>
<table id="A2.Ex89" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex89.m1" class="ltx_Math">S_{p,\tau}(\hat{\mu}_{n},\hat{\nu}_{n})\leq\Wp(\hat{\mu}_{n},\hat{\nu}_{n})^{p%
}+2\tau d\log\bigl{(}e^{2}pd^{p/2-1}\tau^{-1}\bigr{)}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Since <span id="A2.SS1.p1.m4" class="ltx_Math">\hat{\pi}_{\tau,n}</span> achieves the left hand side above and KL divergence is non-negative, we have</p>
<table id="A2.Ex90" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex90.m1" class="ltx_Math">\iint\|x-y\|^{p}\dd\hat{\kappa}_{n}(y|x)\dd\hat{\mu}_{n}(x)\leq\Wp(\hat{\mu}_{%
n},\hat{\nu}_{n})^{p}+2\tau d\log\bigl{(}e^{2}pd^{p/2-1}\tau^{-1}\bigr{)}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Taking <span id="A2.SS1.p1.m5" class="ltx_Math">p</span>th roots and noting that <span id="A2.SS1.p1.m6" class="ltx_Math">(\hat{\kappa}_{n})_{\sharp}\hat{\mu}_{n}=\hat{\nu}_{n}</span>, this implies that</p>
<table id="A2.Ex91" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex91.m1" class="ltx_Math">\cE_{p}(\hat{\kappa}_{n};\hat{\mu}_{n},\hat{\nu}_{n})\leq\left[2\tau d\log%
\bigl{(}e^{2}pd^{p/2-1}\tau^{-1}\bigr{)}\right]^{\frac{1}{p}}\leq 4(\tau d)^{%
\frac{1}{p}}\log(e^{2}d\tau^{-1}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Now, by (<a href="#S3.E7" title="(7) ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>), <span id="A2.SS1.p1.m7" class="ltx_Math">(\hat{\kappa}_{n})_{x}</span> is obtained by applying softmax to <span id="A2.SS1.p1.m8" class="ltx_Math">v(x)\defeq\bigl{(}(g_{\tau}(Y_{i})-\|x-Y_{i}\|^{p})/\tau\bigr{)}_{i=1}^{n}\in%
\R^{n}</span>. Since the <span id="A2.SS1.p1.m9" class="ltx_Math">\ell_{1},\ell_{\infty}</span> Lipschitz constant of the softmax operation is <span id="A2.SS1.p1.m10" class="ltx_Math">\leq 1</span>, we have</p>
<table id="A2.Ex92" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex92.m1" class="ltx_Math">\|(\hat{\kappa}_{n})_{x}-(\hat{\kappa}_{n})_{x^{\prime}}\|_{\tv}\leq\frac{1}{2%
}\|v(x)-v(x^{\prime})\|_{\infty}\leq\frac{1}{2}pd^{(p-1)/2}\tau^{-1}\|x-x^{%
\prime}\|_{2}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">for all <span id="A2.SS1.p1.m11" class="ltx_Math">x,x^{\prime}\in[0,1]^{d}</span>. Thus, by <a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <span id="A2.SS1.p1.m12" class="ltx_Math">\hat{\kappa}_{n}</span> is Hölder continuous under <span id="A2.SS1.p1.m13" class="ltx_Math">\Wp</span> with exponent <span id="A2.SS1.p1.m14" class="ltx_Math">1/p</span> and constant <span id="A2.SS1.p1.m15" class="ltx_Math">2\sqrt{d}\tau^{-1/p}</span>. Applying <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> now gives</p>
<table id="A5.EGx24" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex93"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex93.m1" class="ltx_Math">\displaystyle\cE_{p}(\hat{\kappa}_{n};\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex93.m2" class="ltx_Math">\displaystyle\leq\cE_{p}(\hat{\kappa}_{n};\mu,\hat{\nu}_{n})+\Wp(\hat{\nu}_{n}%
,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex94"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex94.m1" class="ltx_Math">\displaystyle\leq\cE_{p}(\hat{\kappa}_{n};\hat{\mu}_{n},\hat{\nu}_{n})+2\Wp(%
\mu,\hat{\mu}_{n})+4\sqrt{d}\,\Wp(\mu,\hat{\mu})^{\frac{1}{p}}\tau^{-\frac{1}{%
p}}+\Wp(\nu,\hat{\nu}_{n})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex95"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex95.m1" class="ltx_Math">\displaystyle\leq 4(\tau d)^{\frac{1}{p}}\log(e^{2}d\tau^{-1})+4\sqrt{d}\,\Wp(%
\mu,\hat{\mu})^{\frac{1}{p}}\tau^{-\frac{1}{p}}+2\Wp(\mu,\hat{\mu}_{n})+\Wp(%
\nu,\hat{\nu}_{n}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Taking expectations, applying <a href="#Thmlemma1" title="Lemma 1 (\Wp empirical convergence, Lei, 2020). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and plugging in <span id="A2.SS1.p1.m16" class="ltx_Math">\tau</span> gives the theorem.∎</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Minimax Lower Bound under Sampling</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p class="ltx_p">Fix <span id="A2.SS2.p1.m1" class="ltx_Math">\mu=\delta_{0}</span>, so that the constant kernel <span id="A2.SS2.p1.m2" class="ltx_Math">\kappa^{\star}</span> defined by <span id="A2.SS2.p1.m3" class="ltx_Math">\kappa^{\star}_{x}\equiv\nu</span> is optimal. Note that the error <span id="A2.SS2.p1.m4" class="ltx_Math">\cE_{p}(\kappa;\mu,\nu)</span> of any kernel <span id="A2.SS2.p1.m5" class="ltx_Math">\kappa</span> is thus lower bounded by the feasibility gap <span id="A2.SS2.p1.m6" class="ltx_Math">\Wp(\kappa_{0},\nu)</span>. Since we only observe <span id="A2.SS2.p1.m7" class="ltx_Math">n</span> i.i.d. samples from <span id="A2.SS2.p1.m8" class="ltx_Math">\nu\in\cP([0,1]^{d})</span>, any upper bound on an estimator for this problem instance also gives an upper bound for <span id="A2.SS2.p1.m9" class="ltx_Math">n</span>-sample distribution estimation of <span id="A2.SS2.p1.m10" class="ltx_Math">\nu</span> under <span id="A2.SS2.p1.m11" class="ltx_Math">\Wp</span>. However, the minimax lower bound of <cite class="ltx_cite ltx_citemacro_cite">Singh and Póczos [<a href="#bib.bib41" title="" class="ltx_ref">2018</a>]</cite> implies that no distribution estimator can achieve <span id="A2.SS2.p1.m12" class="ltx_Math">\Wp</span> error less than <span id="A2.SS2.p1.m13" class="ltx_Math">n^{-1/(2p\lor d)}</span> for all <span id="A2.SS2.p1.m14" class="ltx_Math">\nu\in\cP([0,1]^{d})</span>.</p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Proof of <a href="#Thmtheorem2" title="Theorem 2 (Rounding estimator). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p class="ltx_p">We start with some helpful lemmas.</p>
</div>
<div id="Thmlemma8" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 8</span></span><span class="ltx_text ltx_font_bold"> </span>(<cite class="ltx_cite ltx_citemacro_cite">Rigollet [<a href="#bib.bib37" title="" class="ltx_ref">2015</a>]</cite>, Theorem 1.14)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma8.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <span id="Thmlemma8.p1.m1" class="ltx_Math">\mu\in\cP(\R)</span> be 1-sub-Gaussian. Then, for <span id="Thmlemma8.p1.m2" class="ltx_Math">X_{1},\dots,X_{n}</span> sampled i.i.d. from <span id="Thmlemma8.p1.m3" class="ltx_Math">\mu</span>, we have <span id="Thmlemma8.p1.m4" class="ltx_Math">\max_{i=1,\dots,n}X_{i}\leq\sqrt{2\log(n/\delta)}</span> with probability at least <span id="Thmlemma8.p1.m5" class="ltx_Math">1-\delta</span>.</span></p>
</div>
</div>
<div id="Thmlemma9" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 9</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma9.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <span id="Thmlemma9.p1.m1" class="ltx_Math">\mu\in\cP(\R^{d})</span> be 1-sub-Gaussian and let <span id="Thmlemma9.p1.m2" class="ltx_Math">\cP</span> denote the regular partition of <span id="Thmlemma9.p1.m3" class="ltx_Math">\R^{d}</span> into cubes of side-length <span id="Thmlemma9.p1.m4" class="ltx_Math">r&gt;0</span>. Then, for any choice of rounding map <span id="Thmlemma9.p1.m5" class="ltx_Math">r_{\cP}</span>, we have</span></p>
<table id="A2.Ex96" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex96.m1" class="ltx_Math">\E\left[\|(r_{\cP})_{\sharp}(\hat{\mu}_{n}-\mu)\|_{\tv}\right]=\widetilde{O}%
\left(\sqrt{\frac{5^{d}r^{-d}}{n}}\right).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS3.p2" class="ltx_para">
<p class="ltx_p">Let <span id="A2.SS3.p2.m1" class="ltx_Math">B</span> denote a ball of radius <span id="A2.SS3.p2.m2" class="ltx_Math">R=\sqrt{2\log(n)}</span> centered at the origin, so that <span id="A2.SS3.p2.m3" class="ltx_Math">\mu(B)\geq 1-1/n</span> by <a href="#Thmlemma8" title="Lemma 8 (Rigollet [2015], Theorem 1.14). ‣ B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
Write <span id="A2.SS3.p2.m4" class="ltx_Math">\cP_{R}</span> for the subset of partition blocks <span id="A2.SS3.p2.m5" class="ltx_Math">P\in\cP</span> which intersect <span id="A2.SS3.p2.m6" class="ltx_Math">B</span>, and note that <span id="A2.SS3.p2.m7" class="ltx_Math">|\cP_{R}|\leq\vol(B)r^{-d}\leq(3R/r)^{d}</span>. We then bound</p>
<table id="A5.EGx25" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex97"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex97.m1" class="ltx_Math">\displaystyle\E\left[\|(r_{\cP})_{\sharp}(\hat{\mu}_{n}-\mu)\|_{\tv}\right]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex97.m2" class="ltx_Math">\displaystyle=\frac{1}{2}\E\left[\sum_{P\in\cP}|(\hat{\mu}_{n}-\mu)(P)|\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex98"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex98.m1" class="ltx_Math">\displaystyle=\frac{1}{2}\E\left[\sum_{P\in\cP_{R}}|(\hat{\mu}_{n}-\mu)(P)|+%
\sum_{P\in\cP\setminus\cP_{R}}|(\hat{\mu}_{n}-\mu)(P)|\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex99"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex99.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{\frac{|\cP_{R}|}{n}}+\E\left[\sum_{P\in\cP\setminus%
\cP_{R}}\hat{\mu}_{n}(P)+\mu(P)\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex100"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex100.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{\frac{|\cP_{R}|}{n}}+\mu(\R^{d}\setminus B)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex101"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex101.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{\frac{(3\sqrt{2\log(n)})^{d}r^{-d}}{n}}+\frac{1}{n}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex102"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex102.m1" class="ltx_Math">\displaystyle=\widetilde{O}\left(\sqrt{\frac{5^{d}r^{-d}}{n}}\right),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.
∎</p>
</div>
</div>
<div id="Thmlemma10" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 10</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma10.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">There exists a partition <span id="Thmlemma10.p1.m1" class="ltx_Math">\cP</span> parameterized by <span id="Thmlemma10.p1.m2" class="ltx_Math">\delta&gt;0</span> such that, for all <span id="Thmlemma10.p1.m3" class="ltx_Math">\mu\in\cP(\R^{d})</span> with <span id="Thmlemma10.p1.m4" class="ltx_Math">\E_{\mu}[\|X\|^{p+1}]\leq 1</span> and any rounding map <span id="Thmlemma10.p1.m5" class="ltx_Math">r_{\cP}</span>, we have <span id="Thmlemma10.p1.m6" class="ltx_Math">\E\left[\|(r_{\cP})_{\sharp}(\hat{\mu}_{n}-\mu)\|_{\tv}\right]=\widetilde{O}%
\bigl{(}\sqrt{\delta^{-d}/n}\bigr{)}</span> and <span id="Thmlemma10.p1.m7" class="ltx_Math">\|r_{\cP}-\Id\|_{L^{p}(\mu)}\lesssim\delta</span>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS3.p3" class="ltx_para">
<p class="ltx_p">Let <span id="A2.SS3.p3.m1" class="ltx_Math">X_{0}</span> be a minimal <span id="A2.SS3.p3.m2" class="ltx_Math">(3\delta)</span>-covering of the unit ball, denoted <span id="A2.SS3.p3.m3" class="ltx_Math">S_{0}</span>. In particular, this implies that <span id="A2.SS3.p3.m4" class="ltx_Math">|X_{0}|\leq\delta^{-d}</span>. Now, take <span id="A2.SS3.p3.m5" class="ltx_Math">\cP_{0}</span> to be the Voronoi partition of <span id="A2.SS3.p3.m6" class="ltx_Math">S_{0}</span> induced by <span id="A2.SS3.p3.m7" class="ltx_Math">X_{0}</span>, so that <span id="A2.SS3.p3.m8" class="ltx_Math">\cP_{0}</span> has at most <span id="A2.SS3.p3.m9" class="ltx_Math">\delta^{-d}</span> cells of diameter at most <span id="A2.SS3.p3.m10" class="ltx_Math">6\delta</span>. Then for each integer <span id="A2.SS3.p3.m11" class="ltx_Math">i&gt;0</span>, set <span id="A2.SS3.p3.m12" class="ltx_Math">S_{i}\defeq 2^{i}S_{0}\setminus 2^{i-1}S_{0}</span>, and let <span id="A2.SS3.p3.m13" class="ltx_Math">\cP_{i}</span> be the dilated partition <span id="A2.SS3.p3.m14" class="ltx_Math">\{(2^{i}P)\cap S_{i}:P\in\cP_{0}\}</span>. By construction, <span id="A2.SS3.p3.m15" class="ltx_Math">|\cP_{i}|\leq\delta^{-d}</span> and each <span id="A2.SS3.p3.m16" class="ltx_Math">P\in\cP_{i}</span> has diameter at most <span id="A2.SS3.p3.m17" class="ltx_Math">2^{i}\cdot 6\delta</span>, for all <span id="A2.SS3.p3.m18" class="ltx_Math">i\geq 0</span>. Moreover, by Markov’s inequality, we have</p>
<table id="A2.Ex103" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex103.m1" class="ltx_Math">\mu(S_{i})\leq\Pr_{\mu}\bigl{(}\|X\|&gt;2^{i-1}\bigr{)}=\Pr_{\mu}\bigl{(}\|X\|^{p%
+1}&gt;2^{(p+1)(i-1)}\bigr{)}\leq 2^{(1-i)(p+1)}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">for each <span id="A2.SS3.p3.m19" class="ltx_Math">i&gt;0</span>. We thus bound</p>
<table id="A5.EGx26" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex104"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex104.m1" class="ltx_Math">\displaystyle\E\left[\|(r_{\cP})_{\sharp}(\mu-\hat{\mu}_{n})\|_{\tv}\right]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex104.m2" class="ltx_Math">\displaystyle=\E\left[\sum_{i=0}^{\infty}\sum_{P\in\cP_{i}}|(\mu-\hat{\mu}_{n}%
)(P)|\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex105"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex105.m1" class="ltx_Math">\displaystyle\leq\sum_{i=0}^{\infty}\sum_{P\in\cP_{i}}\sqrt{\Var_{\mu^{\otimes
n%
}}[\hat{\mu}_{n}(P)]}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex106"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex106.m1" class="ltx_Math">\displaystyle\leq\frac{1}{\sqrt{n}}\cdot\sum_{i=0}^{\infty}\sum_{P\in\cP_{i}}%
\sqrt{\mu(P)}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex107"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex107.m1" class="ltx_Math">\displaystyle\leq\frac{1}{\sqrt{n}}\cdot\sum_{i=0}^{\infty}\sqrt{|\cP_{i}|\mu(%
S_{i})}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex108"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex108.m1" class="ltx_Math">\displaystyle\leq(\delta^{d}n)^{-\frac{1}{2}}\cdot\left(1+\sum_{i=1}^{\infty}2%
^{\frac{1-i}{2}}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex109"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex109.m1" class="ltx_Math">\displaystyle\lesssim(\delta^{d}n)^{-\frac{1}{2}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Similarly, we bound</p>
<table id="A5.EGx27" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex110"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex110.m1" class="ltx_Math">\displaystyle\|r_{\cP}-\Id\|_{L^{p}(\mu)}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex110.m2" class="ltx_Math">\displaystyle\leq\left(\sum_{i=0}^{\infty}\sum_{P\in\cP_{i}}\mu(P)\diam(P)^{p}%
\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex111"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex111.m1" class="ltx_Math">\displaystyle\leq\left(\sum_{i=0}^{\infty}(2^{i}\cdot 6\delta)^{p}\mu(S_{i})%
\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex112"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex112.m1" class="ltx_Math">\displaystyle\lesssim\left(\sum_{i=0}^{\infty}(2^{i}\delta)^{p}2^{(1-i)(p+1)}%
\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex113"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex113.m1" class="ltx_Math">\displaystyle=\delta\left(\sum_{i=0}^{\infty}2^{p+1-i}\right)^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex114"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex114.m1" class="ltx_Math">\displaystyle\lesssim\delta,</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.
∎</p>
</div>
</div>
<div id="A2.SS3.p4" class="ltx_para">
<p class="ltx_p">We now prove the theorem. First, note that for <span id="A2.SS3.p4.m1" class="ltx_Math">D=\sqrt{4\log(n)}</span>, we have <span id="A2.SS3.p4.m2" class="ltx_Math">\max_{i=1,\dots,n}\|Y_{i}\|\leq DS</span> with probability at least <span id="A2.SS3.p4.m3" class="ltx_Math">1/n</span>, by <a href="#Thmlemma8" title="Lemma 8 (Rigollet [2015], Theorem 1.14). ‣ B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Now, for a general partition <span id="A2.SS3.p4.m4" class="ltx_Math">\cP</span>, we bound
</p>
<table id="A5.EGx28" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex115"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex115.m1" class="ltx_Math">\displaystyle\cE_{p}(\hat{\kappa}_{n};\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex115.m2" class="ltx_Math">\displaystyle=\cE_{p}(\bar{\kappa}_{n}\circ r_{\cP};\mu,\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex116"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex116.m1" class="ltx_Math">\displaystyle\leq\cE_{p}(\bar{\kappa}_{n};\mu^{\prime},\nu)+2\,\|r_{\cP}-\Id\|%
_{L^{p}(\mu)}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>)</span></td>
</tr></tbody>
<tbody id="A2.Ex117"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex117.m1" class="ltx_Math">\displaystyle\leq\cE_{p}(\bar{\kappa}_{n};\mu^{\prime},\hat{\nu}_{n})+2\,\|r_{%
\cP}-\Id\|_{L^{p}(\mu)}+\Wp(\nu,\hat{\nu}_{n})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)</span></td>
</tr></tbody>
<tbody id="A2.Ex118"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex118.m1" class="ltx_Math">\displaystyle\lesssim\|(r_{\cP})_{\sharp}(\mu-\hat{\mu}_{n})\|_{\tv}^{1/p}%
\cdot\diam(\supp(\hat{\nu}_{n}))+\delta^{\frac{1}{p}}+\|r_{\cP}-\Id\|_{L^{p}(%
\mu)}+\Wp(\nu,\hat{\nu}_{n}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the last inequality follows by <a href="#Thmlemma7" title="Lemma 7. ‣ A.6 Proof of 5 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and our choice of <span id="A2.SS3.p4.m5" class="ltx_Math">\bar{\kappa}_{n}</span>. Applying this bound for the regular cube partition and taking expectations, we bound</p>
<table id="A5.EGx29" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex119"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex119.m1" class="ltx_Math">\displaystyle\E[\cE_{p}(\hat{\kappa}_{n};\mu,\nu)]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex119.m2" class="ltx_Math">\displaystyle\lesssim D\E\left[\|(r_{\cP})_{\sharp}(\mu-\hat{\mu}_{n})\|_{\tv}%
\right]^{1/p}+\frac{1}{n}+\delta^{\frac{1}{p}}+\sqrt{d}r+\Wp(\nu,\hat{\nu}_{n})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex120"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex120.m1" class="ltx_Math">\displaystyle\lesssim\widetilde{O}\left(\frac{5^{d}r^{-d}}{n}\right)^{\frac{1}%
{2p}}+\delta^{\frac{1}{p}}+\sqrt{d}r+\widetilde{O}_{p}\left(n^{-\frac{1}{d\lor
2%
p}}\right).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmlemma8" title="Lemma 8 (Rigollet [2015], Theorem 1.14). ‣ B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_ERROR undefined">\crefmiddleconjunction</span><a href="#Thmlemma9" title="Lemma 9. ‣ B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_ERROR undefined">\creflastconjunction</span><a href="#Thmlemma1" title="Lemma 1 (\Wp empirical convergence, Lei, 2020). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Taking <span id="A2.SS3.p4.m6" class="ltx_Math">r=n^{-1/(d+2p)}</span>, we obtain <span id="A2.SS3.p4.m7" class="ltx_Math">\E[\cE_{p}(\hat{\kappa}_{n};\mu,\nu)]=\widetilde{O}_{p,d}(n^{-1/(d+2p)})+%
\delta^{1/p}</span>. The same rate is obtained under bounded <span id="A2.SS3.p4.m8" class="ltx_Math">2p</span>th moments by using the alternative partition from <a href="#Thmlemma10" title="Lemma 10. ‣ B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Thus, to achieve the desired rate, it suffices to solve the preliminary OT problem to accuracy <span id="A2.SS3.p4.m9" class="ltx_Math">\delta=n^{-p/(d+2p)}</span>.</p>
</div>
<div id="A2.SS3.p5" class="ltx_para">
<p class="ltx_p">Computational complexity is dominated by this OT computation. The source and target distributions are both supported on <span id="A2.SS3.p5.m1" class="ltx_Math">n</span> points, and we require accuracy <span id="A2.SS3.p5.m2" class="ltx_Math">\delta=n^{-p/(d+2p)}</span>. Computing the relevant cost matrix requires time <span id="A2.SS3.p5.m3" class="ltx_Math">O(n^{2}d)</span>. Using a state of the art OT solver based on entropic OT (e.g., <cite class="ltx_cite ltx_citemacro_citep">Luo et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a></cite>) gives a running time of <span id="A2.SS3.p5.m4" class="ltx_Math">O(C_{\infty}n^{2}/\delta)=O(C_{\infty}n^{2+p/(d+2p)})</span>, where <span id="A2.SS3.p5.m5" class="ltx_Math">C_{\infty}</span> is the largest distance between a source point and a target point.</p>
</div>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>One-Dimensional Refinements (<a href="#Thmremark3" title="Remark 3 (One-dimensional refinements). ‣ 3 Finite-Sample Estimation and Computation ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)</h3>

<div id="A2.SS4.p1" class="ltx_para">
<p class="ltx_p">In one dimension, OT maps can be expressed concisely in terms of CDFs; in particular, if <span id="A2.SS4.p1.m1" class="ltx_Math">\mu</span> and <span id="A2.SS4.p1.m2" class="ltx_Math">\nu</span> have strictly increasing CDFs <span id="A2.SS4.p1.m3" class="ltx_Math">F_{\mu}</span> and <span id="A2.SS4.p1.m4" class="ltx_Math">F_{\nu}</span>, respectively, then the map <span id="A2.SS4.p1.m5" class="ltx_Math">T^{\star}(x)=F_{\nu}^{-1}(F_{\mu}(x))</span> solves the <span id="A2.SS4.p1.m6" class="ltx_Math">\Wp(\mu,\nu)</span> problem for all <span id="A2.SS4.p1.m7" class="ltx_Math">p\geq 1</span>. As a result, many OT-based inference tasks become more analytically tractable when <span id="A2.SS4.p1.m8" class="ltx_Math">d=1</span>, including map estimation. In fact, minor adjustments to folklore techniques imply that the optimal risk of <span id="A2.SS4.p1.m9" class="ltx_Math">n^{-1/(2p)}</span> is achievable when <span id="A2.SS4.p1.m10" class="ltx_Math">d=1</span>. We now provide a clean derivation of this risk bound using the Kolmogorov-Smirnov (KS) distance.</p>
</div>
<div id="A2.SS4.p2" class="ltx_para">
<p class="ltx_p">The KS distance is a useful alternative to the TV metric in one dimension, defined via <span id="A2.SS4.p2.m1" class="ltx_Math">\|\mu-\nu\|_{\KS}\defeq\sup_{t\in\R}|(\mu-\nu)((-\infty,t])|=\|F_{\mu}-F_{\nu}%
\|_{\infty}</span>. We always have <span id="A2.SS4.p2.m2" class="ltx_Math">\|\mu-\nu\|_{\KS}\leq\|\mu-\nu\|_{\tv}</span>, since <span id="A2.SS4.p2.m3" class="ltx_Math">\|\mu-\nu\|_{\tv}</span> can alternatively be expressed as <span id="A2.SS4.p2.m4" class="ltx_Math">\sup_{A\text{ meas.}}|(\mu-\nu)(A)|</span>. A comparison with <span id="A2.SS4.p2.m5" class="ltx_Math">\Wp</span> mirroring Fact <a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is direct.</p>
</div>
<div id="Thmlemma11" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 11</span></span><span class="ltx_text ltx_font_bold"> </span>(<span id="Thmlemma11.m1" class="ltx_Math">\Wp</span>-KS comparison)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma11.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For <span id="Thmlemma11.p1.m1" class="ltx_Math">\mu,\nu\in\cP([0,D])</span>, we have
<span id="Thmlemma11.p1.m2" class="ltx_Math">\Wp(\mu,\nu)\leq D\|\mu-\nu\|_{\KS}^{1/p}</span>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS4.p3" class="ltx_para">
<p class="ltx_p">Writing <span id="A2.SS4.p3.m1" class="ltx_Math">F,G</span> for the CDFs of <span id="A2.SS4.p3.m2" class="ltx_Math">\mu</span> and <span id="A2.SS4.p3.m3" class="ltx_Math">\nu</span>, with generalized inverses <span id="A2.SS4.p3.m4" class="ltx_Math">F^{-1}</span> and <span id="A2.SS4.p3.m5" class="ltx_Math">G^{-1}</span>, we bound</p>
<table id="A5.EGx30" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex121"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex121.m1" class="ltx_Math">\displaystyle\Wp(\mu,\nu)^{p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex121.m2" class="ltx_Math">\displaystyle=\int_{0}^{1}|F^{-1}(u)-G^{-1}(u)|^{p}\dd u</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex122"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex122.m1" class="ltx_Math">\displaystyle\leq D^{p-1}\int_{0}^{1}|F^{-1}(u)-G^{-1}(u)|\dd u</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex123"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex123.m1" class="ltx_Math">\displaystyle=D^{p-1}\int_{0}^{D}|F(x)-G(x)|\dd x</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex124"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex124.m1" class="ltx_Math">\displaystyle\leq D^{p}\|\mu-\nu\|_{\KS}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Taking <span id="A2.SS4.p3.m6" class="ltx_Math">p</span>th roots gives the statement.
∎</p>
</div>
</div>
<div id="A2.SS4.p4" class="ltx_para">
<p class="ltx_p">The KS distance admits useful empirical convergence guarantees not shared by the TV distance.</p>
</div>
<div id="Thmfact2" class="ltx_theorem ltx_theorem_fact">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Fact 2</span></span><span class="ltx_text ltx_font_bold"> </span>(KS empirical convergence, <cite class="ltx_cite ltx_citemacro_citep">Massart, <a href="#bib.bib27" title="" class="ltx_ref">1990</a></cite>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmfact2.p1" class="ltx_para">
<p class="ltx_p">For all <span id="Thmfact2.p1.m1" class="ltx_Math">\mu\in\cP(\R)</span>, <span id="Thmfact2.p1.m2" class="ltx_Math">\E[\|\mu-\hat{\mu}_{n}\|_{\KS}]\leq 1/\sqrt{n}</span>.</p>
</div>
</div>
<div id="A2.SS4.p5" class="ltx_para">
<p class="ltx_p">Moreover, for fixed <span id="A2.SS4.p5.m1" class="ltx_Math">\mu</span> and <span id="A2.SS4.p5.m2" class="ltx_Math">\nu</span>, there exists an optimal kernel for <span id="A2.SS4.p5.m3" class="ltx_Math">\Wp(\mu,\nu)</span> (namely, based on CDFs as above), which is near-optimal for all <span id="A2.SS4.p5.m4" class="ltx_Math">\mu^{\prime}</span> in a KS neighborhood of <span id="A2.SS4.p5.m5" class="ltx_Math">\mu</span>, as shown next.</p>
</div>
<div id="Thmlemma12" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 12</span></span><span class="ltx_text ltx_font_bold"> </span>(KS corruptions in <span id="Thmlemma12.m1" class="ltx_Math">\mu</span>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma12.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For <span id="Thmlemma12.p1.m1" class="ltx_Math">\cX,\cY\subseteq\R</span>, fix <span id="Thmlemma12.p1.m2" class="ltx_Math">\mu\in\cP(\cX)</span> and <span id="Thmlemma12.p1.m3" class="ltx_Math">\nu\in\cP(\cY)</span>. There exists an optimal kernel <span id="Thmlemma12.p1.m4" class="ltx_Math">\kappa^{\star}\in\cK(\cX,\cY)</span> for the <span id="Thmlemma12.p1.m5" class="ltx_Math">\Wp(\mu,\nu)</span> problem such that, for all <span id="Thmlemma12.p1.m6" class="ltx_Math">\mu^{\prime}\in\cP(\cX)</span>, we have
</span></p>
<table id="A2.Ex125" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A2.Ex125.m1" class="ltx_Math">\cE_{p}(\kappa^{\star};\mu^{\prime},\nu)\lesssim\diam(\cY)\|\mu-\mu^{\prime}\|%
_{\KS}^{1/p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS4.p6" class="ltx_para">
<p class="ltx_p">Write <span id="A2.SS4.p6.m1" class="ltx_Math">F,F^{\prime},G,</span> for the CDFs of <span id="A2.SS4.p6.m2" class="ltx_Math">\mu</span>, <span id="A2.SS4.p6.m3" class="ltx_Math">\mu^{\prime}</span>, and <span id="A2.SS4.p6.m4" class="ltx_Math">\nu</span>, respectively, and let <span id="A2.SS4.p6.m5" class="ltx_Math">\eps=\|\mu-\mu^{\prime}\|_{\KS}=\|F-F^{\prime}\|_{\infty}</span>. Write <span id="A2.SS4.p6.m6" class="ltx_Math">D=\diam(\cY)</span> and suppose without loss of generality that <span id="A2.SS4.p6.m7" class="ltx_Math">\cY=[0,D]</span>. For now, suppose further that <span id="A2.SS4.p6.m8" class="ltx_Math">1/\eps=3M</span> is a multiple of 3 (without loss of generality) and that <span id="A2.SS4.p6.m9" class="ltx_Math">F</span> is continuous (which will be relaxed). We consider the kernel induced by the map <span id="A2.SS4.p6.m10" class="ltx_Math">T^{\star}=G^{-1}\circ F</span>, where <span id="A2.SS4.p6.m11" class="ltx_Math">G^{-1}</span> is the generalized inverse of <span id="A2.SS4.p6.m12" class="ltx_Math">G</span> with <span id="A2.SS4.p6.m13" class="ltx_Math">G^{-1}(q)</span> defined as as <span id="A2.SS4.p6.m14" class="ltx_Math">0</span> for <span id="A2.SS4.p6.m15" class="ltx_Math">q\leq 0</span> and <span id="A2.SS4.p6.m16" class="ltx_Math">D</span> for <span id="A2.SS4.p6.m17" class="ltx_Math">q\geq 1</span>. In particular, we compare <span id="A2.SS4.p6.m18" class="ltx_Math">T^{\star}</span> with the optimal kernel <span id="A2.SS4.p6.m19" class="ltx_Math">G^{-1}\circ F^{\prime}</span> for the <span id="A2.SS4.p6.m20" class="ltx_Math">\Wp(\mu^{\prime},\nu)</span> problem, bounding</p>
<table id="A5.EGx31" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex126"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex126.m1" class="ltx_Math">\displaystyle\int_{\cX}\bigl{|}G^{-1}(F(x))-G^{-1}(F^{\prime}(x))\bigr{|}^{p}%
\dd F^{\prime}(x)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex126.m2" class="ltx_Math">\displaystyle\leq\int_{\cX}\bigl{|}G^{-1}(F^{\prime}(x)\pm\eps)-G^{-1}(F^{%
\prime}(x))\bigr{|}^{p}\dd F^{\prime}(x)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex127"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex127.m1" class="ltx_Math">\displaystyle=\int_{0}^{1}\bigl{|}G^{-1}(u\pm\eps)-G^{-1}(u)\bigr{|}^{p}\dd u</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex128"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex128.m1" class="ltx_Math">\displaystyle=\sum_{i=0}^{3M-1}\int_{i\eps}^{(i+1)\eps}\bigl{|}G^{-1}(u\pm\eps%
)-G^{-1}(u)\bigr{|}^{p}\dd u</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex129"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex129.m1" class="ltx_Math">\displaystyle\leq\eps\sum_{i=0}^{3M-1}\bigl{[}G^{-1}((i+2)\eps)-G^{-1}((i-1)%
\eps)\bigr{]}^{p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, the first equality uses that <span id="A2.SS4.p6.m21" class="ltx_Math">F^{\prime}_{\sharp}\mu^{\prime}=\Unif([0,1])</span>, and the second inequality uses that <span id="A2.SS4.p6.m22" class="ltx_Math">G^{-1}</span> is monotonic. If <span id="A2.SS4.p6.m23" class="ltx_Math">F^{\prime}</span> is discontinuous, one should replace it with the kernel <span id="A2.SS4.p6.m24" class="ltx_Math">\tilde{F}^{\prime}</span> which coincides with <span id="A2.SS4.p6.m25" class="ltx_Math">F^{\prime}</span> where continuous and, at any point <span id="A2.SS4.p6.m26" class="ltx_Math">x</span> where there is a jump from <span id="A2.SS4.p6.m27" class="ltx_Math">p_{1}</span> to <span id="A2.SS4.p6.m28" class="ltx_Math">p_{2}</span>, satisfies <span id="A2.SS4.p6.m29" class="ltx_Math">\tilde{F}^{\prime}_{\sharp}\delta_{x}=\Unif([p_{1},p_{2}])</span>. By this choice, we have <span id="A2.SS4.p6.m30" class="ltx_Math">\tilde{F}^{\prime}_{\sharp}\mu=\Unif([0,1])</span>, and one can do the same for <span id="A2.SS4.p6.m31" class="ltx_Math">F</span> to obtain <span id="A2.SS4.p6.m32" class="ltx_Math">\tilde{F}</span> such that <span id="A2.SS4.p6.m33" class="ltx_Math">\mathsf{W}_{\infty}(\tilde{F}_{\sharp}\mu,\tilde{F}^{\prime}_{\sharp}\mu)\leq\eps</span>. At this point, we can derive the same bound as above. Now, writing <span id="A2.SS4.p6.m34" class="ltx_Math">\Delta_{i}=G^{-1}((i+3)\eps)-G^{-1}(i\eps)</span>, we have</p>
<table id="A5.EGx32" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex130"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex130.m1" class="ltx_Math">\displaystyle\int_{\cX}\bigl{|}G^{-1}(F(x))</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex130.m2" class="ltx_Math">\displaystyle-G^{-1}(F^{\prime}(x))\bigr{|}^{p}\dd F^{\prime}(x)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex131"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex131.m1" class="ltx_Math">\displaystyle\leq\eps\sum_{i=-1}^{3M-2}\Delta_{i}^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex132"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex132.m1" class="ltx_Math">\displaystyle=\eps\left(\sum_{i=0}^{M-1}\Delta_{3i-1}^{p}+\sum_{i=0}^{M-1}%
\Delta_{3i}^{p}+\sum_{i=0}^{M-1}\Delta_{3i+1}^{p}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex133"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex133.m1" class="ltx_Math">\displaystyle=\eps D^{p}\left(\sum_{i=0}^{M-1}\left(\frac{\Delta_{3i-1}}{D}%
\right)^{p}+\sum_{i=0}^{M-1}\left(\frac{\Delta_{3i}}{D}\right)^{p}+\sum_{i=0}^%
{M-1}\left(\frac{\Delta_{3i+1}}{D}\right)^{p}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex134"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex134.m1" class="ltx_Math">\displaystyle\leq\eps D^{p}\left(\left(\sum_{i=0}^{M-1}\frac{\Delta_{3i-1}}{D}%
\right)^{p}+\left(\sum_{i=0}^{M-1}\frac{\Delta_{3i}}{D}\right)^{p}+\left(\sum_%
{i=0}^{M-1}\frac{\Delta_{3i+1}}{D}\right)^{p}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex135"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex135.m1" class="ltx_Math">\displaystyle=O(D\eps^{1/p})^{p}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, we have <span id="A2.SS4.p6.m35" class="ltx_Math">\cE_{p}(G^{-1}\circ F;\mu^{\prime},\nu)\lesssim\|G^{-1}\circ F-G^{-1}\circ F^{%
\prime}\|_{L^{p}(\mu^{\prime})}\leq D\eps^{1/p}</span>, as desired.
∎</p>
</div>
</div>
<div id="A2.SS4.p7" class="ltx_para">
<p class="ltx_p">Together, the three results stated above yield our desired risk bound.
</p>
</div>
<div id="Thmproposition2" class="ltx_theorem ltx_theorem_proposition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Proposition 2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmproposition2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <span id="Thmproposition2.p1.m1" class="ltx_Math">X_{1},\dots,X_{n}\stackrel{{\scriptstyle\text{i.i.d.}}}{{\sim}}\mu\in\cP(\R)</span> and <span id="Thmproposition2.p1.m2" class="ltx_Math">Y_{1},\dots,Y_{n}\stackrel{{\scriptstyle\text{i.i.d.}}}{{\sim}}\nu\in\cP([0,1])</span>. Then the estimator <span id="Thmproposition2.p1.m3" class="ltx_Math">\hat{\kappa}_{n}</span> which, given <span id="Thmproposition2.p1.m4" class="ltx_Math">\hat{\mu}_{n}</span> and <span id="Thmproposition2.p1.m5" class="ltx_Math">\hat{\nu}_{n}</span>, returns the optimal kernel for <span id="Thmproposition2.p1.m6" class="ltx_Math">\Wp(\hat{\mu}_{n},\hat{\nu}_{n})</span> given by <a href="#Thmlemma12" title="Lemma 12 (KS corruptions in μ). ‣ B.4 One-Dimensional Refinements (3) ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, achieves risk <span id="Thmproposition2.p1.m7" class="ltx_Math">\E[\cE_{p}(\hat{\kappa}_{n};\mu,\nu)]\lesssim n^{-1/(2p)}</span>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A2.SS4.p8" class="ltx_para">
<p class="ltx_p">By Fact <a href="#Thmfact2" title="Fact 2 (KS empirical convergence, Massart, 1990). ‣ B.4 One-Dimensional Refinements (3) ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we have that <span id="A2.SS4.p8.m1" class="ltx_Math">\E[\|\mu-\hat{\mu}_{n}\|_{\KS}]\leq n^{-1/2}</span>. Consequently, we bound</p>
<table id="A5.EGx33" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.Ex136"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A2.Ex136.m1" class="ltx_Math">\displaystyle\cE_{p}(\hat{\kappa}_{n};\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex136.m2" class="ltx_Math">\displaystyle\leq\cE_{p}(\hat{\kappa}_{n};\mu,\hat{\nu}_{n})+\Wp(\nu,\hat{\nu}%
_{n})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex137"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A2.Ex137.m1" class="ltx_Math">\displaystyle\leq\|\mu-\hat{\mu}_{n}\|_{\mathrm{KS}}^{1/p}+\Wp(\nu,\hat{\nu}_{%
n}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Taking expectations and applying <a href="#Thmfact2" title="Fact 2 (KS empirical convergence, Massart, 1990). ‣ B.4 One-Dimensional Refinements (3) ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#Thmlemma1" title="Lemma 1 (\Wp empirical convergence, Lei, 2020). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives the desired rate.
∎</p>
</div>
</div>
<div id="A2.SS4.p9" class="ltx_para">
<p class="ltx_p">Unfortunately, we are unaware of any multivariate extension of the KS distance that obeys a useful comparison inequality with <span id="A2.SS4.p9.m1" class="ltx_Math">\Wp</span> (like Fact <a href="#Thmlemma11" title="Lemma 11 (\Wp-KS comparison). ‣ B.4 One-Dimensional Refinements (3) ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) while maintaining strong empirical convergence guarantees (like Fact <a href="#Thmfact2" title="Fact 2 (KS empirical convergence, Massart, 1990). ‣ B.4 One-Dimensional Refinements (3) ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), inhibiting the further development of this approach.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Details for <a href="#S4" title="4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">We note that the minimax lower bounds in <a href="#Thmcorollary1" title="Corollary 1 (Plug-in estimators). ‣ 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_ERROR undefined">\crefpairconjunction</span><a href="#Thmcorollary2" title="Corollary 2 (Wavelet estimators). ‣ 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> follow by combining the reduction to distribution estimation from <a href="#A2.SS2" title="B.2 Minimax Lower Bound under Sampling ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> with existing lower bounds for distribution estimation under <span id="A3.p1.m1" class="ltx_Math">\Wp</span> from <cite class="ltx_cite ltx_citemacro_cite">Singh and Póczos [<a href="#bib.bib41" title="" class="ltx_ref">2018</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">Weed and Berthet [<a href="#bib.bib47" title="" class="ltx_ref">2019</a>]</cite>, respectively.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Proofs for <a href="#S5" title="5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>
</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p">We first recall some basic facts used throughout.
</p>
</div>
<div id="Thmfact3" class="ltx_theorem ltx_theorem_fact">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Fact 3</span></span><span class="ltx_text ltx_font_bold"> </span>(TV contraction under Markov kernels)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmfact3.p1" class="ltx_para">
<p class="ltx_p">For <span id="Thmfact3.p1.m1" class="ltx_Math">\mu,\nu\in\cP(\cX)</span> and kernel <span id="Thmfact3.p1.m2" class="ltx_Math">\kappa\in\cK(\cX,\cY)</span>, we have <span id="Thmfact3.p1.m3" class="ltx_Math">\|\kappa_{\sharp}\mu-\kappa_{\sharp}\nu\|_{\tv}\leq\|\mu-\nu\|_{\tv}</span>.</p>
</div>
</div>
<div id="A4.p2" class="ltx_para">
<p class="ltx_p">This follows by the data processing inequality.</p>
</div>
<div id="Thmfact4" class="ltx_theorem ltx_theorem_fact">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Fact 4</span></span><span class="ltx_text ltx_font_bold"> </span>(<span id="Thmfact4.m1" class="ltx_Math">\Wp</span> contraction under convolution)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmfact4.p1" class="ltx_para">
<p class="ltx_p">For <span id="Thmfact4.p1.m1" class="ltx_Math">\mu,\nu,\alpha\in\cP(\cX)</span>, we have <span id="Thmfact4.p1.m2" class="ltx_Math">\Wp(\mu*\alpha,\nu*\alpha)\leq\Wp(\mu,\nu)</span>, where <span id="Thmfact4.p1.m3" class="ltx_Math">*</span> denotes convolution between probability measures.</p>
</div>
</div>
<div id="A4.p3" class="ltx_para">
<p class="ltx_p">This follows by considering the couplings <span id="A4.p3.m1" class="ltx_Math">(X+Z,Y+Z^{\prime})</span> of <span id="A4.p3.m2" class="ltx_Math">\mu*\alpha</span> and <span id="A4.p3.m3" class="ltx_Math">\nu*\alpha</span> which set <span id="A4.p3.m4" class="ltx_Math">Z=Z^{\prime}</span>.</p>
</div>
<div id="Thmfact5" class="ltx_theorem ltx_theorem_fact">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Fact 5</span></span><span class="ltx_text ltx_font_bold"> </span>(TV discrete empirical convergence)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmfact5.p1" class="ltx_para">
<p class="ltx_p">For a finite set <span id="Thmfact5.p1.m1" class="ltx_Math">S</span> with <span id="Thmfact5.p1.m2" class="ltx_Math">|S|=k</span>, any distribution <span id="Thmfact5.p1.m3" class="ltx_Math">\mu\in\Delta(S)</span> exhibits empirical convergence in TV at rate
<span id="Thmfact5.p1.m4" class="ltx_Math">\E[\|\hat{\mu}_{n}-\mu\|_{\tv}]\lesssim\sqrt{k/n}</span>.</p>
</div>
</div>
<div id="A4.p4" class="ltx_para">
<p class="ltx_p">To simplify discussion of our corruption model, we employ the <em class="ltx_emph ltx_font_italic"><span id="A4.p4.m1" class="ltx_Math">\eps</span>-outlier-robust <span id="A4.p4.m2" class="ltx_Math">p</span>-Wasserstein distance</em></p>
<table id="A4.E9" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.E9.m1" class="ltx_Math">\RWp(\mu,\nu)\defeq\min_{\begin{subarray}{c}\mu^{\prime}\in\cP(\R^{d})\\
\|\mu^{\prime}-\mu\|_{\tv}\leq\eps\end{subarray}}\Wp(\mu^{\prime},\nu)=\min_{%
\begin{subarray}{c}\nu^{\prime}\in\cP(\R^{d})\\
\|\nu^{\prime}-\nu\|_{\tv}\leq\eps\end{subarray}}\Wp(\mu,\nu^{\prime}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr>
</table>
<p class="ltx_p">The second equality follows from the observation that, if <span id="A4.p4.m3" class="ltx_Math">\E[\|X^{\prime}-Y\|^{p}]\leq c</span> and <span id="A4.p4.m4" class="ltx_Math">X=X^{\prime}</span> with probability at least <span id="A4.p4.m5" class="ltx_Math">1-\eps</span>, then the random variable <span id="A4.p4.m6" class="ltx_Math">Y^{\prime}=Y\mathds{1}\{X=X^{\prime}\}+X\mathds{1}\{X\neq X^{\prime}\}</span> satisfies <span id="A4.p4.m7" class="ltx_Math">\E[\|X-Y^{\prime}\|^{p}]\leq c</span>. See <cite class="ltx_cite ltx_citemacro_cite">Nietert et al. [<a href="#bib.bib30" title="" class="ltx_ref">2023a</a>]</cite> for a thorough examination of <span id="A4.p4.m8" class="ltx_Math">\RWp</span> in the context of robust statistics. Under the setting of <a href="#S5" title="5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, our corruption model can be equivalently stated as follows: given the standard empirical measures <span id="A4.p4.m9" class="ltx_Math">\hat{\mu}_{n}\in\cP(\cX)</span> and <span id="A4.p4.m10" class="ltx_Math">\hat{\nu}_{n}\in\cP(\cY)</span>, we observe corrupted versions <span id="A4.p4.m11" class="ltx_Math">\tilde{\mu}_{n}\in\cP(\cX)</span> and <span id="A4.p4.m12" class="ltx_Math">\tilde{\mu}_{n}\in\cP(\cY)</span> such that <span id="A4.p4.m13" class="ltx_Math">\RWp(\tilde{\mu}_{n},\hat{\mu}_{n})\lor\RWp(\tilde{\nu}_{n},\hat{\nu}_{n})\leq\rho</span>.</p>
</div>
<div id="A4.p5" class="ltx_para">
<p class="ltx_p">For this setting, we handle sampling error using the following lemma, which mirrors <a href="#Thmlemma9" title="Lemma 9. ‣ B.3 Proof of 2 ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="Thmlemma13" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 13</span></span><span class="ltx_text ltx_font_bold"> </span>(Prop. 2 of <cite class="ltx_cite ltx_citemacro_citep">Goldfeld et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a></cite>)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma13.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fix <span id="Thmlemma13.p1.m1" class="ltx_Math">\sigma&gt;0</span> and <span id="Thmlemma13.p1.m2" class="ltx_Math">1</span>-sub-Gaussian <span id="Thmlemma13.p1.m3" class="ltx_Math">\mu\in\cP(\R^{d})</span>. Then, the <span id="Thmlemma13.p1.m4" class="ltx_Math">n</span>-sample empirical measure <span id="Thmlemma13.p1.m5" class="ltx_Math">\hat{\mu}_{n}</span> satisfies <span id="Thmlemma13.p1.m6" class="ltx_Math">\E\bigl{[}\|N^{\sigma}_{\sharp}(\mu-\hat{\mu}_{n})\|_{\tv}\bigr{]}\leq\sqrt{3^%
{d}(1\lor\sigma^{-d})/n}</span>.</span></p>
</div>
</div>
<div id="A4.p6" class="ltx_para">
<p class="ltx_p">In order to apply our <span id="A4.p6.m1" class="ltx_Math">\Wp</span> stability result, <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we use that any kernel become continuous if one first applies Gaussian convolution.</p>
</div>
<div id="Thmlemma14" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 14</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma14.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Fix <span id="Thmlemma14.p1.m1" class="ltx_Math">\bar{\kappa}\in\cK(\cX,\cY)</span>, <span id="Thmlemma14.p1.m2" class="ltx_Math">\sigma&gt;0</span>, and let <span id="Thmlemma14.p1.m3" class="ltx_Math">\kappa=\bar{\kappa}\circ N^{\sigma}</span>. Then, for all <span id="Thmlemma14.p1.m4" class="ltx_Math">x,x^{\prime}\in\cX</span>, we have <span id="Thmlemma14.p1.m5" class="ltx_Math">\Wp((\kappa_{x},\kappa_{x^{\prime}})\leq\diam(\cY)[\|x-x^{\prime}\|/(2\sigma)]%
^{1/p}</span>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A4.p7" class="ltx_para">
<p class="ltx_p">We simply compute</p>
<table id="A5.EGx34" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex138"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex138.m1" class="ltx_Math">\displaystyle\Wp(\kappa_{x},\kappa_{x^{\prime}})</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex138.m2" class="ltx_Math">\displaystyle\leq\diam(\cY)\|\kappa_{\sharp}(N^{\sigma}_{x}-N^{\sigma}_{x^{%
\prime}})\|_{\tv}^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<a href="#Thmfact1" title="Fact 1 (\Wp-TV comparison). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)</span></td>
</tr></tbody>
<tbody id="A4.Ex139"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex139.m1" class="ltx_Math">\displaystyle\leq\diam(\cY)\|N^{\sigma}_{x}-N^{\sigma}_{x^{\prime}}\|_{\tv}^{1%
/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(data processing ineq.)</span></td>
</tr></tbody>
<tbody id="A4.Ex140"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex140.m1" class="ltx_Math">\displaystyle\leq\diam(\cY)\|\cN(x,\sigma^{2}I_{d})-\cN(x^{\prime},\sigma^{2}I%
_{d})\|_{\tv}^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex141"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex141.m1" class="ltx_Math">\displaystyle\leq\diam(\cY)\|x-x^{\prime}\|^{1/p}(2\sigma)^{-1/p},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where the final inequality follows by the closed form of KL divergence between Gaussians, combined with Pinsker’s inequality.
∎</p>
</div>
</div>
<div id="A4.p8" class="ltx_para">
<p class="ltx_p">We split the proof of <a href="#Thmtheorem4" title="Theorem 4 (Robust estimation guarantee). ‣ 5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> into the upper bound (<a href="#A4.SS1" title="D.1 Proof of 4 (Upper Bound) ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.1</span></a>) and lower bound (<a href="#A4.SS2" title="D.2 Proof of 4 (Lower Bound) ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.2</span></a>).</p>
</div>
<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Proof of <a href="#Thmtheorem4" title="Theorem 4 (Robust estimation guarantee). ‣ 5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (Upper Bound)</h3>

<div id="A4.SS1.p1" class="ltx_para">
<p class="ltx_p">To start, we decompose <span id="A4.SS1.p1.m1" class="ltx_Math">N^{\sigma}=N^{\sigma_{1}+\sigma_{2}}=N^{\sigma_{1}}\circ N^{\sigma_{2}}</span>, for <span id="A4.SS1.p1.m2" class="ltx_Math">\sigma_{1},\sigma_{2}</span> to be tuned later. By our corruption model, there exists an intermediate measure <span id="A4.SS1.p1.m3" class="ltx_Math">\mu^{\prime}_{n}\in\cP(\R^{d})</span> such that <span id="A4.SS1.p1.m4" class="ltx_Math">\|\hat{\mu}_{n}-\mu^{\prime}_{n}\|_{\tv}\leq\eps</span> and <span id="A4.SS1.p1.m5" class="ltx_Math">\Wp(\mu^{\prime}_{n},\tilde{\mu}_{n})\leq\rho</span>. By Facts <a href="#Thmfact3" title="Fact 3 (TV contraction under Markov kernels). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#Thmfact4" title="Fact 4 (\Wp contraction under convolution). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, these bounds are preserved under convolution, so <span id="A4.SS1.p1.m6" class="ltx_Math">\|N^{\sigma_{1}}_{\sharp}(\hat{\mu}_{n}-\mu^{\prime}_{n})\|_{\tv}\leq\eps</span> and <span id="A4.SS1.p1.m7" class="ltx_Math">\Wp(N^{\sigma_{1}}_{\sharp}\mu^{\prime}_{n},N^{\sigma_{1}}_{\sharp}\tilde{\mu}%
_{n})\leq\rho</span>. By the TV triangle inequality, we have <span id="A4.SS1.p1.m8" class="ltx_Math">\|N^{\sigma_{1}}_{\sharp}(\mu-\mu^{\prime}_{n})\|_{\tv}\leq\tau\defeq\eps+\|N^%
{\sigma_{1}}_{\sharp}(\mu-\hat{\mu}_{n})\|_{\tv}</span>. We conclude that <span id="A4.SS1.p1.m9" class="ltx_Math">\Wp^{\tau}(N^{\sigma_{1}}_{\sharp}\tilde{\mu}_{n},N^{\sigma_{1}}_{\sharp}\mu)\leq\rho</span>. By the symmetric nature of <span id="A4.SS1.p1.m10" class="ltx_Math">\Wp^{\tau}</span>, there must also exist <span id="A4.SS1.p1.m11" class="ltx_Math">\alpha\in\cP(\R^{d})</span> such that <span id="A4.SS1.p1.m12" class="ltx_Math">\Wp(N^{\sigma_{1}}_{\sharp}\mu,\alpha)\leq\rho</span> and <span id="A4.SS1.p1.m13" class="ltx_Math">\|\alpha-N^{\sigma_{1}}_{\sharp}\tilde{\mu}_{n}\|_{\tv}\leq\tau</span>.</p>
</div>
<div id="A4.SS1.p2" class="ltx_para">
<p class="ltx_p">Now set <span id="A4.SS1.p2.m1" class="ltx_Math">\bar{\kappa}=\kappa^{\star}_{p}[N^{\sigma}_{\sharp}\tilde{\mu}_{n}\to\tilde{%
\nu}_{n}]</span>, so that <span id="A4.SS1.p2.m2" class="ltx_Math">\cE_{p}(\bar{\kappa};N^{\sigma}_{\sharp}\tilde{\mu}_{n},\tilde{\nu}_{n})=0</span>. Using this, the TV bound above, and the fact that <span id="A4.SS1.p2.m3" class="ltx_Math">N^{\sigma}_{\sharp}\tilde{\mu}_{n}=N^{\sigma_{2}}_{\sharp}(N^{\sigma_{1}}_{%
\sharp}\tilde{\mu}_{n})</span>, we have <span id="A4.SS1.p2.m4" class="ltx_Math">\cE_{p}(\bar{\kappa};N^{\sigma_{2}}_{\sharp}\alpha,\tilde{\nu}_{n})\lesssim%
\diam(\cY)\tau^{1/p}\leq\sqrt{d}\tau^{1/p}</span>.
Applying <a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, this gives</p>
<table id="A5.EGx35" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex142"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex142.m1" class="ltx_Math">\displaystyle\cE_{p}(\bar{\kappa}\circ N^{\sigma_{2}};\alpha,\tilde{\nu}_{n})</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex142.m2" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\tau^{\frac{1}{p}}+\E_{Z\sim\cN(0,\sigma_{2}^{2}I%
_{d})}[\|Z\|^{p}]^{\frac{1}{p}}\lesssim\sqrt{d}\tau^{\frac{1}{p}}+\sqrt{d+p}\,%
\sigma_{2}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Consequently, by <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#Thmlemma14" title="Lemma 14. ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, we have that</p>
<table id="A5.EGx36" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex143"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex143.m1" class="ltx_Math">\displaystyle\cE_{p}(\bar{\kappa}\circ N^{\sigma_{2}};N^{\sigma_{1}}_{\sharp}%
\mu,\tilde{\nu}_{n})</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex143.m2" class="ltx_Math">\displaystyle\lesssim\cE_{p}(\bar{\kappa}\circ N^{\sigma_{2}};\alpha,\tilde{%
\nu}_{n})+\rho+(\sqrt{d}/\sigma_{2})^{1/p}\rho^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex144"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex144.m1" class="ltx_Math">\displaystyle\lesssim\cE_{p}(\bar{\kappa}\circ N^{\sigma_{2}};\alpha,\tilde{%
\nu}_{n})+\rho+(\sqrt{d}/\sigma_{2})^{1/p}\rho^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex145"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex145.m1" class="ltx_Math">\displaystyle\leq\sqrt{d}\tau^{\frac{1}{p}}+\rho+(\sqrt{d}/\sigma_{2})^{1/p}%
\rho^{1/p}+\sqrt{d+p}\,\sigma_{2}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Tuning <span id="A4.SS1.p2.m5" class="ltx_Math">\sigma_{2}</span> gives</p>
<table id="A4.Ex146" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex146.m1" class="ltx_Math">\cE_{p}(\bar{\kappa}\circ N^{\sigma_{2}};N^{\sigma_{1}}_{\sharp}\mu,\tilde{\nu%
}_{n})\lesssim\sqrt{d}\tau^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+\rho.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Apply <a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> once more, we bound</p>
<table id="A5.EGx37" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex147"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex147.m1" class="ltx_Math">\displaystyle\cE_{p}(\bar{\kappa}\circ N^{\sigma};\mu,\tilde{\nu}_{n})</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex147.m2" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\tau^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+%
\rho+\sqrt{d+p}\,\sigma_{1}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex148"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex148.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+%
\rho+\sqrt{d+p}\,\sigma_{1}+\sqrt{d}\,\|N^{\sigma_{1}}_{\sharp}(\mu-\hat{\mu}_%
{n})\|_{\tv}^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Taking expectations and applying <a href="#Thmlemma13" title="Lemma 13 (Prop. 2 of Goldfeld et al., 2020). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> yields</p>
<table id="A5.EGx38" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex149"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex149.m1" class="ltx_Math">\displaystyle\E[\cE_{p}(\bar{\kappa}\circ N^{\sigma};\mu,\tilde{\nu}_{n})]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex149.m2" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+%
\rho+\sqrt{d+p}\,\sigma_{1}+\E[\|N^{\sigma_{1}}_{\sharp}(\mu-\hat{\mu}_{n})\|_%
{\tv}]^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex150"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex150.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+%
\rho+\sqrt{d+p}\,\sigma_{1}+\left(\frac{3^{d}(1\lor\sigma^{-d})}{n}\right)^{%
\frac{1}{2p}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Tuning <span id="A4.SS1.p2.m6" class="ltx_Math">\sigma_{1}</span> then gives</p>
<table id="A4.Ex151" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex151.m1" class="ltx_Math">\E[\cE_{p}(\bar{\kappa}\circ N^{\sigma};\mu,\tilde{\nu}_{n})]\lesssim\sqrt{d}%
\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+\rho+O_{p,d}(n^{-\frac{1}{d+2p%
}}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Finally, we note that <span id="A4.SS1.p2.m7" class="ltx_Math">\Wp(\tilde{\nu}_{n},\hat{\nu}_{n})\leq\rho+\sqrt{d}\eps^{1/p}</span> due to the support bound. Thus, <a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> gives</p>
<table id="A5.EGx39" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex152"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex152.m1" class="ltx_Math">\displaystyle\E[\cE_{p}(\bar{\kappa}\circ N^{\sigma};\mu,\nu)]</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex152.m2" class="ltx_Math">\displaystyle\leq\E[\cE_{p}(\bar{\kappa}\circ N^{\sigma};\mu,\tilde{\nu}_{n})+%
\Wp(\tilde{\nu}_{n},\hat{\nu}_{n})+\Wp(\hat{\nu}_{n},\nu)]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex153"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex153.m1" class="ltx_Math">\displaystyle\leq\E[\cE_{p}(\bar{\kappa}\circ N^{\sigma};\mu,\tilde{\nu}_{n})]%
+\rho+\sqrt{d}\eps^{\frac{1}{p}}+\E[\Wp(\hat{\nu}_{n},\nu)]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex154"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex154.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+%
\rho+O_{p,d}(n^{-\frac{1}{d+2p}}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.</p>
</div>
<div id="A4.SS1.p3" class="ltx_para">
<p class="ltx_p">For the null estimator, let <span id="A4.SS1.p3.m1" class="ltx_Math">\kappa^{\star}</span> be an optimal kernel for the <span id="A4.SS1.p3.m2" class="ltx_Math">\Wp(\mu,\nu)</span> problem and bound</p>
<table id="A5.EGx40" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex155"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex155.m1" class="ltx_Math">\displaystyle\cE(\hat{\kappa}_{\mathrm{null}};\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex155.m2" class="ltx_Math">\displaystyle=\left[\left(\int\|x\|^{p}\dd\mu(x)\right)^{\frac{1}{p}}-\left(%
\iint\|y-x\|^{p}\dd\kappa_{x}^{\star}(y)\dd\mu(x)\right)^{\frac{1}{p}}\right]_%
{+}+\Wp(\delta_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex156"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex156.m1" class="ltx_Math">\displaystyle\leq\left[\left(\iint\|y\|^{p}\dd\kappa_{x}^{\star}(y)\dd\mu(x)%
\right)^{\frac{1}{p}}\right]_{+}+\Wp(\delta_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(Minkowski’s inequality)</span></td>
</tr></tbody>
<tbody id="A4.Ex157"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex157.m4" class="ltx_Math">\displaystyle\leq 2\sqrt{d},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A4.Ex157.m1" class="ltx_Math">\cY\subseteq[0,1]^{d}</span>)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">as desired.</p>
</div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Proof of <a href="#Thmtheorem4" title="Theorem 4 (Robust estimation guarantee). ‣ 5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (Lower Bound)</h3>

<div id="A4.SS2.p1" class="ltx_para">
<p class="ltx_p">Since <span id="A4.SS2.p1.m1" class="ltx_Math">\sqrt{d}\eps^{1/p}</span> and <span id="A4.SS2.p1.m2" class="ltx_Math">n^{-1/(d\lor 2p)}</span> are less than <span id="A4.SS2.p1.m3" class="ltx_Math">\sqrt{d}</span>, it suffices to prove a lower bound of <span id="A4.SS2.p1.m4" class="ltx_Math">\sqrt{d}\eps^{1/p}+d^{1/4}\rho^{1/2}\land\sqrt{d}+n^{-1/(d\lor 2p)}</span>. We inherit the <span id="A4.SS2.p1.m5" class="ltx_Math">n^{-1/(d\lor 2p)}</span> sampling error term of the lower bound from the Dirac mass construction described in <a href="#A2.SS2" title="B.2 Minimax Lower Bound under Sampling ‣ Appendix B Proofs for 3 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>. For the remaining terms, we prove lower bounds which hold even in the infinite-sample population limit, and even when only the source measure is corrupted. Here, an estimator can be viewed as a map <span id="A4.SS2.p1.m6" class="ltx_Math">\hat{\kappa}</span> from <span id="A4.SS2.p1.m7" class="ltx_Math">\cP(\cX)\times\cP(\cY)\to\cK(\cX,\cY)</span>, mapping the corrupted source measure <span id="A4.SS2.p1.m8" class="ltx_Math">\mu</span>, guaranteed to satisfy <span id="A4.SS2.p1.m9" class="ltx_Math">\RWp(\tilde{\mu},\mu)\leq\rho</span>, and the clean target measure <span id="A4.SS2.p1.m10" class="ltx_Math">\nu</span> to a kernel estimate <span id="A4.SS2.p1.m11" class="ltx_Math">\hat{\kappa}[\tilde{\mu},\nu]</span>. For <span id="A4.SS2.p1.m12" class="ltx_Math">\cX=\unitball</span> (which, in particular, forces each <span id="A4.SS2.p1.m13" class="ltx_Math">\mu\in\cP(\cX)</span> to be 1-sub-Gaussian) and <span id="A4.SS2.p1.m14" class="ltx_Math">\cY=[-1,1]^{d}</span>, we prove that</p>
<table id="A5.EGx41" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex162"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex162.m1" class="ltx_Math">\displaystyle\sup_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu\in%
\cP(\cX)\\
\hidden@noalign{}\hfil\scriptsize\nu\in\cP(\cY)\end{subarray}}\sup_{\begin{%
subarray}{c}\hidden@noalign{}\hfil\scriptsize\tilde{\mu}\in\cP(\cX)\\
\hidden@noalign{}\hfil\scriptsize\RWp(\tilde{\mu},\mu)\leq\rho\end{subarray}}%
\cE_{p}(\hat{\kappa}[\tilde{\mu},\nu];\mu,\nu)\gtrsim\sqrt{d}\eps^{\frac{1}{p}%
}+\rho^{\frac{1}{2}}d^{\frac{1}{4}}\land\sqrt{d}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The choice of <span id="A4.SS2.p1.m15" class="ltx_Math">\cY=[-1,1]^{d}</span> rather than <span id="A4.SS2.p1.m16" class="ltx_Math">[0,1]^{d}</span> is solely to simplify notation in one of our constructions and can be reverted without loss. Finally, it suffices to lower bound the supremum by <span id="A4.SS2.p1.m17" class="ltx_Math">\sqrt{d}\eps^{1/p}</span> when <span id="A4.SS2.p1.m18" class="ltx_Math">\rho=0</span> and <span id="A4.SS2.p1.m19" class="ltx_Math">\sqrt{d\rho}\land\sqrt{d}</span> when <span id="A4.SS2.p1.m20" class="ltx_Math">\eps=0</span>, separately, which we do presently.</p>
</div>
<section id="A4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">TV lower bound.</h4>

<div id="A4.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p">Fix target measure <span id="A4.SS2.SSS0.Px1.p1.m1" class="ltx_Math">\nu=(1-\eps)\delta_{0}+\eps\delta_{y}</span>, where <span id="A4.SS2.SSS0.Px1.p1.m2" class="ltx_Math">y=(1,\dots,1)\in\R^{d}</span>. Consider the candidate clean measures <span id="A4.SS2.SSS0.Px1.p1.m3" class="ltx_Math">\mu_{1}=\nu</span> and <span id="A4.SS2.SSS0.Px1.p1.m4" class="ltx_Math">\mu_{2}=\delta_{0}</span>. Because they are within TV distance <span id="A4.SS2.SSS0.Px1.p1.m5" class="ltx_Math">\eps</span>, the observation <span id="A4.SS2.SSS0.Px1.p1.m6" class="ltx_Math">\tilde{\mu}=\nu</span> is compatible with both candidates. Abbreviating <span id="A4.SS2.SSS0.Px1.p1.m7" class="ltx_Math">\kappa=\hat{\kappa}[\tilde{\mu},\nu]</span>, we have</p>
<table id="A5.EGx42" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex163"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex163.m1" class="ltx_Math">\displaystyle\cE_{p}(\kappa;\mu_{1},\nu)+\cE_{p}(\kappa;\mu_{2},\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex163.m2" class="ltx_Math">\displaystyle\geq\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\mu_{1}(x)\right)%
^{\frac{1}{p}}-\Wp(\mu_{1},\nu)\right]_{+}+\Wp(\kappa_{\sharp}\mu_{2},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex164"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex164.m1" class="ltx_Math">\displaystyle=\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\nu(x)\right)^{\frac{1}{p}%
}+\Wp(\kappa_{\sharp}\delta_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex165"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex165.m1" class="ltx_Math">\displaystyle\geq(1-\eps)^{\frac{1}{p}}\left(\int\|y\|^{p}\dd\kappa_{0}(y)%
\right)^{\frac{1}{p}}+\Wp(\kappa_{\sharp}\delta_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex166"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex166.m1" class="ltx_Math">\displaystyle\geq(1-\eps)^{\frac{1}{p}}\Wp(\kappa_{\sharp}\delta_{0},\delta_{0%
})+(1-\eps)^{\frac{1}{p}}\Wp(\kappa_{\sharp}\delta_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex167"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex167.m1" class="ltx_Math">\displaystyle\geq(1-\eps)^{\frac{1}{p}}\Wp(\delta_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex168"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex168.m1" class="ltx_Math">\displaystyle\geq(1-\eps)^{\frac{1}{p}}\eps^{\frac{1}{p}}\sqrt{d}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex169"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex169.m1" class="ltx_Math">\displaystyle\geq\frac{1}{2}\eps^{\frac{1}{p}}\sqrt{d}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, we must have <span id="A4.SS2.SSS0.Px1.p1.m8" class="ltx_Math">\cE_{p}(\kappa;\mu_{1},\nu)\lor\cE_{p}(\kappa;\mu_{2},\nu)\gtrsim\sqrt{d}\eps^%
{1/p}</span>, as desired.</p>
</div>
</section>
<section id="A4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">
<span id="A4.SS2.SSS0.Px2.m1" class="ltx_Math">\bm{\Wp}</span> lower bound.</h4>

<div id="A4.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p">For the remaining bound, we first argue that, for any kernel <span id="A4.SS2.SSS0.Px2.p1.m1" class="ltx_Math">\kappa</span>, its performance for the <span id="A4.SS2.SSS0.Px2.p1.m2" class="ltx_Math">\Wp(\mu,\nu)</span> problem cannot suffer to much if we compose it with the Euclidean projection onto <span id="A4.SS2.SSS0.Px2.p1.m3" class="ltx_Math">\supp(\nu)</span>, denoted by <span id="A4.SS2.SSS0.Px2.p1.m4" class="ltx_Math">\proj_{\supp(\nu)}</span>.</p>
</div>
<div id="Thmlemma15" class="ltx_theorem ltx_theorem_lemma">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma 15</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmlemma15.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For <span id="Thmlemma15.p1.m1" class="ltx_Math">\mu\in\cP(\cX)</span>, <span id="Thmlemma15.p1.m2" class="ltx_Math">\nu\in\cP(\cY)</span>, and <span id="Thmlemma15.p1.m3" class="ltx_Math">\kappa\in\cK(\cX,\cY)</span>, we have</span></p>
<table id="A5.EGx43" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex170"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex170.m1" class="ltx_Math">\displaystyle\cE_{p}(\proj_{\supp(\nu)}\circ\kappa;\mu,\nu)\leq 4\cE_{p}(%
\kappa;\mu,\nu).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A4.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p">Write <span id="A4.SS2.SSS0.Px2.p2.m1" class="ltx_Math">f=\proj_{\supp(\nu)}</span> and <span id="A4.SS2.SSS0.Px2.p2.m2" class="ltx_Math">\eps=\cE_{p}(\kappa;\mu,\nu)</span>. Fix a coupling <span id="A4.SS2.SSS0.Px2.p2.m3" class="ltx_Math">X,Y,Z</span> such that <span id="A4.SS2.SSS0.Px2.p2.m4" class="ltx_Math">(X,Z)\sim(\Id,\kappa)_{\sharp}\mu</span>, <span id="A4.SS2.SSS0.Px2.p2.m5" class="ltx_Math">Y\sim\nu</span>, and <span id="A4.SS2.SSS0.Px2.p2.m6" class="ltx_Math">\E[\|Z-Y\|^{p}]=\Wp(\kappa_{\sharp}\mu,\nu)^{p}\leq\eps^{p}</span>. Taking <span id="A4.SS2.SSS0.Px2.p2.m7" class="ltx_Math">Z^{\prime}=f(Z)</span>, we then bound</p>
<table id="A5.EGx44" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex171"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex171.m1" class="ltx_Math">\displaystyle\E[\|X-Z^{\prime}\|^{p}]^{1/p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex171.m2" class="ltx_Math">\displaystyle\leq\E[\|X-Z\|^{p}]^{1/p}+\E[\|Z-Z^{\prime}\|^{p}]^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex172"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex172.m1" class="ltx_Math">\displaystyle=\E[\|X-Z\|^{p}]^{1/p}+\E[\|Z-f(Z)\|^{p}]^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex173"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex173.m1" class="ltx_Math">\displaystyle\leq\Wp(\mu,\nu)+\eps+\E[\|Z-Y\|^{p}]^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex174"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex174.m1" class="ltx_Math">\displaystyle\leq\Wp(\mu,\nu)+2\eps.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Similarly, we have</p>
<table id="A5.EGx45" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex175"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex175.m1" class="ltx_Math">\displaystyle\Wp(\kappa^{\prime}_{\sharp}\mu,\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex175.m2" class="ltx_Math">\displaystyle\leq\E[\|Z^{\prime}-Y\|^{p}]^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex176"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex176.m1" class="ltx_Math">\displaystyle\leq\E[\|Z-Y\|^{p}]^{1/p}+\E[\|Z-Z^{\prime}\|^{p}]^{1/p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex177"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex177.m1" class="ltx_Math">\displaystyle\leq\eps+\eps=2\eps.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, the sum of these two errors is at most <span id="A4.SS2.SSS0.Px2.p2.m8" class="ltx_Math">4\eps</span>, as desired.
∎</p>
</div>
</div>
<div id="A4.SS2.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p">Now, fix target measure <span id="A4.SS2.SSS0.Px2.p3.m1" class="ltx_Math">\nu=\frac{1}{2}\delta_{-y}+\frac{1}{2}\delta_{y}</span>, where <span id="A4.SS2.SSS0.Px2.p3.m2" class="ltx_Math">y=(1,\dots,1)</span>, and take <span id="A4.SS2.SSS0.Px2.p3.m3" class="ltx_Math">c\in[0,1]</span> to be tuned later. Then, for each <span id="A4.SS2.SSS0.Px2.p3.m4" class="ltx_Math">0\leq t\leq 1/2</span>, define measure <span id="A4.SS2.SSS0.Px2.p3.m5" class="ltx_Math">\mu_{t}=(1/2-t)\delta_{-cy}+(1/2+t)\delta_{+cy}</span>. Now, fix any kernel <span id="A4.SS2.SSS0.Px2.p3.m6" class="ltx_Math">\kappa\in\cK(\cX,\{\pm y\})</span>, where the codomain restriction is without loss of generality due to <a href="#Thmlemma15" title="Lemma 15. ‣ \Wp lower bound. ‣ D.2 Proof of 4 (Lower Bound) ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. Note that its performance on each <span id="A4.SS2.SSS0.Px2.p3.m7" class="ltx_Math">\mu_{t}</span> is determined by the two-point distributions <span id="A4.SS2.SSS0.Px2.p3.m8" class="ltx_Math">\kappa_{\pm}\defeq\kappa_{\pm cy}=(1-\alpha_{\pm})\delta_{-cy}+\alpha_{\pm}%
\delta_{cy}</span>. In particular, for <span id="A4.SS2.SSS0.Px2.p3.m9" class="ltx_Math">0\leq t&lt;1/2</span>, we compute</p>
<table id="A5.EGx46" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex178"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex178.m1" class="ltx_Math">\displaystyle\Wp(\mu_{t},\nu)^{p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex178.m2" class="ltx_Math">\displaystyle=\left(\tfrac{1}{2}-t\right)(1-c)^{p}\|y\|^{p}+t(1+c)^{p}\|y\|^{p%
}+\tfrac{1}{2}(1-c)^{p}\|y\|^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex179"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex179.m1" class="ltx_Math">\displaystyle=d^{\frac{p}{2}}\left[(1-t)(1-c)^{p}+t(1+c)^{p}\right],</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex180"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex180.m1" class="ltx_Math">\displaystyle\Wp(\kappa_{\sharp}\mu_{t},\nu)^{p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex180.m2" class="ltx_Math">\displaystyle=\Wp\left(\left(\tfrac{1}{2}-t\right)\kappa_{-}+\left(\tfrac{1}{2%
}+t\right)\kappa_{+},\nu\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex181"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex181.m1" class="ltx_Math">\displaystyle=\|y-(-y)\|^{p}\cdot\Wp\left(\left(\tfrac{1}{2}-t\right)\Ber(%
\alpha_{-})+\left(\tfrac{1}{2}+t\right)\Ber(\alpha_{+}),\Ber\left(\tfrac{1}{2}%
\right)\right)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex182"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex182.m1" class="ltx_Math">\displaystyle=(2d)^{\frac{p}{2}}\,\Wp\left(\Ber\left(\left(\tfrac{1}{2}-t%
\right)\alpha_{-}+\left(\tfrac{1}{2}+t\right)\alpha_{+}\right),\Ber\left(%
\tfrac{1}{2}\right)\right)^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex183"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex183.m1" class="ltx_Math">\displaystyle=(2d)^{\frac{p}{2}}\,\bigl{|}\left(\tfrac{1}{2}-t\right)\alpha_{-%
}+\left(\tfrac{1}{2}+t\right)\alpha_{+}-\tfrac{1}{2}\bigr{|},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex184"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex184.m1" class="ltx_Math">\displaystyle\Wp(\delta_{cy},\kappa_{+})^{p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex184.m2" class="ltx_Math">\displaystyle=\alpha_{+}(1-c)^{p}\|y\|^{p}+(1-\alpha_{+})(1+c)^{p}\|y\|^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex185"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex185.m1" class="ltx_Math">\displaystyle=d^{\frac{p}{2}}\left(\alpha_{+}(1-c)^{p}+(1-\alpha_{+})(1+c)^{p}%
\right),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex186"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex186.m1" class="ltx_Math">\displaystyle\Wp(\delta_{-cy},\kappa_{-})^{p}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex186.m2" class="ltx_Math">\displaystyle=\alpha_{-}(1+c)^{p}\|y\|^{p}+(1-\alpha_{-})(1-c)^{p}\|y\|^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex187"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex187.m1" class="ltx_Math">\displaystyle=d^{\frac{p}{2}}\left(\alpha_{-}(1+c)^{p}+(1-\alpha_{+})(1-c)^{p}%
\right),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex188"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex188.m1" class="ltx_Math">\displaystyle\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu_{t}(x)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex188.m2" class="ltx_Math">\displaystyle=\left(\tfrac{1}{2}-t\right)\Wp(\delta_{-cy},\kappa_{-})^{p}+%
\left(\tfrac{1}{2}+t\right)\Wp(\delta_{cy},\kappa_{+})^{p}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex189"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex189.m1" class="ltx_Math">\displaystyle=d^{\frac{p}{2}}\bigl{[}\left(\tfrac{1}{2}-t\right)\left(\alpha_{%
-}(1+c)^{p}+(1-\alpha_{+})(1-c)^{p}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex190"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex190.m1" class="ltx_Math">\displaystyle\hskip 28.452756pt+\left(\tfrac{1}{2}+t\right)\left(\alpha_{+}(1-%
c)^{p}+(1-\alpha_{+})(1+c)^{p}\right)\bigr{]}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A4.SS2.SSS0.Px2.p4" class="ltx_para">
<p class="ltx_p">Writing <span id="A4.SS2.SSS0.Px2.p4.m1" class="ltx_Math">\Delta=\alpha_{-}-\alpha_{+}</span>, we next bound</p>
<table id="A5.EGx47" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex191"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex191.m1" class="ltx_Math">\displaystyle\invisequals\Wp(\kappa_{\sharp}\mu_{t},\nu)+\Wp(\kappa_{\sharp}%
\mu_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex192"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex192.m1" class="ltx_Math">\displaystyle=\sqrt{2d}\,\bigl{|}\left(\tfrac{1}{2}-t\right)\alpha_{-}+\left(%
\tfrac{1}{2}+t\right)\alpha_{+}-\tfrac{1}{2}\bigr{|}^{\frac{1}{p}}+\sqrt{2d}\,%
\bigl{|}\tfrac{1}{2}\alpha_{-}+\tfrac{1}{2}\alpha_{+}-\tfrac{1}{2}\bigr{|}^{%
\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex193"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex193.m4" class="ltx_Math">\displaystyle\geq\sqrt{2d}\,t^{\frac{1}{p}}\left|\alpha_{+}-\alpha_{-}\right|^%
{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(subadditivity of <span id="A4.Ex193.m1" class="ltx_Math">a\mapsto a^{1/p}</span>)</span></td>
</tr></tbody>
<tbody id="A4.Ex194"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex194.m1" class="ltx_Math">\displaystyle=\sqrt{2d}\,t^{\frac{1}{p}}|\Delta|^{\frac{1}{p}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and we simplify</p>
<table id="A5.EGx48" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex195"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex195.m1" class="ltx_Math">\displaystyle\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu_{0}(x)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex195.m2" class="ltx_Math">\displaystyle=d^{\frac{p}{2}}\left(\frac{1+\alpha_{-}-\alpha_{+}}{2}(1+c)^{p}+%
\frac{1-\alpha_{-}+\alpha_{+}}{2}(1-c)^{p}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex196"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex196.m1" class="ltx_Math">\displaystyle=d^{\frac{p}{2}}\left(\frac{1+\Delta}{2}(1+c)^{p}+\frac{1-\Delta}%
{2}(1-c)^{p}\right)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex197"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex197.m1" class="ltx_Math">\displaystyle\Wp(\mu_{0},\nu)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex197.m2" class="ltx_Math">\displaystyle=\sqrt{d}\,(1-c).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Thus, we further bound</p>
<table id="A5.EGx49" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex198"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex198.m1" class="ltx_Math">\displaystyle\left[\left(\iint\|y-x\|^{p}\dd\kappa_{x}(y)\dd\mu_{0}(x)\right)^%
{\frac{1}{p}}-\Wp(\mu_{0},\nu)\right]_{+}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex199"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex199.m1" class="ltx_Math">\displaystyle=\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex199.m2" class="ltx_Math">\displaystyle\sqrt{d}\left[\left(\frac{1+\Delta}{2}(1+c)^{p}+\frac{1-\Delta}{2%
}(1-c)^{p}\right)^{\frac{1}{p}}-1+c\right]_{+}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex200"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex200.m1" class="ltx_Math">\displaystyle=\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex200.m2" class="ltx_Math">\displaystyle\sqrt{d}\left[\left(\frac{1+\Delta}{2}(1+c)^{p}+\frac{1-\Delta}{2%
}(1-c)^{p}\right)^{\frac{1}{p}}-1+c\right].</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Combining, this gives</p>
<table id="A5.EGx50" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex201"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex201.m1" class="ltx_Math">\displaystyle\invisequals\cE_{p}(\kappa;\mu_{t},\nu)+\cE_{p}(\kappa;\mu_{0},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex202"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex202.m1" class="ltx_Math">\displaystyle\geq\left[\sqrt{d}\left(\frac{1+\Delta}{2}(1+c)^{p}+\frac{1-%
\Delta}{2}(1-c)^{p}\right)^{\frac{1}{p}}-1+c+\sqrt{2d}\,t^{\frac{1}{p}}|\Delta%
|^{\frac{1}{p}}\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex203"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex203.m1" class="ltx_Math">\displaystyle\geq\left[\sqrt{d}\left(\frac{1+\Delta}{2}(1+c)+\frac{1-\Delta}{2%
}(1-c)\right)-1+c+\sqrt{2d}\,t^{\frac{1}{p}}|\Delta|^{\frac{1}{p}}\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex204"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex204.m1" class="ltx_Math">\displaystyle=\left[\sqrt{d}c(\Delta+1)+\sqrt{2d}\,t^{\frac{1}{p}}|\Delta|^{1/%
p}\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex205"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex205.m1" class="ltx_Math">\displaystyle\geq\sqrt{d}\left[c(1-|\Delta|)+t^{1/p}|\Delta|\right]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex206"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex206.m1" class="ltx_Math">\displaystyle\geq\sqrt{d}\min\{c/2,t^{1/p}/2\}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, supposing that <span id="A4.SS2.SSS0.Px2.p4.m2" class="ltx_Math">\rho&lt;\sqrt{d}</span>, we can safely take <span id="A4.SS2.SSS0.Px2.p4.m3" class="ltx_Math">c=t^{1/p}=\rho^{1/2}d^{-1/4}/2</span> while ensuring that <span id="A4.SS2.SSS0.Px2.p4.m4" class="ltx_Math">c\in[0,1]</span> and <span id="A4.SS2.SSS0.Px2.p4.m5" class="ltx_Math">t\in[0,1/2]</span>, which were the only constraints on our construction. Otherwise, we take <span id="A4.SS2.SSS0.Px2.p4.m6" class="ltx_Math">c=t^{1/p}=1/2</span>. In either case, we have <span id="A4.SS2.SSS0.Px2.p4.m7" class="ltx_Math">\Wp(\mu_{0},\mu_{t})=t^{1/p}\cdot 2c\sqrt{d}=(\rho\land\sqrt{d})/2\leq\rho</span>. Thus, the observation <span id="A4.SS2.SSS0.Px2.p4.m8" class="ltx_Math">\tilde{\mu}=\mu_{0}</span> is compatible with both <span id="A4.SS2.SSS0.Px2.p4.m9" class="ltx_Math">\mu=\mu_{0}</span> and <span id="A4.SS2.SSS0.Px2.p4.m10" class="ltx_Math">\mu_{t}</span> under our corruption model. This gives the desired minimax lower bound of <span id="A4.SS2.SSS0.Px2.p4.m11" class="ltx_Math">\Omega(\sqrt{d}c\land t^{1/p})=\Omega(d^{1/4}\rho^{1/2}\land\sqrt{d})</span>.</p>
</div>
</section>
</section>
<section id="A4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>Efficient Computation</h3>

<div id="A4.SS3.p1" class="ltx_para">
<p class="ltx_p">We now introduce <a href="#A4.SS3" title="D.3 Efficient Computation ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.3</span></a> to achieve efficient computation, focusing on <span id="A4.SS3.p1.m1" class="ltx_Math">p=1</span> where we match the rate of <a href="#Thmtheorem4" title="Theorem 4 (Robust estimation guarantee). ‣ 5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Here, we identify finite sets with their uniform distributions when convenient.</p>
</div>
<span class="ltx_ERROR undefined">{algorithm2e}</span>
<div id="A4.SS3.p2" class="ltx_para">
<p class="ltx_p">[t]
<span class="ltx_text ltx_caption">Randomized Rounding for Efficient OT Kernel Estimation</span>

<span class="ltx_ERROR undefined">\KwIn</span><span id="A4.SS3.p2.m1" class="ltx_Math">n</span> corrupted source points <span id="A4.SS3.p2.m2" class="ltx_Math">S\subseteq\R^{d}</span> and target points <span id="A4.SS3.p2.m3" class="ltx_Math">T\subseteq[0,1]^{d}</span>, budgets <span id="A4.SS3.p2.m4" class="ltx_Math">\rho\geq 0</span>, <span id="A4.SS3.p2.m5" class="ltx_Math">\eps\in[0,1]</span>
<span class="ltx_ERROR undefined">{algorithmic}</span>[1]
<span class="ltx_ERROR undefined">\State</span><span id="A4.SS3.p2.m6" class="ltx_Math">m\leftarrow n^{2}</span>, <span id="A4.SS3.p2.m7" class="ltx_Math">\tau\leftarrow n^{-1/(d+2)}</span>, <span id="A4.SS3.p2.m8" class="ltx_Math">\sigma\leftarrow 3^{d/(2+d)}(nd)^{-1/(d+2)}+\rho^{1/2}d^{-1/4}</span>
<span class="ltx_ERROR undefined">\State</span><span id="A4.SS3.p2.m9" class="ltx_Math">S^{\prime}\leftarrow\{\proj_{S}(X^{\prime}_{i}+Z_{i})\}_{i=1}^{m}</span>, where each <span id="A4.SS3.p2.m10" class="ltx_Math">X^{\prime}_{i}\!\sim\!S</span> and <span id="A4.SS3.p2.m11" class="ltx_Math">Z_{i}\!\sim\!\cN_{\sigma}</span> are sampled independently
<span class="ltx_ERROR undefined">\State</span>Compute kernel <span id="A4.SS3.p2.m12" class="ltx_Math">\bar{\kappa}\in\cK(S^{\prime},T)</span> s.t. <span id="A4.SS3.p2.m13" class="ltx_Math">\bar{\kappa}_{\sharp}\Unif(S^{\prime})=\Unif(T)</span></p>
<table id="A4.Ex207" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex207.m1" class="ltx_Math">\frac{1}{m}\sum_{x\in S^{\prime}}\int\|x-y\|\dd\bar{\kappa}(y|x)\leq\Wone(S^{%
\prime},T)+\tau\vspace{-2mm}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<span class="ltx_ERROR undefined">\State</span>
<p class="ltx_p">Return <span id="A4.SS3.p2.m14" class="ltx_Math">\hat{\kappa}\in\cK(\R^{d},T)</span> defined by <span id="A4.SS3.p2.m15" class="ltx_Math">\hat{\kappa}=\bar{\kappa}\circ\proj_{S}\circ\,N^{\sigma}</span></p>
</div>
<div id="Thmtheorem5" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 5</span></span><span class="ltx_text ltx_font_bold"> </span>(Efficient implementation)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmtheorem5.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Under the setting of <a href="#S5" title="5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> with <span id="Thmtheorem5.p1.m1" class="ltx_Math">p=1</span>, the kernel <span id="Thmtheorem5.p1.m2" class="ltx_Math">\hat{\kappa}</span> returned by <a href="#A4.SS3" title="D.3 Efficient Computation ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.3</span></a> matches the risk bound of <a href="#Thmtheorem4" title="Theorem 4 (Robust estimation guarantee). ‣ 5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Using an entropic OT solver for Step 3, <a href="#A4.SS3" title="D.3 Efficient Computation ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.3</span></a> runs in time <span id="Thmtheorem5.p1.m3" class="ltx_Math">O((C_{\infty}+d)n^{2+o_{d}(1)})</span>, where <span id="Thmtheorem5.p1.m4" class="ltx_Math">C_{\infty}=\max_{i,j}\|\tilde{X}_{i}-\tilde{Y}_{j}\|</span>. Moreover, <span id="Thmtheorem5.p1.m5" class="ltx_Math">\hat{\kappa}</span> can be evaluated (i.e., given <span id="Thmtheorem5.p1.m6" class="ltx_Math">x\in\cX</span> we can sample <span id="Thmtheorem5.p1.m7" class="ltx_Math">Y\sim\hat{\kappa}_{x}</span>) in time <span id="Thmtheorem5.p1.m8" class="ltx_Math">O(nd)</span>.</span></p>
</div>
</div>
<div id="A4.SS3.p3" class="ltx_para">
<p class="ltx_p">The proof below employs a similar analysis to that of <a href="#Thmtheorem4" title="Theorem 4 (Robust estimation guarantee). ‣ 5 Robust Estimation with Adversarial Corruptions ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, with multiple applications of <a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> to account for various sampling errors along with TV contamination. We restrict to <span id="A4.SS3.p3.m1" class="ltx_Math">p=1</span> due to the worsened scaling of <a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for <span id="A4.SS3.p3.m2" class="ltx_Math">p&gt;1</span>.</p>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A4.SS3.p4" class="ltx_para">
<p class="ltx_p">Set <span id="A4.SS3.p4.m1" class="ltx_Math">\alpha=N^{\sigma}_{\sharp}\tilde{\mu}_{n}</span>, <span id="A4.SS3.p4.m2" class="ltx_Math">\beta=\proj_{S}\alpha</span>, and <span id="A4.SS3.p4.m3" class="ltx_Math">\beta_{m}=\Unif(S^{\prime})</span>. By construction, <span id="A4.SS3.p4.m4" class="ltx_Math">S^{\prime}</span> is sampled i.i.d. from <span id="A4.SS3.p4.m5" class="ltx_Math">\beta</span>, so Fact <a href="#Thmfact5" title="Fact 5 (TV discrete empirical convergence). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> gives that <span id="A4.SS3.p4.m6" class="ltx_Math">\E[\|\beta-\beta_{m}\|_{\tv}]=\E[\E[\|\beta-\beta_{m}\|_{\tv}|S^{\prime}]]%
\lesssim\sqrt{n/m}</span>. Moreover,</p>
<table id="A5.EGx51" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex208"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex208.m1" class="ltx_Math">\displaystyle\int\|\proj_{S}(x)-x\|\dd\alpha(x)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex208.m2" class="ltx_Math">\displaystyle=\frac{1}{n}\sum_{x\in S}\int\|\proj_{S}(x+z)-x+z\|\dd N^{\sigma}%
(z)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex209"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex209.m4" class="ltx_Math">\displaystyle\leq\frac{1}{n}\sum_{x\in S}\int\|x-x+z\|\dd N^{\sigma}(z)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(<span id="A4.Ex209.m1" class="ltx_Math">x\in S</span>)</span></td>
</tr></tbody>
<tbody id="A4.Ex210"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex210.m1" class="ltx_Math">\displaystyle=\int\|z\|\dd N^{\sigma}(z)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex211"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex211.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\,\sigma.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Now, we restate our guarantee for <span id="A4.SS3.p4.m7" class="ltx_Math">\bar{\kappa}</span>; namely, we have:</p>
<table id="A4.Ex212" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex212.m1" class="ltx_Math">\iint\|x-y\|\dd\bar{\kappa}(y|x)\dd\beta_{m}(x)\leq\Wone(\beta_{m},\tilde{\nu}%
_{n})+\tau.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Thus, by <a href="#Thmlemma7" title="Lemma 7. ‣ A.6 Proof of 5 ‣ Appendix A Proofs for 2 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we have</p>
<table id="A4.Ex213" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex213.m1" class="ltx_Math">\cE_{1}(\bar{\kappa};\beta,\tilde{\nu}_{n})\lesssim\tau+\sqrt{d}\,\|\beta-%
\beta_{m}\|_{\tv},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">and, applying <a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we obtain</p>
<table id="A4.Ex214" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex214.m1" class="ltx_Math">\cE_{1}(\bar{\kappa}\circ\proj_{S};\alpha,\tilde{\nu}_{n})\lesssim\tau+\sqrt{d%
}\,\|\beta-\beta_{m}\|_{\tv}+\sqrt{d}\,\sigma.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Now, write <span id="A4.SS3.p4.m8" class="ltx_Math">\mu^{\prime}_{n}\in\cP(\R^{d})</span> for an intermediate measure such that <span id="A4.SS3.p4.m9" class="ltx_Math">\|\mu^{\prime}_{n}-\tilde{\mu}_{n}\|_{\tv}\leq\eps</span> and <span id="A4.SS3.p4.m10" class="ltx_Math">\Wp(\mu^{\prime}_{n},\hat{\mu}_{n})\leq\rho</span>. Noting that <span id="A4.SS3.p4.m11" class="ltx_Math">\alpha=N^{\sigma}_{\sharp}\tilde{\mu}_{n}</span>, we have by Fact <a href="#Thmfact3" title="Fact 3 (TV contraction under Markov kernels). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> that <span id="A4.SS3.p4.m12" class="ltx_Math">\|\alpha-N^{\sigma}_{\sharp}\mu^{\prime}_{n}\|_{\tv}\leq\eps</span>. Thus, <a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> gives</p>
<table id="A4.Ex215" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex215.m1" class="ltx_Math">\cE_{1}(\bar{\kappa}\circ\proj_{S};N^{\sigma}_{\sharp}\mu^{\prime}_{n},\tilde{%
\nu}_{n})\lesssim\sqrt{d}\eps+\tau+\sqrt{d}\,\|\beta-\beta_{m}\|_{\tv}+\sqrt{d%
}\,\sigma.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Applying <a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> once more, we obtain</p>
<table id="A4.Ex216" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex216.m1" class="ltx_Math">\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma/2};N^{\sigma/2}_{\sharp}\mu^%
{\prime}_{n},\tilde{\nu}_{n})\lesssim\sqrt{d}\eps+\tau+\sqrt{d}\,\|\beta-\beta%
_{m}\|_{\tv}+\sqrt{d}\,\sigma.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">By <a href="#Thmlemma14" title="Lemma 14. ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, the fact that this latest kernel begins with the convolution <span id="A4.SS3.p4.m13" class="ltx_Math">N^{\sigma/2}</span> ensures that it is <span id="A4.SS3.p4.m14" class="ltx_Math">O(\sqrt{d}\sigma^{-1})</span>-Lipschitz w.r.t. <span id="A4.SS3.p4.m15" class="ltx_Math">\Wone</span>. Moreover, by Fact <a href="#Thmfact4" title="Fact 4 (\Wp contraction under convolution). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we have <span id="A4.SS3.p4.m16" class="ltx_Math">\Wone(N^{\sigma/2}_{\sharp}\mu^{\prime}_{n},N^{\sigma/2}_{\sharp}\hat{\mu}_{n}%
)\leq\Wone(\mu^{\prime}_{n},\hat{\mu}_{n})\leq\rho</span>. Thus, <a href="#Thmlemma4" title="Lemma 4 (\Wp stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives</p>
<table id="A4.Ex217" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span id="A4.Ex217.m1" class="ltx_Math">\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma/2};N^{\sigma/2}_{\sharp}\hat%
{\mu}_{n},\tilde{\nu}_{n})\lesssim\sqrt{d}\eps+\tau+\sqrt{d}\,\|\beta-\beta_{m%
}\|_{\tv}+\sqrt{d}\,\sigma+\rho+\frac{\sqrt{d}\,\rho}{\sigma}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">Next, we apply <a href="#Thmlemma5" title="Lemma 5 (TV stability in μ). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#Thmlemma6" title="Lemma 6 (Kernel composition). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> to bound</p>
<table id="A5.EGx52" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex218"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex218.m1" class="ltx_Math">\displaystyle\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma};\mu,\tilde{\nu%
}_{n})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex219"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex219.m1" class="ltx_Math">\displaystyle\lesssim\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex219.m2" class="ltx_Math">\displaystyle\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma/2};N^{\sigma/2}%
_{\sharp}\mu,\tilde{\nu}_{n})+\sqrt{d}\,\sigma</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex220"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex220.m1" class="ltx_Math">\displaystyle\lesssim\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex220.m2" class="ltx_Math">\displaystyle\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma/2};N^{\sigma/2}%
_{\sharp}\hat{\mu}_{n},\tilde{\nu}_{n})+\sqrt{d}\,\sigma+\sqrt{d}\,\|N^{\sigma%
/2}(\mu-\hat{\mu}_{n})\|_{\tv}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex221"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex221.m1" class="ltx_Math">\displaystyle\lesssim\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex221.m2" class="ltx_Math">\displaystyle\sqrt{d}\,\eps+\tau+\sqrt{d}\,\sigma+\rho+\frac{\sqrt{d}\,\rho}{%
\sigma}+\sqrt{d}\,\|\beta-\beta_{m}\|_{\tv}+\sqrt{d}\,\|N^{\sigma/2}(\mu-\hat{%
\mu}_{n})\|_{\tv}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Finally, we correct the target measure, using <a href="#Thmlemma3" title="Lemma 3 (Stability in ν). ‣ 2 Basic Properties of the Error Functional ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to bound</p>
<table id="A5.EGx53" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex222"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex222.m1" class="ltx_Math">\displaystyle\invisequals\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma};%
\mu,\nu)\leq\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma};\mu,\tilde{\nu}%
_{n})+2\Wp(\tilde{\nu}_{n},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex223"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex223.m1" class="ltx_Math">\displaystyle\lesssim\sqrt{d}\,\eps+\tau+\sqrt{d}\,\sigma+\rho+\frac{\sqrt{d}%
\,\rho}{\sigma}+\sqrt{d}\,\|\beta-\beta_{m}\|_{\tv}+\sqrt{d}\,\|N^{\sigma/2}(%
\mu-\hat{\mu}_{n})\|_{\tv}+\Wp(\hat{\nu}_{n},\nu)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Taking expectations, using our early bound on the first TV distance, and applying <a href="#Thmlemma13" title="Lemma 13 (Prop. 2 of Goldfeld et al., 2020). ‣ Appendix D Proofs for 5 ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> for the second TV distance, and applying <a href="#Thmlemma1" title="Lemma 1 (\Wp empirical convergence, Lei, 2020). ‣ Statistical distances and empirical convergence. ‣ 1.2 Preliminaries ‣ 1 Introduction ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the Wasserstein distance, we obtain</p>
<table id="A5.EGx54" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A4.Ex224"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex224.m1" class="ltx_Math">\displaystyle\E[\cE_{1}(\bar{\kappa}\circ\proj_{S}\circ N^{\sigma};\mu,\nu)]</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A4.Ex225"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="A4.Ex225.m1" class="ltx_Math">\displaystyle\lesssim\,</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="A4.Ex225.m2" class="ltx_Math">\displaystyle\sqrt{d}\,\eps+\tau+\sqrt{d}\,\sigma+\rho+\frac{\sqrt{d}\,\rho}{%
\sigma}+\sqrt{\frac{dn}{m}}+\sqrt{d\,3^{d}(1\lor\sigma^{-d})/n}+c_{p,d}n^{-%
\frac{1}{p\lor 2d}}\log^{2}n</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Our choice of <span id="A4.SS3.p4.m17" class="ltx_Math">\sigma</span>, <span id="A4.SS3.p4.m18" class="ltx_Math">m</span>, and <span id="A4.SS3.p4.m19" class="ltx_Math">\tau</span> ensure that the desired risk bound holds.</p>
</div>
<div id="A4.SS3.p5" class="ltx_para">
<p class="ltx_p">Computational complexity is dominated by the OT computation at Step 3. The source and target distributions are both supported on <span id="A4.SS3.p5.m1" class="ltx_Math">n</span> points, and we require accuracy <span id="A4.SS3.p5.m2" class="ltx_Math">\tau=n^{-1/(d+2)}</span>. Computing the relevant cost matrix requires time <span id="A4.SS3.p5.m3" class="ltx_Math">O(n^{2}d)</span>. Using a state of the art OT solver based on entropic OT (e.g., <cite class="ltx_cite ltx_citemacro_citep">Luo et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a></cite>) gives a running time of <span id="A4.SS3.p5.m4" class="ltx_Math">O(C_{\infty}n^{2}/\tau)=O(C_{\infty}n^{2+1/(d+2)})</span>, where <span id="A4.SS3.p5.m5" class="ltx_Math">C_{\infty}</span> is the largest distance between a point in <span id="A4.SS3.p5.m6" class="ltx_Math">S</span> and a point in <span id="A4.SS3.p5.m7" class="ltx_Math">T</span>. Combining these two gives the first bound. Evaluation complexity is dominated by the projection step, which can be computed in a brute-force manner using <span id="A4.SS3.p5.m8" class="ltx_Math">O(nd)</span> time.
∎</p>
</div>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Additional Experiments</h2>

<div id="A5.p1" class="ltx_para">
<p class="ltx_p">All code needed to reproduce our experiments and figures is available at <a href="https://github.com/sbnietert/map-estimation" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/sbnietert/map-estimation</a>. Here, we include two additional experiments beyond those in the main body, one with higher dimensions and sample sizes, and one in dimension two, for visualization.</p>
</div>
<figure id="A5.F4" class="ltx_figure"><img src="" id="A5.F4.g1" class="ltx_graphics ltx_centering" alt=" (left), optimality gap (middle), and feasibility gap (right) performance of nearest-neighbor and rounding estimators for Setting A.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="A5.F4.m2" class="ltx_Math">\cE_{1}</span> (left), optimality gap (middle), and feasibility gap (right) performance of nearest-neighbor and rounding estimators for Setting A.</figcaption>
</figure>
<div id="A5.p2" class="ltx_para">
<p class="ltx_p">First, in <a href="#S6.F3" title="Figure 3 ‣ 6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we repeat the Setting A experiments from <a href="#S6" title="6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (<a href="#S6.F3" title="Figure 3 ‣ 6 Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, middle), but with <span id="A5.p2.m1" class="ltx_Math">N=10000</span>, dimensions <span id="A5.p2.m2" class="ltx_Math">d\in\{5,10,15\}</span>, and sample sizes <span id="A5.p2.m3" class="ltx_Math">n\in\{100,200,\dots,1000\}</span>. To extend to these larger parameters, we reduced the number of iterations to <span id="A5.p2.m4" class="ltx_Math">T=5</span> and omitted the bootstrapped error bars. Also, we include the decomposition of <span id="A5.p2.m5" class="ltx_Math">\cE_{1}</span> into its optimality gap and feasibility gap components, the latter of which is measurably larger. As predicted by our analysis, our error rates worsen with dimension.</p>
</div>
<div id="A5.p3" class="ltx_para">
<p class="ltx_p">Finally, in <a href="#A5.F5" title="Figure 5 ‣ Appendix E Additional Experiments ‣ Estimation of Stochastic Optimal Transport Maps" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we provide a visual depiction of the rounding estimator on a toy checkerboard dataset. In the top left, we present our source measure (orange) and target measure (green). For the top right plot, we sampled <span id="A5.p3.m1" class="ltx_Math">n=100</span> source and target samples, rounded the source samples onto a regular grid with side length <span id="A5.p3.m2" class="ltx_Math">\delta=n^{-1/(d+2)}</span> (orange), and computed an OT plan (light blue) from the rounded source samples to the target samples (green). For the bottom left, we rounded the full source distribution onto the same grid (orange), route these according to the same OT plan (light blue), reaching a destination measure (red) that approximates the target distribution (green).</p>
</div>
<figure id="A5.F5" class="ltx_figure"><img src="processed_dataset/2512.09499v1/temp_source/figures/toy_plots.png" id="A5.F5.g1" class="ltx_graphics ltx_centering" width="486" height="348" alt="Visual depiction of rounding kernel estimator on checkerboard dataset.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visual depiction of rounding kernel estimator on checkerboard dataset.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Dec 11 22:13:29 2025 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
