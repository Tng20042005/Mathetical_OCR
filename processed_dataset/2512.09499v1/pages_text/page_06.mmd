This improved n^{-1/(d+2p)} rate is near-optimal; indeed, when \mu is a point mass, the problem reduces to estimation of \nu under \Wp, for which there are existing minimax lower bounds of order n^{-1/(d\lor 2p)} (Singh and Póczos, 2018) (see Appendix B.2 for full details). We also note that the tail bounds on \mu and \nu can be weakened further under our analysis, but not without worsening the rate.

We sketch the proof when \mu is sub-Gaussian, \delta=0, and \diam(\cY)\leq 1; see Appendix B.3 for the full derivation. We first show, for the cubic partition \cP with side length r, that \mu^{\prime}\_{n}=(r\_{\cP})\_{\sharp}\hat{\mu}\_{n} and \mu^{\prime}=(r\_{\cP})\_{\sharp}\mu converge in TV at rate \sqrt{r^{-d}/n}. Here, r^{-d} arises as a bound on the number of relevant partition blocks. We then bound

|  |  |  |  |
| --- | --- | --- | --- |
|  | \displaystyle\cE\_{p}(\hat{\kappa}\_{n};\mu,\nu) | \displaystyle=\cE\_{p}(\bar{\kappa}\_{n}\circ r\_{\cP};\mu,\nu) |  |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | \displaystyle\leq\cE\_{p}(\bar{\kappa}\_{n};\mu^{\prime},\nu)+\sqrt{d}r |  | (6) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | \displaystyle\lesssim\Wp(\nu,\hat{\nu}\_{n})+(nr^{d})^{-\frac{1}{2p}}+\sqrt{d}r |  | (5\crefpairconjunction3) |

Applying 1 and tuning r gives the theorem. The general sub-Gaussian case follows by a similar argument. If \mu only has bounded 2pth moments, we can still achieve TV convergence at a comparable rate by employing a partition whose bins increase in size away from the origin.

###### Remark 3 (One-dimensional refinements).

Given the gap between our n^{-1/(d+2p)} upper bound and the n^{-1/(d\lor 2p)} lower bound of Singh and Póczos (2018), it is natural to ask if the lower bound can be improved. At least when d=1, this is impossible. In B.4, we improve the rate from 2 to n^{-1/2} when d=1, using stability of \cE\_{p} under the Kolmogorov-Smirnov metric.

## 4 Improved Statistical Guarantees with Hölder Continuous Optimal Kernels

For p=2, many existing works assume the existence of a Brenier map T^{\star} whose gradient has eigenvalues bounded from above and below (in particular, such a T^{\star} is Lipschitz). For example, Balakrishnan and Manole (2025) show that a nearest-neighbor estimator achieves \E\|\hat{T}\_{n}-T^{\star}\|\_{L^{2}(\mu)}=\smash{\widetilde{O}(n^{-1/(d\lor 4)})}, matching the \Wtwo empirical convergence rate. Of course, by 1, this guarantee also holds under \cE\_{2}. In this section, we treat general p\geq 1 under the related but distinctly weaker assumption that \Wp(\mu,\nu) admits an optimal kernel which is Hölder continuous under \Wp.

###### Assumption 1.

There exists L\geq 1, \alpha\in(0,1], and an optimal kernel \kappa^{\star}\in\cK(\cX,\cY) for \Wp(\mu,\nu) such that \Wp(\kappa^{\star}\_{x},\kappa^{\star}\_{x^{\prime}})\leq L\|x-x^{\prime}\|^{\alpha} for all x,x^{\prime}\in\cX.

Here, we propose an estimator based on Wasserstein distributionally robust optimization (WDRO):

|  |  |  |
| --- | --- | --- |
|  | \hat{\kappa}^{\rho}\_{\mathrm{DRO}}[\hat{\mu},\hat{\nu}]\defeq\argmin\_{\kappa% \in\cK(\R^{d},\cY)}\sup\_{\begin{subarray}{c}\mu^{\prime}\in\cP(\R^{d}):\,\Wp(% \mu^{\prime},\hat{\mu})\leq\rho\end{subarray}}\cE\_{p}(\kappa;\mu^{\prime},\hat% {\nu}), |  |

where \hat{\mu},\hat{\nu} are any proxies for \mu,\nu (potentially their empirical measures).
This estimator is computationally inefficient but allows us to better understand the statistical limits of estimation.

###### Theorem 3 (WDRO estimator).

Under Assumption 1, suppose \Wp(\hat{\mu},\mu)\leq\rho\_{\mu} and \Wp(\hat{\nu},\nu)\leq\rho\_{\nu}. Then the estimate \hat{\kappa}=\hat{\kappa}\_{\mathrm{DRO}}^{2\rho\_{\mu}}[\hat{\mu},\hat{\nu}] achieves \cE\_{p}(\hat{\kappa};\mu,\nu)\lesssim L\rho\_{\mu}^{\alpha}+\rho\_{\mu}+\rho\_{\nu}.

###### Proof.

Using the WDRO problem structure and our stability lemmas, we bound

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | \displaystyle\cE\_{p}(\hat{\kappa};\mu,\nu) | \displaystyle\leq\cE\_{p}(\hat{\kappa};\mu,\hat{\nu})+2\rho\_{\nu} |  | (3) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | \displaystyle\leq\sup\_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu% ^{\prime}:\,\Wp(\mu^{\prime},\hat{\mu})\leq\rho\_{\mu}\end{subarray}}\cE\_{p}(% \hat{\kappa};\mu^{\prime},\hat{\nu})+2\rho\_{\nu} |  | (\Wp(\mu,\hat{\mu})\leq\rho\_{\mu}) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | \displaystyle\leq\sup\_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu% ^{\prime}:\,\Wp(\mu^{\prime},\hat{\mu})\leq\rho\_{\mu}\end{subarray}}\cE\_{p}(% \kappa\_{\star};\mu^{\prime},\hat{\nu})+2\rho\_{\nu} |  | (optimality of \hat{\kappa}) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | \displaystyle\leq\sup\_{\begin{subarray}{c}\hidden@noalign{}\hfil\scriptsize\mu% ^{\prime}\in\cP(\R^{d}):\,\Wp(\mu^{\prime},\mu)\leq 2\rho\_{\mu}\end{subarray}}% \cE\_{p}(\kappa\_{\star};\mu^{\prime},\hat{\nu})+2\rho\_{\nu} |  | (\Wp triangle inequality) |
|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  |  | \displaystyle\leq 4L\rho\_{\mu}^{\alpha}+4\rho\_{\mu}+2\rho\_{\nu}, |  | (4) |

as desired.
∎

This gives an information-theoretic reduction from kernel estimation under \cE\_{p} to estimation of \mu and \nu under \Wp, i.e., if we can estimate \mu up to error \rho\_{\mu} and \nu up to error \rho\_{\nu}, then we can find a kernel with error O(L\rho\_{\mu}^{\alpha}+\rho\_{\mu}+\rho\_{\nu}). Focusing on the Lipschitz case with \alpha=1, we first apply 3 using the plug-in estimators \mu=\hat{\mu}\_{n}, \nu=\hat{\nu}\_{n}.

###### Corollary 1 (Plug-in estimators).

Under Assumption 1 with \alpha=1 and L=O(1), suppose \mu,\nu\in\cP\_{2p}(\R^{d}). Then taking \hat{\mu}=\hat{\mu}\_{n} and \hat{\nu}=\hat{\nu}\_{n}, \rho can be tuned to achieve \cE\_{p}(\hat{\kappa}\_{\mathrm{DRO}}^{\rho};\mu,\nu)=\widetilde{O}\_{p,d}(n^{-1/%
(d\lor 2p)}) with probability 0.9, which is minimax optimal up to logarithmic factors.