f \bigl{(}\sqrt{d}\eps^{\frac{1}{p}}+\sqrt{d}\rho^{\frac{1}{p+1}}+\rho+O\_{p,d}(n%
^{-\frac{1}{d+2p}})\bigr{)}\land\sqrt{d}. Moreover, up to constants, no estimator can achieve worst-case expected error less than \bigl{(}\sqrt{d}\eps^{\frac{1}{p}}+d^{1/4}\rho^{1/2}+n^{-\frac{1}{d\lor 2p}}%
\bigr{)}\land\sqrt{d}.

The upper bound follows by a remarkably straightforward application of our stability lemmas. For the d^{1/4}\rho^{1/2} term in the LB, we construct a pair of instances (with all distributions supported on two points) which are indistinguishable from \rho-corrupted samples and such that no kernel achieves error o(d^{1/4}\rho^{1/2}) on both. Interestingly, this \sqrt{\rho} dependence rules out a lossless reduction from estimation under \cE\_{p} to distribution estimation under \Wp. That is, our rounding estimator from 3 achieves \cE\_{p}=\widetilde{O}(n^{-1/(d+2p)}) but the guarantee that \Wp(\hat{\mu}\_{n},\mu)\lor\Wp(\hat{\nu}\_{n},\nu)=\widetilde{O}(n^{-1/(d\lor 2p%
)}) alone cannot imply a rate faster than \widetilde{O}(n^{-1/(2d\lor 4p)}). Finally, although the convolved OT problem for our estimator may not be efficiently solvable, we show in D that an additional rounding step enables efficient computation, mirroring the proof of 2.

## 6 Experiments

To empirically validate our theory, we run experiments in two synthetic settings with OT maps whose irregularities limit the utility of the L^{p} objective and prevent application of existing theory.
For Setting A, we fix \mu and \nu as uniform discrete measures over N=2000 points, obtained as i.i.d. samples from \Unif(\{0\}\times[0,1]^{d-1}) and \frac{1}{2}\Unif(\{-1\}\times[0,1]^{d-1})+\frac{1}{2}\Unif(\{1\}\times[0,1]^{d%
-1}), respectively.
In the N\to\infty limit, the optimal kernel satisfies \kappa^{\star}\_{(0,x\_{2:d})}=\Unif(\{(-1,x\_{2:d}),(1,x\_{2:d})\}). For our discrete \mu and \nu, there is an optimal deterministic map T^{\star} induced by a permutation, but it is highly oscillatory. For Setting B, we set \mu and \nu as discrete distributions over N samples from \Unif([-1,1]^{d}) and f\_{\sharp}\Unif([-1,1]^{d}), respectively, where f(x)=x+(\operatorname{sign}(x\_{1}),\dots,\operatorname{sign}(x\_{d})) pushes each orthant of the cube away from the origin. Here, the OT map is discontinuous but Lipschitz within each orthant.

Now, for each setting and sample size n\in\{10,20,\dots,100\}, we take n i.i.d. samples from \mu and \nu and compute the p=1 nearest-neighbor map estimate \smash{\hat{T}\_{n}^{\mathrm{NN}}} (Manole et al., 2024) and the rounding kernel estimate \hat{\kappa}\_{n}^{\mathrm{round}} (3). (Specifically, the NN estimator first computes an optimal \Wone map \bar{T}\_{n} from \hat{\mu}\_{n} to \hat{\nu}\_{n}. Then, \hat{T}\_{n}^{\mathrm{NN}} maps each x\in\R^{d} to the image of its nearest source point under \bar{T}\_{n}.) We then compute the L^{1} error \|\hat{T}\_{n}^{\mathrm{NN}}-T^{\star}\|\_{L^{1}(\mu)} and the \cE\_{1} errors \smash{\cE\_{1}(\hat{T}\_{n}^{\mathrm{NN}};\mu,\nu),\cE\_{1}(\hat{\kappa}\_{n}^{%
\mathrm{round}};\mu,\nu)}. Since \mu and \nu are discrete, these can be computed using finite sums and the default Python Optimal Transport solver (Flamary et al., 2021). Repeating this process for K=100 iterations, we compute mean errors for each sample size and dimension d\in\{3,5,10\}, along with bootstrapped 10% and 90% quantiles (via 1000 bootstrap resamples). In 3 (left), we compare the \cE\_{1} vs L^{1} performance of the NN estimator under Setting A (where the latter is well-defined since each \hat{T}\_{n}^{\mathrm{NN}} is a deterministic map). As expected, L^{1} performance is quite poor, with error always greater than 1. Although we currently lack formal guarantees for the NN estimator (and our setting lies outside of existing theory), it achieves strong \cE\_{1} performance, with faster rates in lower dimensions.
In 3 (center), we compare the NN and rounding estimators under \cE\_{1}, the latter enjoying formal guarantees by 2. Empirically, the NN estimator performs better, but this gap diminishes in high dimensions. We suspect that low-dimensional performance of the rounding estimator is more sensitive to its side-length hyperparameter, which we have simply set to n^{-1/(d+2)} as per the proof of 2. Finally, in 3 (right) we turn to Setting B, again comparing \smash{\hat{T}\_{n}^{\mathrm{NN}}} and \hat{\kappa}\_{n}^{\mathrm{round}} under \cE\_{1} and observing similar trends. We note that all experiments were performed on an M1 MacBook Air with 16GB RAM and 8 CPU cores. See Appendix E for full experiment details and two additional experiments (one with larger parameter settings, but no bootstrapping, and one in two dimensions, so that our estimator can be visualized).

###### Remark 5 (Neural map estimation).

Given the connections between \cE\_{p} and the Monge gap objective discussed in 1\crefpairconjunction2, one could consider training a neural map estimator to minimize an empirical \cE\_{p} objective, perhaps after approximating the \Wp terms via EOT. However, while both \cE\_{p} and the Monge gap objective nullify on optimal maps, they behave quite differently when far from optimality. Indeed, gradients of the feasibility gap term in \cE\_{p} push towards the identity map (since it achieves the minimum transport cost of zero), while gradients of the Monge gap push towards the much larger set of c-cyclically monotone maps. In preliminary tests, we found that the Monge gap objective led to significantly more stable training dynamics, which we attribute to this difference. Thus, we maintain our recommendation of \cE\_{p} as an evaluation metric, enabling provable error guarantees under weaker assumptions, rather than a training objective for neural map estimation. Still, we hope that our analysis under \cE\_{p} might inspire new regularization methods in the future.

Figure 3: \cE\_{1} and L^{1} performance of nearest-neighbor and rounding estimators in two settings.

## 7 Discussion