|  |  |  |  |  |
| --- | --- | --- | --- | --- |
|  | \displaystyle\dd\pi\_{\tau}(y|x) | \displaystyle=\exp\bigl{(}(f\_{\tau}(x)+g\_{\tau}(y)-\|x-y\|^{p})/\tau\bigr{)}\,% \dd\nu(y) |  | (7) |
|  |  | \displaystyle=\frac{\exp\bigl{(}(g\_{\tau}(y)-\|x-y\|^{p})/\tau\bigr{)}\,\dd\nu% (y)}{\int\exp\bigl{(}(g\_{\tau}(y^{\prime})-\|x-y^{\prime}\|^{p})/\tau\bigr{)}% \,\dd\nu(y^{\prime})}. |  |

By this result, we may assume that the entropic kernel is defined over all x\in\R^{d}.
We show that, if \diam(\cX\cup\cY) is bounded, the empirical entropic kernel achieves a vanishing \cE\_{p} error.

###### Theorem 1 (Entropic kernel estimator).

Assume \cX,\cY\subseteq[0,1]^{d} and set \tau=d^{p/4}n^{-1/(2d\lor 4)}\log n.
Let \hat{\pi}\_{\tau,n} be the optimal coupling for S\_{p,\tau}(\hat{\mu}\_{n},\hat{\nu}\_{n}). Then, the conditional kernel \hat{\kappa}\_{n} defined by (\hat{\kappa}\_{n})\_{x}=\pi\_{\tau,n}(\cdot|x) satisfies \E[\cE\_{p}(\hat{\kappa}\_{n};\mu,\nu)]\lesssim\_{p,d}n^{-1/(2pd\lor 4p)}\log^{2}%
(n).

The proof in B.1 has three steps. First, we control \cE\_{p}(\hat{\kappa}\_{n};\hat{\mu}\_{n},\hat{\nu}\_{n})=\widetilde{O}\_{d}(\tau^{%
1/p}) using a known bound of Genevay et al. (2019). Then, using the support constraint and the softmax form in (7), we bound the TV Lipschitz constant of \hat{\kappa}\_{n} by O\_{d}(\tau^{-1}), which implies the kernel is \Wp Hölder continuous with exponent 1/p and constant \smash{O\_{d}(\tau^{-1/p})}. Finally, we apply 4\crefpairconjunction3 to bound

|  |  |  |
| --- | --- | --- |
|  | \cE\_{p}(\hat{\kappa}\_{n};\mu,\nu)\leq\cE\_{p}(\hat{\kappa}\_{n};\hat{\mu}\_{n},% \hat{\nu}\_{n})+O\_{d}((\rho/\tau)^{1/p})+O(\rho)=\widetilde{O}\_{d}(\tau^{1/p}+(% \rho/\tau)^{1/p}+\rho), |  |

where \rho=\Wp(\hat{\mu}\_{n},\mu)\lor\Wp(\hat{\nu}\_{n},\nu). Applying 1 to bound \rho and tuning \tau gives the theorem. Note that \tau controls a bias-variance trade-off (\tau\!\to\!0 overfits, while \tau\!\to\!\infty blurs out all structure).

To understand the quality of this bound, we compare to Brenier map estimation with p=2. Here, the conditional kernel is usually converted into a deterministic map \smash{\hat{T}\_{n}} via barycentric projection, which sends x to the mean of Y\sim\hat{\pi}\_{\tau,n}(\cdot|x). Existing work has derived a variety of L^{2} estimation guarantees for this estimator with respect to the Brenier map T^{\star}. In particular, Pooladian and Niles-Weed (2021) show that \E\|\hat{T}\_{n}-T^{\star}\|\_{L^{2}(\mu)}\lesssim\_{d}n^{-(\alpha+1)/[4(d+\alpha%
+1)]}\log n if T^{\star}\in\cC^{\alpha+1} for 1<\alpha\leq 3 and \nabla T^{\star} has eigenvalues bounded from above and below. For the sake of comparison, we can take a formal limit as \alpha\to 0 (although not covered by their theory) to obtain a rate of n^{-1/(4d+4)}, which is always worse than our n^{-1/(4d\lor 8)} rate (which does not require p=2 nor the existence of T^{\star}).

Improved rounding estimator. While guarantees for the entropic kernel estimator from 1 are compelling, we can achieve sharper rates using the following rounding estimator, via an analysis based on TV stability of \cE\_{p}. The estimator is specified by an accuracy \delta\geq 0, a trimming radius R>0, a partition \cP of \R^{d}, and a collection of centers C\_{\cP}=\{c\_{P}\}\_{P\in\cP} such that each c\_{P}\in P. This induces a rounding function r\_{\cP}:\R^{d}\to C\_{\cP} which, for each P\in\cP, maps x\in P to c\_{P}. Given empirical measures \hat{\mu}\_{n} and \hat{\nu}\_{n}, we proceed as follows:

1. 1.

   Round \hat{\mu}\_{n} onto \cP, taking \mu^{\prime}\_{n}=(r\_{\cP})\_{\sharp}\hat{\mu}\_{n}
2. 2.

   Compute a preliminary kernel \bar{\kappa}\_{n}\in\cK(C\_{\cP},\supp(\hat{\nu}\_{n})) which pushes \mu^{\prime}\_{n} onto \hat{\nu}\_{n} and is near-optimal for the \Wp problem, satisfying \iint\|x-y\|^{p}\dd\bar{\kappa}\_{n}(y|x)\dd\mu^{\prime}\_{n}(x)\leq\Wp(\mu^{%
   \prime}\_{n},\hat{\nu}\_{n})^{p}+\delta.
3. 3.

   Return kernel \hat{\kappa}\_{n}=\bar{\kappa}\_{n}\circ r\_{\cP}, which, given x\in\R^{d}, rounds it to r\_{\cP}(x) before applying \bar{\kappa}\_{n}.

For a simple choice of \cP, this procedure achieves low \cE\_{p} error when \mu and \nu are sub-Gaussian. With a more complex partition, we can support \mu with only bounded 2pth moments.

###### Theorem 2 (Rounding estimator).

Let \mu,\nu be 1-sub-Gaussian, and take \cP as the regular partition of \R^{d} into cubes of side length r. Then, for R, r, and \delta tuned independently of \mu and \nu, we have \E[\cE\_{p}(\hat{\kappa}\_{n};\mu,\nu)]=\widetilde{O}\_{p,d}\bigl{(}n^{-1/(d+2p)}%
\bigr{)}. For an alternative, non-uniform partition, this guarantee still holds if the sub-Gaussianity assumption on \mu is relaxed to \E\_{\mu}[\|X\|^{2p}]\leq 1. In both cases, computation is dominated by Step 2 which, if implemented via an EOT solver, runs in time O((C\_{\infty}+d)n^{2+o\_{d}(1)}), where C\_{\infty}=\max\_{i,j}\|X\_{i}-Y\_{j}\|.