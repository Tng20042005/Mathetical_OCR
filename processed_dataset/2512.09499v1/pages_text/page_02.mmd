|  |  |  |  |
| --- | --- | --- | --- |
|  | \Wp(\mu,\nu)=\min\_{\begin{subarray}{c}\kappa\in\cK(\cX,\cY)\\ \kappa\_{\sharp}\mu=\nu\end{subarray}}\left(\iint\|x-y\|^{p}\dd\kappa(y|x)\dd% \mu(x)\right)^{\frac{1}{p}}, |  | (2) |

where \kappa(\cdot|\cdot) varies over Markov kernels (regular conditional probability distributions) from \cX to \cY and \kappa\_{\sharp}\mu denotes the pushforward measure \int\kappa(\cdot|x)\dd\mu(x).111The standard Kantorovich OT problem optimizes the cost over couplings \pi\in\Pi(\mu,\nu), but the disintegration theorem yields that each such coupling can be decomposed as \dd\pi(x,y)=\dd\mu(x)\dd\pi(y|x), where \pi(\cdot|\cdot) is a Markov kernel induced by conditioning on the left argument. When a coupling is induced by a deterministic map T, i.e., \pi=(\mathrm{Id},T)\_{\sharp}\mu, the corresponding kernel \kappa is given by \kappa\_{x}=\delta\_{T(x)}. We propose a novel framework for stochastic OT map estimation by furnishing a suitable error metric. For source distribution \mu\in\cP(\cX), target distribution \nu\in\cP(\cY), and kernel \kappa from \cX to \cY, we define the *transportation error* \cE\_{p}(\kappa;\mu,\nu) of \kappa for the \Wp(\mu,\nu) problem by

|  |  |  |
| --- | --- | --- |
|  | \displaystyle\underbrace{\biggl{[}\!\left(\iint\!\|x\!-\!y\|^{p}\dd\kappa\_{x}(% y)\dd\mu(x)\!\right)^{\frac{1}{p}}\!\!-\,\Wp(\mu,\nu)\biggr{]}\_{+}}\_{\text{% optimality gap}}\!+\underbrace{\vphantom{\biggr{]}\_{+}}\Wp(\kappa\_{\sharp}\mu,% \nu)}\_{\text{feasibility gap}}, |  |

where [c]\_{+}\!\defeq\!\max\{c,0\} and \kappa\_{x}(\cdot)\defeq\kappa(\cdot|x). Under \cE\_{p}, the quality of \kappa is thus measured by its transportation cost overhead on top of the optimum \Wp(\mu,\nu) (dubbed *optimality gap*), plus its p-Wasserstein gap from matching the target \nu (the *feasibility gap*). While the \cE\_{p} error metric naturally accounts for deterministic OT maps, it does not require uniqueness or even existence thereof. This enables treating OT map estimation settings far beyond those accounted by existing theory, as illustrated in 1 to the right. Remarkably, beyond the broad coverage of the proposed framework, quantitative bounds on \cE\_{p} can be derived under minimal and easy-to-verify assumptions, rendering the guarantees applicable in practice.

Our technical contributions build upon a foundation of stability lemmas for \cE\_{p} established in 2. These characterize how \cE\_{p} responds to TV and Wasserstein perturbations of the input measures and to compositions of the kernel. In 3, we apply these to finite-sample estimation and computation. Here, our strongest result holds when \nu is sub-Gaussian and \mu has bounded 2pth moments, but assuming no regularity of an optimal kernel. For i.i.d. samples X\_{1},\dots,X\_{n}\sim\mu and Y\_{1},\dots,Y\_{n}\sim\nu, we present a rounding-based estimator \hat{\kappa}\_{n} which achieves
\E[\cE\_{p}(\hat{\kappa}\_{n};\mu,\nu)]=\widetilde{O}\_{p,d}\bigl{(}n^{-1/(d+2p)}%
\bigr{)}, with running time O(n^{2+o\_{d}(1)}) dominated by a single, low-accuracy call to an entropic OT solver. We also observe a minimax lower bound of \Omega(n^{-1/(d\lor 2p)}), showing that our rate is near-optimal.

In 4, we examine the statistical landscape of estimation when there exists a Hölder continuous optimal kernel, a condition that is still significantly weaker than typical assumptions for Brenier map estimation when p=2. In particular, for the case of a Lipschitz optimal kernel \kappa\_{\star}, i.e., when \Wp((\kappa\_{\star})\_{x},(\kappa\_{\star})\_{x^{\prime}})\lesssim\|x-x^{\prime}\| for all x,x^{\prime}\in\cX, we show that kernel estimation under \cE\_{p} has the same statistical complexity as estimating \mu and \nu under \Wp, with rate \widetilde{O}(n^{-1/(d\lor 2p)}). For this reduction, we employ an estimator based on Wasserstein distributionally robust optimization.

In 5, we show that effective kernel estimation is possible in the presence of adversarial data contamination. Historically, robust statistics has been well-studied under Huber’s \eps-contamination model for global outliers, which is subsumed by TV \eps-corruptions of the input data (Huber, 1964). More recently, statisticians have examined robust estimation under localized Wasserstein corruptions of the input samples (Zhu et al., 2022; Chao and Dobriban, 2023; Liu and Loh, 2023). We consider a strong corruption model where the clean samples can be corrupted both in TV and under the Wasserstein metric. This combination of local and global corruptions only has only explored recently (Nietert et al., 2023b, 2024; Pittas and Pensia, 2025), and their interaction has required careful analysis.
Here, stability of \cE\_{p} enables us to cleanly decouple the two corruption types. Against an adversary with TV budget \eps and \Wp budget \rho, we show that a convolutional estimator achieves error \sqrt{d}\eps^{1/p}+\sqrt{d}\rho^{1/(p+1)}+O\_{p,d}(n^{-1/(d+2p)}). An accompanying minimax lower bound of \sqrt{d}\eps^{1/p}+d^{1/4}\rho^{1/2}+n^{-1/(d\lor 2p)} implies a separation between robust map estimation under \cE\_{p} and robust distribution estimation under \Wp, where one can achieve linear dependence on \rho.

In 6 , we validate our theory with numerical simulations for two settings with irregular OT maps that are poorly suited for existing theory. These showcase the performance of our rounding estimator and the benefits of \cE\_{p} over L^{p}. Overall, our results constitute a general-purpose theory for (possibly stochastic) OT map estimation, subject to minimal primitive assumptions. As such, it is capable of providing formal performance guarantees in the \cE\_{p} sense in various practically relevant settings.

#### Related work.

Most related to this paper is a line of statistics work on the minimax sample complexity of Brenier map estimation when p=2, initiated by Hütter and Rigollet (2021). Under density assumptions on \mu and smoothness conditions on the unique Brenier map T^{\star} (in particular, Lipschitzness), they obtain near-optimal risk bounds of the form \smash{\|\hat{T}-T^{\star}\|\_{L^{2}(\mu)}}=\widetilde{O}(n^{-1/d}), using empirical risk minimization for a semi-dual objective.
The myriad of follow-ups include Pooladian and Niles-Weed (2021); Deb et al. (2021); Manole et al. (2024), all of which impose density and smoothness assumptions. Pooladian et al. (2023) considered the semi-discrete setting where the Brenier map is piecewise constant, employing an estimator based on entropic OT (EOT).
Recently, Balakrishnan and Manole (2025) provided refined guarantees that sidestep the typical density assumptions, but they still rely on the Brenier map being the gradient of a sufficiently regular convex potential. Lastly, a variety of neural map estimators have been developed by the machine learning community (Seguy et al., 2018; Meng et al., 2019; Wang and Goldfeld, 2024), with applications to domain adaptation, style transfer, trajectory estimation, and the like.