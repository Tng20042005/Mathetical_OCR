{\epsilon}\sim\epsilon^{-1}. We take this chance to cover \|u\|\_{L\_{t,x}^{\infty}} rather than estimate pointwise in t. obtained (1.3) for such types of random initial data. It has been remarked in [7, Remark 1.3], finding optimal family of coefficients (in terms of decay), rather than considering exponential decay c\_{n}, remains an interesting open question.

By combining random data type techniques in [2, 3] and some ideas in [4] (which also goes back to Bourgain’s 1990s seminal work), we are able to extend the result in [7] to coefficients with polynomial decay \langle n\rangle^{-\frac{1}{2}-\theta}, \theta>0. Such constraint is optimal in the sense when \theta\leq 0, even the initial data will leave L\_{x}^{\infty}. It remains an interesting question to go beyond the time scale T\_{\epsilon}. We note that this \epsilon^{-1} time scale (neglecting logarithm corrections) is in some sense critical and we refer to [7, Remark 1.7] for more details.
We note that both current work and [7], can cover time scale of form c\frac{|\ln\epsilon|}{\epsilon}, where c is some small universal number.

The overall proof scheme of current article follows [7], which contains a large deviation principle for the (modified) linear flow and a smoothing estimate for the Duhamel part, (there is some subtle part here since one needs to first slightly perturb the free linear flow, we will discuss it later).

Our main contributions in this work are

* •

  We find a rather straightforward proof for the large deviation principle for the linear flow, which simplifies the arguments in [7], and more importantly, extends to random initial data of form \sum\_{n}\frac{g^{\omega}\_{n}}{\langle n\rangle^{\frac{1}{2}+\theta}}e^{inx}, \forall\theta>0. Our techniques for this part are crucial for our further analysis in the nonlinear smoothing part.
* •

  We adapt the X^{s,b} analysis in [2, 3] in the setting of long time analysis of [7], which gives the desired nonlinear smoothing444On one hand, we will use the X^{s,b} analysis in the local random data theory as a black box; on the other hand, the long time analysis we use to get nonlinear smoothing does explicitly use computations of X^{s,b} type, but without going to the explicit form of X^{s,b} analysis..

We end this session by pointing out ever since the seminal work [2, 3], random initial data theory in nonlinear dispersive PDEs has become a very active research field. Many researchers contribute to this field and it is impossible to survey the field here. We simply mention that in the perspective of regularity in NLS model, the recent breakthrough [5] gives a quite complete answer. We refer to [5], [7] and reference therein for more related reference.

### 1.3 Notation

Many terms g\_{n}^{\omega},u^{\omega},u\_{0}^{\omega} are random, and thus depend on \omega. For simplicity, we will hide the implicit \omega and simply denote them as g\_{n},u,u\_{0}.

We introduce \mathcal{N}\_{1},\mathcal{N}\_{2} as

|  |  |  |  |
| --- | --- | --- | --- |
|  | \mathcal{N}\_{1}(f\_{1},f\_{2},f\_{3})=\sum\_{n\_{2}\neq n\_{1},n\_{3}}\hat{f}\_{1}(n\_{% 1})\overline{\hat{f}\_{2}}(n\_{2})\hat{f}\_{3}(n\_{3})e^{i(n\_{1}-n\_{2}+n\_{3})x}, |  | (1.4) |

|  |  |  |  |
| --- | --- | --- | --- |
|  | \mathcal{N}\_{2}(f\_{1},f\_{2},f\_{3})=\sum\_{n}\hat{f}\_{1}(n)\overline{\hat{f}\_{2}% }(n)\hat{f}\_{3}(n)e^{inx} |  | (1.5) |

and one has

|  |  |  |  |
| --- | --- | --- | --- |
|  | |f|^{2}f-2\frac{1}{2\pi}(\int|f|^{2})f=\mathcal{N}\_{1}(f,f,f)-\mathcal{N}\_{2}(% f,f,f) |  | (1.6) |

We sometimes short \mathcal{N}\_{i}(f,f,f) as \mathcal{N}\_{i}(f).

## Acknowledgement

C. Fan was partially supported by the National Key R&D Program of China, 2021YFA1000800, CAS Project for Young Scientists in Basic Research, Grant No.YSBR-031, and NSFC Grant (Nos. 12288201 & 12471232).

## 2 Preliminary

### 2.1 X^{s,b} space

In this section, we recall that X^{s,b} norm is defined by

|  |  |  |
| --- | --- | --- |
|  | \left\|u\right\|\_{X^{s,b}}:=\left(\int\_{\mathbb{R}}\sum\_{n}\langle\tau+n^{2}% \rangle^{2b}\langle n\rangle^{2s}|\hat{u}(\tau,n)|^{2}d\tau\right)^{\frac{1}{2% }}, |  |

where \hat{u}(\tau,n) is the spacetime Fourier transformation of u. One defines X^{s,b}[0,T] local in time by

|  |  |  |
| --- | --- | --- |
|  | \left\|u\right\|\_{X^{s,b}[0,T]}:=\inf\_{\tilde{u}=u,t\in[0,T]}\left\|\tilde{u}% \right\|\_{X^{s,b}}. |  |

These kinds of spaces were first introduced by Bourgain [1] and are of essential use in random data theory, [2], [3]. We will only use them as a black box. We note that

* •

  X^{s,b}[0,T]\subset C\_{t}H^{s}([0,T]\times\mathbb{T}) for any b>\frac{1}{2}.
* •

  X^{s,b} space is useful, among others, because of the following X^{s,b} smoothing estimate,

  |  |  |  |  |
  | --- | --- | --- | --- |
  |  | \left\|\eta(t)\int\_{0}^{t}e^{i(t-s)\Delta}F(s)ds\right\|\_{X^{s,b+1}[0,1]}% \lesssim\left\|F\right\|\_{X^{s,b}[0,1]}. |  | (2.1) |

### 2.2 Hyper-contractivity estimate

We recall the following well-known hypercontractivity of multi-Guassian from [10], see also [3], [6]. Those estimates are widely used in random data theory, and is probably well known in the probability community.

We record it here for the convenience of readers.

###### Lemma 2.1.

Let \{g^{w}\_{n}\} be i.i.d complex Gaussian, let \{c\_{n\_{1},\ldots,n\_{k}}\} be (fixed and deterministic) complex numbers. Let

|  |  |  |
| --- | --- | --- |
|  | \displaystyle F\_{k}(w)=\sum\limits\_{n\_{1},\ldots,n\_{k}}c\_{n\_{1},\ldots,n\_{k}}g% ^{w}\_{n\_{1}}\cdots g^{w}\_{n\_{k}}. |  |

Then there holds the associated large deviation type estimate

|  |  |  |
| --- | --- | --- |
|  | \displaystyle\mathbb{P}\{|F\_{k}|>\lambda\|F\_{k}\|\_{L^{2}(\Upsilon)}\}\leq e^{-% C\lambda^{{2}/{k}}},\quad\forall\lambda>0. |  |

## 3 An overview of the proof

Let u,u\_{0} be as in Theorem 1.1. Recall

|  |  |  |  |
| --- | --- | --- | --- |
|  | u\_{0}=\sum\_{n}c\_{n}g\_{n}e^{inx}=\sum\_{n}\frac{g\_{n}}{\langle n\rangle^{\frac{1% }{2}+\theta}}e^{inx}. |  | (3.1) |

We will fix \theta>0. We will only consider \theta\ll 1 and the arguments easily extend to the case when \theta takes larger value.

For every \epsilon>0, we fix T\_{\epsilon}\sim\epsilon^{-1}.