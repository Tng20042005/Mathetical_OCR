import streamlit as st
import torch
from PIL import Image
from transformers import NougatProcessor, VisionEncoderDecoderModel
import time
import re

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(
    page_title="Mathematical Expression OCR",
    page_icon="üî¨",
    layout="wide"
)

# --- 1. H√ÄM CHUY·ªÇN ƒê·ªîI MARKDOWN -> LATEX ---
def markdown_to_latex_converter(md_text):
    """
    Chuy·ªÉn ƒë·ªïi c√∫ ph√°p Markdown c·ªßa Nougat sang LaTeX code ho√†n ch·ªânh.
    """
    latex_out = r"""\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}

\title{Nougat OCR Output}
\date{\today}

\begin{document}
\maketitle

"""
    lines = md_text.split('\n')
    processed_lines = []
    in_abstract = False
    
    for line in lines:
        line = line.strip()
        if not line:
            processed_lines.append("")
            continue
            
        # X·ª≠ l√Ω Header
        if line.startswith('# '): line = f"\\section{{{line[2:]}}}"
        elif line.startswith('## '): line = f"\\subsection{{{line[3:]}}}"
        elif line.startswith('### '): line = f"\\subsubsection{{{line[4:]}}}"
        
        # X·ª≠ l√Ω Bold/Italic
        line = re.sub(r'\*\*(.*?)\*\*', r'\\textbf{\1}', line)
        line = re.sub(r'\*(.*?)\*', r'\\textit{\1}', line)
        
        # X·ª≠ l√Ω Abstract
        if "Abstract" in line and "\\textbf" in line:
            line = "\\begin{abstract}\n" + line.replace("\\textbf{Abstract}", "").strip()
            in_abstract = True
            
        processed_lines.append(line)

    if in_abstract:
         processed_lines.append("\\end{abstract}")

    latex_out += "\n".join(processed_lines) + "\n\n\\end{document}"
    return latex_out

# --- 2. LOAD MODEL ---
@st.cache_resource
def load_model():
    print("ƒêang t·∫£i model Nougat...")
    model_name = "facebook/nougat-small"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        processor = NougatProcessor.from_pretrained(model_name)
        model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)
    except Exception as e:
        st.error(f"L·ªói t·∫£i model: {e}")
        return None, None, None
    return processor, model, device

# --- 3. H√ÄM SUY LU·∫¨N ---
def predict(image, processor, model, device):
    pixel_values = processor(images=image, return_tensors="pt").pixel_values.to(device)
    
    outputs = model.generate(
        pixel_values,
        min_length=1,
        max_length=3584,
        bad_words_ids=[[processor.tokenizer.unk_token_id]],
        return_dict_in_generate=True,
        output_scores=True,
        stopping_criteria=[],
    )
    
    generated_text = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]
    generated_text = processor.post_process_generation(generated_text, fix_markdown=False)
    return generated_text

# --- 4. GIAO DI·ªÜN CH√çNH ---
def main():
    st.title("üìÑ Mathematical Expression OCR    : Chuy·ªÉn ƒë·ªïi 3 D·∫°ng")
    st.markdown("Tool chuy·ªÉn ƒë·ªïi ·∫£nh t√†i li·ªáu sang: **Render View**, **Raw Markdown** v√† **LaTeX Source**.")

    with st.spinner("ƒêang kh·ªüi ƒë·ªông AI..."):
        processor, model, device = load_model()

    if model is None: return

    st.sidebar.markdown(f"**Thi·∫øt b·ªã ch·∫°y:** `{device.upper()}`")
    
    uploaded_file = st.file_uploader("T·∫£i ·∫£nh l√™n (PNG, JPG)", type=["png", "jpg", "jpeg"])

    if uploaded_file is not None:
        col1, col2 = st.columns([1, 1.2]) # C·ªôt ph·∫£i r·ªông h∆°n ch√∫t ƒë·ªÉ hi·ªÉn th·ªã text
        
        with col1:
            st.subheader("üñºÔ∏è ·∫¢nh g·ªëc")
            image = Image.open(uploaded_file).convert("RGB")
            # S·ª≠a warning use_column_width
            st.image(image, use_container_width=True)

        generate_btn = st.sidebar.button("üöÄ X·ª≠ l√Ω ngay", type="primary")

        if generate_btn:
            with col2:
                with st.spinner("ƒêang ƒë·ªçc v√† d·ªãch sang LaTeX..."):
                    start_time = time.time()
                    
                    # 1. L·∫•y k·∫øt qu·∫£ Markdown t·ª´ Model
                    md_text = predict(image, processor, model, device)
                    
                    # 2. Convert sang LaTeX
                    latex_code = markdown_to_latex_converter(md_text)
                    
                    end_time = time.time()
                
                st.success(f"Ho√†n t·∫•t trong {end_time - start_time:.2f} gi√¢y!")
                
                # --- T·∫†O 3 TAB HI·ªÇN TH·ªä ---
                tab_render, tab_markdown, tab_latex = st.tabs([
                    "üëÅÔ∏è Xem tr∆∞·ªõc (Render)", 
                    "üìù Markdown Result", 
                    "üíª LaTeX Output"
                ])
                
                # Tab 1: Render (Xem ƒë·∫πp m·∫Øt)
                with tab_render:
                    st.markdown("### B·∫£n xem tr∆∞·ªõc:")
                    st.markdown("---")
                    st.markdown(md_text)
                    st.markdown("---")
                
                # Tab 2: Markdown (Code g·ªëc c·ªßa Nougat)
                with tab_markdown:
                    st.markdown("Copy ƒëo·∫°n n√†y n·∫øu d√πng Obsidian/Notion:")
                    st.text_area("Raw Markdown", md_text, height=500)
                
                # Tab 3: LaTeX (Code ƒë·ªÉ bi√™n d·ªãch)
                with tab_latex:
                    st.markdown("Copy ƒëo·∫°n n√†y n·∫øu d√πng Overleaf/TeXShop:")
                    st.code(latex_code, language='latex')
                    st.download_button(
                        label="üì• T·∫£i file .tex",
                        data=latex_code,
                        file_name="output.tex",
                        mime="text/plain"
                    )

if __name__ == "__main__":
    main()